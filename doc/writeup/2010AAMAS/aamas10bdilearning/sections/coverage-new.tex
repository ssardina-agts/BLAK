%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Informed Plan Selection}\label{sec:coverage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%In this section, we extend our BDI learning so as to include a confidence test
%within the plan selection mechanism itself. More concretely, in the new approach,
%instead of using the stability measure to ensure that we only record results that
%we are confident in, we instead modify our plan selection procedure to take into
%account of how confident we are in the current decision tree.
% %
Our new approach relies on the idea that confidence in a plan's
decision tree increases as more of the possible choices below the plan
(in the goal-plan structure) are explored.
% %
%Note that while the stability mechanism ensures that failure information is
%\emph{not recorded} against a decision tree until the corresponding goal-plan
%structure is adequately explored; the new approach instead influences how the
%decision tree is \emph{used} at plan selection time.

With each plan in the goal-plan tree hierarchy, we identify its set of
potential \textit{choices} as the set of all potential execution paths
\textit{below} the plan in the hierarchy. This can easily be computed
offline.
% %
Intuitively, a plan's decision tree is more \textit{informed} for a world
state, if it is based on a larger number of choices having been
explored in that state.
%accounts for a larger number of the plan's choices in that state.
% %
We say that a plan has a higher degree of \emph{coverage} when more of its
underlying choices have been explored in the past and accounted for in the
corresponding decision tree.
% %
Technically, given a decision tree $T$ for given plan, we define its coverage
$c_T(w) \in [0,\ldots,1]$ as the coverage degree of $T$ for world state $w$.
% %
Initially, when the plan has not yet been executed in a world $w$, its coverage
in the state, namely $c_T(w)$, is zero and the agent has no basis for
confidence in
the probability of plan success estimated by the decision tree at $w$. As the
different ways of executing the plan in the world are explored, the value of
$c_T(w)$ approaches $1$. When all choices have been tried, $c_T(w)=1$ and the
agent may rely fully on the decision tree estimation of success.
% %
Put differently, the coverage degree provides a confidence test on the decision
tree.

We define an equation for determining the final plan selection
weight for a plan that is influenced by the above coverage-based confidence measure.
% %
Formally, we define the plan's weight for plan selection, namely $\Omega'(w)$, based
on the plan success expectation returned by the decision tree and the current
coverage degree:
%
\begin{equation*}\label{eqn:coverage}   
\Omega'_T(w) = 0.5 + \left[  c_T(w) *  \left( p_T(w) - 0.5 \right)  \right].
\end{equation*}
	
	
Initially, the weight of the plan for a world where it has never been executed is
assigned a default value ($0.5$ in our case).
% %
As the coverage degree increases with time, by trying the various ways the plan
could be executed, the weight of the plan approaches the true value estimated by
the plan's decision tree.


%Regarding how the coverage degree is calculated at every point, we should make
%the following two observations.
% %
Each time a result is recorded in a decision tree, $T$, the coverage
$c_T(w)$ for a world $w$ is calculated and stored for the decision tree.
% %
It requires, in principle, $\tau \times |S|$ \emph{unique} executions of
a plan for it to reach \emph{full} coverage, where $\chi$ is the total number of
choices below the plan and $|S|$ is the number of possible worlds. Practically,
however, it takes significantly less since choices below a plan are effectively
represented by an AND/OR tree, and each time an AND node fails, the subsequent
nodes are not tried and are counted as covered for the world in question.
% %
Also, a plan may not be accessible from all world states, so in
practice it will only need to be assessed in the subset of the world
states that are relevant for it.
%Moreover, since plans are typically executed in the context of other plans
%(e.g., the plan to check-in is always executed after a plan to go to the
%airport), it would not be necessary to account for the whole world space, but
%only for its subset that is relevant for the plan.


\newcommand{\CLSELA}{\mbox{$\CL\!\!+\!\!\Omega$}}
\newcommand{\CLSELB}{\mbox{$\CL\!\!+\!\!\Omega'$}}
\newcommand{\BULSELA}{\mbox{$\BUL\!\!+\!\!\Omega$}}
\newcommand{\BULSELB}{\mbox{$\BUL\!\!+\!\!\Omega'$}}

\medskip With the new account for plans' weight, we (re)considered the two
learning approaches \CL\ and \BUL\ from the previous section with a plan
selection based on such weights. We shall refer to such new agents as \CLSELB\
and \BULSELB, respectively.
% %
Similarly, let us call $\CLSELA$ and $\BULSELA$ the corresponding agents using
the \emph{original} probabilistic selection where the weight of a plan depends
only on its decision tree success expectation $\Omega_T(w) = p_T(w)$.


First of all, when it comes to the \BUL\ stability-based learning scheme, the
 $\BULSELA$ and $\BULSELB$ approaches show similar performance.
% %
This is not surprising, as the stability test performed by these agents at each
plan node implicitly implies a test of coverage. Indeed, for a plan to become
``stable,'' the agent needs to (substantially) explore all possible  ways of
executing it. Consequently,  until the plan is deemed stable, $\Omega_T$ behaves
like $\Omega_T'$ with coverage $c_T(w)=0$, thus effectively reducing
$\Omega'_T(w)$ to $\Omega_T(w)$.



Now, let us focus on the \CL\ approach.
% %
For the \CL-favouring structure $\T_1$, the performance of \CLSELB matched that
of \CLSELA\ reported in the previous section (cf. Figure~\ref{fig:T1_result}).
% %
This was also the case with the balanced structure $\T_3$ where both \CL\ and
\BUL\ performed equally well under the original plan selection mechanism---the
performance of \CLSELB\ was the same as that reported in
Figure~\ref{fig:T3_result} for \CLSELA.
% %
Thus, for the cases where \CL\ was performing reasonably well, nothing is lost
when using the new plan selection mechanism.



The situation is completely different, though, when one considers the goal-plan
structure $\T_2$ in which the \CLSELA\ used to performance poorly (cf. Figure
\ref{fig:T2_result}).
% %
When ran in $\T_2$, the \CLSELB\ approach showed a dramatic improvement over the
\CLSELA-based one. Figure \ref{fig:T2_result2} shows this change with the results
for the new approach to plan selection $\BULSELB$ and $\CLSELB$ superimposed over
the original results from Figure \ref{fig:T2_result}.
% %
The reason why the new plan selection mechanism improves the \CL\ learning scheme
is that even though the success estimation for plan $P_i$ would still be low
initially (remember that \CL, in contrast with \BUL, would record all initial
failure runs against $P_i$), the agent would not be so confident on such
estimation until the plan's coverage degree increases and it will therefore boost
such estimation towards the default weight of $0.5$. In other words, the false
negative executions collected by the agent for plan $P_i$ would not be considered
so seriously due to low plan coverage. As full coverage is approached, one would
expect the agent to have discovered the success execution encoded in $P_i$.



\begin{figure}[t]
\begin{center}
\input{figs/T2-result2}
\caption{Performance comparison of \CLSELB\ (solid crosses) over
\CLSELA\ and \BULSELA\ (both in dotted grey) for structure $\T_2$.}
\label{fig:T2_result2}
\end{center}
\end{figure}



Even more interesting is the the impact of the new plan selection mechanism on
agents that work with an applicability threshold, that is, agents that may not
select a plan if it is deemed not worth it. For this cases, the coverage-based
avoids the \CL\ approach judging a plan not worth it based on false negative
experiences. Even if a plan is deemed with very low probability of success, if it
has not been substantially ``covered,'' its weight would still be biased towards the
default value of $.5$. The system should always have
the applicability threshold substantially below the default plan selection
weight.
%%
Figure~\ref{fig:performance-applicability} shows how the new agent systems
using the $\Omega'$ selection weight perform, and in particular, how \CLSELB\
does not suffer from the limitations of its previous \CLSELA\ version.

Using coverage to bias plan selection weights encourages the agent to
explore all the possible options available. 
This will also ensure that
an agent using this approach can be assured of which is the optimal
solution, faster than an agent using the original approach. In some
applications this may be important.

\omitable{
Finally, we point out that agents based on the new coverage-based plan selection
scheme will be able to realize when the optimal solution for a goal has been
reached before the agents based on the original scheme.
% %
This happens because, by using the coverage degree to calculate the final weight
of a plan for a goal, the system is both looking for successful paths while
maximizing    the uniform exploration of the agent's plan library.
% %
So, for example, since $P$ is more ``complex'' than $P_i'$ in in structure $\T_2$
(Figure~\ref{fig:T2}), an agent based on the $\Omega'$ weighting function, will
tend to give more execution chances to the former in order to achieve a more
uniform exploration of the options for goal $G$.
}

The above results show that the coverage based confidence weighting
can improve the performance of the \CL\ approach in those
cases where it performed poorly due to false negative experiences,
i.e., failure runs for a plan that includes successful executions.
%
Importantly this approach also gives us a flexible mechanism to bias the 
behaviour depending on application characteristics. Observe the formula:
\begin{equation*}\label{eqn:coverage*}
\Omega'_T(w) = 0.5 + \left[  c_T(w)^{1/\alpha} *  \left( p_T(w) - 0.5 \right)
\right].
\end{equation*}
\noindent When $\alpha \approx 0$, the \CLSELB\ scheme will behave more
like a \BULSELA-based system: $c_T(w)^{1/\alpha}$ transitions directly from $0$
to $1$ when $c_T(w)$ reaches $1$ (and it remains zero otherwise).
% %           
On other hand, when $\alpha \approx \infty$, the \CLSELB\ scheme will behave more
like the \CLSELA: $c_T(w)^{1/\alpha}$ transitions from $0$ to $1$ faster and
$\Omega'(w) \approx p_T(w)$.
%
When $\alpha=1$ this is the same as our initial equation.


\omitable{
The above results are significant in that they suggest that the alternative plan
selection mechanism, which includes a confidence test based on the notion of plan
coverage, can substantially improve the performance of the \CL\ approach in those
cases where it performed poorly due to false negative experiences, i.e., failure
runs for a plan that includes successful executions.
% %
The main point here is that \CLSELB\ provides a \emph{middle ground} between
\CLSELA\ and \BULSELA. In fact, one can easily extend the plan weight formula to
bias the architecture towards one of the two extremes, namely: (The
original formula is obtained when $\alpha=1$.)
% %
\begin{equation*}\label{eqn:coverage*}   
\Omega'_T(w) = 0.5 + \left[  c_T(w)^{1/\alpha} *  \left( p_T(w) - 0.5 \right) 
\right].
\end{equation*}

\noindent Hence, when $\alpha \approx 0$, the \CLSELB\ scheme will behave more
like a \BULSELA-based system: $c_T(w)^{1/\alpha}$ transitions directly from $0$
to $1$ when $c_T(w)$ reaches $1$ (and it remains zero otherwise).
% %
On other hand, when $\alpha \approx \infty$, the \CLSELB\ scheme will behave more
like the \CLSELA: $c_T(w)^{1/\alpha}$ transitions from $0$ to $1$ faster and
$\Omega'(w) \approx p_T(w)$.
% %

Putting it altogether, all the above suggests that the \CLSELB\ learning
approach is most adequate to address the problem we are concerned on, since it
is simpler and more flexible than the alternative ones.
}



% Nonetheless, it should be noted that in our problem there is indeed the usual a
% intrinsic trade-off between exploration and execution success.





% \bigskip\bigskip\bigskip
% \textbf{move to discussion??}
% Since the
% coverage $c_T(S)$ in Equation \ref{eqn:coverage} is simply a
% confidence measure, then the way it is \textit{used} will determine
% the weighting of the confidence in the final plan selection
% probability $p'_T(w)$. For instance we could replace $c_T(S)$ in
% Equation \ref{eqn:coverage} with $c_T(S)^{1/b}$ where $b$ is the
% weighting (and $b=1$ gives us the original Equation
% \ref{eqn:coverage}). Then adjusting $b \rightarrow 0, (0 \ne b < 1$)
% we get more \BUL+$E$-like performance, while adjusting $b \rightarrow
% \infty (b > 1)$ will result in more \CL+$E$-like performance. In fact,
% an improved agent could reference the (static) goal-plan tree
% structure at runtime and adjust $b$ automatically based on the offline
% compiled knowledge of which approach works better for which tree
% topology. 
% % This extension is left as a future implementation exercise. 
% 
% We noted earlier in Section \ref{sec:experiments} that plan execution
% in real systems is often not cost-free, so presumably the agent would
% not execute a plan with too low a probability of success. We also show
% that such deliberation does not favour \CL\ but does \BUL. It is clear
% that choosing not to execute a plan below a threshold probability of
% failure would also hurt the \CL+$E'$ configuration (though
% not as much as \CL+$E$). For such systems, we suggest that the
% weighting $b$ be used to get the preferred \BUL-like performance. 


