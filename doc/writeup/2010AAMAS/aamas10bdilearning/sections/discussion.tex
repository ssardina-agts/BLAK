%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Conclusion}\label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this paper we propose a technique to enhance the typical plan selection mechanism in BDI systems by allowing agents to learn and adapt the context conditions of existing plans in the agent's plan library.
%
As designing adequate context conditions that take full account of the agent's environment for its complete life-cycle is often non-trivial, a framework that allows for the \emph{refinement} of (initial) context conditions of plans \textit{based on experience} is desirable. To this end we extend the BDI framework to use {\dt}s as (part of) context conditions and provide a probabilistic plan selection mechanism that caters for both exploration and exploitation of plans.
%
We empirically evaluate two approaches to learning context conditions from experience, an aggressive approach that records and uses every outcome for learning, and a more conservative approach that records failure outcomes only when it considers the preceding choices that led to failure to be well-informed.
We find that although each approach has advantages in certain situations, an important shortcoming of the aggressive approach is the complete inability to learn when applicability thresholds are used i.e. when the agent does not attempt plans whose chances of success are low. We then show that a new plan selection mechanism that also accounts for our confidence in the \dt classification can overcome the earlier issues with the aggressive approach. We conclude that the aggressive approach (that is also simpler), combined with the confidence measure (that also provides flexibility for tuning to different situations) is a better candidate for the general setting.

%This paper makes several contributions. Firstly we develop an extended
%BDI framework using decision trees as (part of) context conditions,
%and a \textit{probabilistic} plan selection function that caters for
%both exploration and exploitation of plans. This provides a basis for
%learning of context condition decision trees.  We propose and
%empirically evaluate two approaches to this task, an aggressive
%approach that records information from every execution, and a
%conservative approach that records failure information only when it is
%considered to be well-informed. Although each approach performs better
%in some circumstances, the aggressive approach can, in some
%situations, lead to total failure to learn. We then explore an
%approach based on accounting for our confidence in the decision tree
%during plan selection, rather than monitoring information recorded for
%the decision tree. We show that this overcomes the issues associated
%with the more aggressive approach when using the simpler plan
%selection. Furthermore the new plan selection mechanism can be tuned to
%different circumstances. We conclude that the preferred approach is to
%use the aggressive approach to building the decision trees, together
%with the confidence measure affecting plan selection.

%Our experimentation did not consider the effects of using failure
%recovery, under which alternative plans are tried upon the failure of
%a plan for achieving a goal. Clearly, failure recovery would induce
%much less ``pure'' execution traces, as execution of partial plans
%would be represented in them. 
%
%One limitation of our learning framework is that the learning is
%\emph{local} to plans, in the sense that it is not possible to learn
%interactions between sub-goals for a higher-level plan (e.g., learn
%that goal $G_b$ may \textit{only} succeed if goal $G_a$ is achieved in
%a certain manner). We are currently working on an approach to deal
%with this issue.

Our experiments do not consider the effects of conflicting interactions between sub-goals of a plan. For instance, if a sub-goal could succeed in more than one way causing different changes in the environment, one of which may in fact cause a subsequent sub-goal to fail, then in our current implementation it is not possible to detect and learn such interactions. Similarly we do not currently consider the effects of using failure recovery, under which alternative plans are tried upon the failure of a plan for achieving a goal. We note that both these features should be resolved for the real system. Moreover, we plan to do further experimentation with more realistic programs from existing applications. 

One critique of the coverage-based confidence measure used is that it has a defined end state ($c_T(S)=1$) whereas for a real system, learning and re-learning will occur indefinitely as the agent continually tries to adapt to a changing environment. This implies that our confidence in a \dt's classification would also require calibration based on a changing environment. If the change in the environment is deliberate (for instance the agent was moved into a new environment) then our confidence could be reset and subsequently \textit{re-built}. Without such a signal the agent must rely on some other method for deciding when the environment has changed in a significant way.

An appealing measure for the identification of environmental changes is through the relatedness of its features. For instance, an observation that the grass is \textit{wet} presumably has a high correlation to the fact that it is \textit{raining}, and a \dt may (based on prior observations) very well use these factors in determining the likelihood of success of a plan to navigate the field. If then, we were to witness a world where it is not raining but the grass is wet (could be morning dew), then this world would be very different from the typical worlds we have seen so far and so we may have strong reason to reduce our confidence in the \dt classification of this new world.



%\bigskip
%\textbf{Things to be discussed:}
%\begin{enumerate}
%  \item handling of continous variables (e.g., temperature)
%  \item more realistic stochastic actions (easy)
%  \item a way of relating worlds and learning their correlations, maybe
%  could help realize when we have changed environment. can we use learning of
%  bayesian networks for this?
%\end{enumerate}

%\bigskip\bigskip\bigskip

%Since decision trees can handle variables with more two values and
%even continous ones, our results generalize directly to such cases. In addition
%the results generalize trivially to scenarios where different actions can fail
%or succeed with different probabilities.
