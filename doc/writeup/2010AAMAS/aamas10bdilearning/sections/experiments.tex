%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results}\label{sec:experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



In order to explore the difference between \BUL\ and \CL, we set up testbed
programs composed of several goals and plans combined in a hierarchical manner
and yielding goal-plan tree structures of different shapes.\footnote{We have
implemented the learning agent system in the \JACK\ BDI platform
\cite{Busetta99jack}. The fact that \JACK\ is a Java-based system and
provides powerful meta-level reasoning capabilities, allows us to integrate \weka\ and
probabilistic plan-selection mechanisms with not much effort. Nonetheless, all
the results are independent on this and any other BDI agent system could
have been used.}
% %
In particular, we crafted goal-plan tree structures representing different
meaningful cases of BDI programs with one main top-level goal, i.e., the event to
be resolved. In addition, each structure enjoys the, generally desirable,
\emph{coverage} property, under which for every possible situation, (i.e., world
state), there is always a way of addressing the main goal, i.e., there is at
least one successful execution of the top-level event, provided the right plan
choices are made. Observe that such successful (plan) choices may be different
for different world states.
% , as know-how information is generally predicated on the situation it is
% applied in. %
When it comes to describing the possible (observable) world states, we have used
a set of logical (binary) propositions, representing the so-called fluent or
features of the environment that are observable to the agent (e.g., fluent
proposition $\mathit{DoorLocked}$ states whether the door is believed open or
not).
% %
Finally, we assume the agent is acting in a non-deterministic environment, in
which actions that are expected to succeed, may still fail with some (small)
probability. In most of our experiments, we assumed a $.1$ probability of
uncontrolled failure for such actions.\footnote{See Discussion section on how our
results generalize to a framework with world state built from non-binary fluents
and with more complex accounts for stochastic actions.}
% %




The experiments consisted in posting the top-level goal repetitively under random
world states, running the corresponding  BDI learning agent, and finally
recording whether the execution terminated successfully or not.
% %
We calculated the average rate of success of the goal every some fixed
number of iterations (in our case $20$), and investigated how such rate evolved
as the agent refined the context condition of plans.
% %%
We ran the tests under both a \BUL-based agent and a \CL-based agent,
ensuring the same sampling of random world states for each agent.

\begin{figure}[t]
\begin{center}
\input{figs/GPtree-T1}
\end{center}
\caption{Goal-plan tree hierarchical structure $\T_1$. To succeed, an agent is thus required to make
two correct choices, including selecting $P$ at the top-level.}
\label{fig:T1}
\end{figure}


From our set of experiments, we have selected three hierarchical structures that
best illustrate the results that we have obtained, namely:
% %
\begin{description}
\item[(Tree $\T_1$; Figure~\ref{fig:T1})] For each world state, the goal has
few plans that can succeed (plans $P_i$), but many other options of comparable
complexity that are bound to fail (plans $P_i'$). 
%%
Under this type of structure, an \CL-based agent will generally perform better 
than an agent using the learning \BUL\ approach.

\item[(Tree $\T_2$; Figure~\ref{fig:T2})] For each world state, the goal has
one plan that can succeed (plan $P$), and a few others that would fail.
However, the plan containing the solution is of substantial higher-complexity. 
%%
In this structure, a \BUL-based agent will outperform an \CL-based one.
% A structure in which \BUL\ is
% expected to have important advantages over \CL, since the latter may wrongly consider a
% top-level plan as a failing plan whereas there is a solution encoded
% under it. 

\item[(Tree $\T_3$; Figure~\ref{fig:T3})] This stands for a ``balanced''
structure that ends up providing different advantages for both \BUL\ and \CL\ in
different parts of the tree.
\end{description}


% In summary, we found that whereas the agent performance under the \BUL\ and \CL\
% approaches is comparable on the first and third cases, the \BUL\ scheme provides
% substantial benefits in the second case. What is more important, if we consider
% agents that may choose not to consider a plan at all when its chances of success
% are believed very low, then the \CL\ approach collapses completely whereas the
% \BUL\ is robust enough to maintain performance.

% For lack of space, we shall only give the form of tree $\T_1$ and informally
% explain the characteristics of the other two.

Let us next discuss each of the plan-goal structures and how the performance of
\BUL-based and \CL-based agents compare.


Under a structure like $\T_1$, the agent basically has several options of the
comparable complexity to resolve the top-level goal $G$ in certain world state,
in our case, $20$ options. However, most of such options---$17$ in our example;
plans $P_i'$---would inevitably lead to failure.
% %
The benefit of using the \CL\ approach comes from the fact that the agent will
decrease the probability of selecting each of those $17$ failing plans as soon as
they fail for the first time. In contrast, \BUL\ would require several failed
experiences of each of those ``bad'' top-level options before decreasing their
probability of selection---to update the decision tree of plan $P_i'$, \BUL\
requires each of their three subgoals to be ``stable.''
% %%
As we expected the \CL\ scheme did perform better in our experiments, in that it
achieved better success rate earlier; see Figure~\ref{fig:T1_result}.
% %
Note then that whereas the agent under the \CL\ approach reaches $50\%$ success
after $1000$ iterations, it takes more than double for \BUL\ to achieve that
performance.
% %
Eventually, both approaches will yield optimal performance. Observe that optimal
performance amounts to a success rate of $81\%$, as the environment fails with
probability $.1$ for every (working) action and each successful execution
involves the performance of two actions (leaf plans consist of single actions).


\begin{figure}[t]
\begin{center}
\input{figs/GPtree-T2}
\end{center}
\caption{Goal-plan tree hierarchical structure $\T_2$.}
\label{fig:T2}
\end{figure}


\begin{figure*}[t]
\begin{center}
\subfigure[Structure $\T_1$]{\label{fig:T1_result}
\input{figs/T1-result}
}
\qquad
\subfigure[Structure $\T_2$]{\label{fig:T2_result}
\input{figs/T2-result}
}
\qquad
\subfigure[Structure $\T_3$]{\label{fig:T3_result}
\input{figs/T3-result}
}
\caption{Agent performance under \BUL\ (circles) and \CL\ (triangles) schemes.
Each point represents results from $5$ experiment runs using a moving average of $100$ samples.}
\end{center}
\end{figure*}


Let us now analyse the goal-plan tree $\T_2$ shown in Figure~\ref{fig:T2}.
% %
Under such structure, a successful execution is encoded in a complex plan, in our
case plan $P$. Other options that the agent may have (e.g., plans $P_i'$) are of
less complexity, but do not lead to solutions for resolving the goal.
% %
Note that because the plan containing the solution, namely $P$, is fairly
complex, the agent needs to make several correct choices in order to arrive to a
successful execution---there are many ways the agent may fail when carrying out
$P$.
% %
Although we expected \BUL\ to yield better agent performance than \CL, the
difference was enormous in our experiments. Figure \ref{fig:T2_result} shows that
while the \BUL\ approach achieves optimal performance, which amounts to slightly
over $40\%$ rate of success, in around $300$ iterations, the \CL\ scheme,
requires close to $2500$ execution experiences. The reason is clear: since there
are more chances to fail plan $P$ initially, \CL\ marks this plan as ``bad,''
compared with the non-working plans $P_i'$. On the other hand, \BUL\ would not
jump to the conclusion that $P$ is a ``bad'' plan even when failing it, since it
is aware that decisions made below $P$ were not informed enough. Consequently,
plan $P$ will continue to be explored with \emph{equal} likelihood to plans
$P_i'$.
% %
Note that this structure shows exactly the problem discussed in the previous
section, namely, the drawbacks of assuming a plan as a bad option just because it
happened to fail, without enough consideration of the confidence on the choices
done below the plan in question.


Finally, consider the hierarchical structure $\T_3$ depicted in Figure
\ref{fig:T3}.
% %
In this case, the top-level goal $G$ has four relevant plans, which are all
``complex,'' that is, they all have several levels and multiple goals and plans.
However, given a world state, only one particular path in this hierarchy will
lead to a successful execution (of course, for different states, different
top-level plans may apply). Among other things, this means that at the top-level
the agent ought to select the right plan given the current world sate, all the
other options plans are bound to eventually fail.
% %
We argue that this is a common feature found in many BDI agent applications, in
that even though the agent has been programmed with several strategies for
resolving a goal, each one is crafted to cover uniquely a particular subset of
states. In other words, these are systems with low \emph{know-how} overlap.
% %
With respect to the two learning approaches we are considering, structure $\T_3$
provides advantages for both of them, in different parts of the tree. The \CL\
scheme is expected to learn faster the inadequacy of the non-working top-level
programs, whereas the \BUL\ would better explore, and find a solution, within the
working top-level plan. This balance was corroborated in our experiments, in
which both type of agents experienced comparable performance; see Figure
\ref{fig:T3_result}.




\subsection{Plan Applicability and Optimality}

So far, we have assumed that the agent, when faced to resolve a goal, considers
all relevant plans as \emph{applicable} too, even for plans with very low chance
of success.
% %
This implies that the, in contrast with standard BDI system, our extended
learning BDI framework will \emph{always} select a plan among the relevant
options.
% %
Because executing a plan is often not cost-free in real systems, it is likely
that an adequate plan selection mechanism would in fact \emph{not} execute plans
with too low probability of success. This, in turn, implies that the system
may, at some point, decide to fail a goal without even trying it if it
considers that the cost of failing at that point is preferrable to expected
benefits of executing some available plan.
%%
In fact, this is exactly what standards BDI systems do when no applicable plan
is found for a certain event goal: the event goal is failed right away.



This would presumably hurt \CL.

To understand the impact of such
In order to demonstrate this we modified the probabilistic plan
selection explained in Section \ref{sec:framework} so that the \JACK\
agent does not consider plans whose chance of success are below 0.2.
In this experiment we also removed the non-determinism---actions
always fail or succeed in each world state.\footnote{Managing the
non-determinism without continuing to, with some probability, try all
plans, requires a more sophisticated mechanism than a simple
probability check, to avoid randomly cutting off options that happen
to have failed on one iteration. However in the interests of
simplicity we deal with the simplified case.}
%
The differences, shown in Figure \ref{fig:T2_result} are striking.

Using the structure 
$\T_3$ we  found that whereas \BUL\ maintains its performance (and in
fact slightly improves, as failing leaf plans are discarded earlier
than before thus reaching optimal performance around $100$
iterations), the \CL\ approach is never able to learn and eventually
is guaranteed to always fail the top-level goal.

The explanation for this wrong behavior under \CL\ is as
follows. Initially, the agent tries all top-level plans for the
top-level goal. Because of their complexity, it is extremely unlikely
that the set of correct choices are made randomly. Thus their
executions fail.
This causes \CL\ to decrease the feasibility of all plans
tried, including the top-level ones. As this will likely happen
for several iterations, eventually all plans for a goal reach a probability of
success lower than required, thus
running out of options, failing the goal in question, and propagating
the failure up in the hierarchy. Eventually, the top-level plans end
up with low expectations of success and are ruled out.
This will give no options to try and the goal will always
fail.\footnote{Such behavior does not arise in the original
system because even if all plans have extremely low success chances,
the agent would pick one anyway. As a result, the successful path
would be eventually be found and plans' context conditions would start
``recovering.''}

We conclude then that overall \BUL\ clearly exhibits more robust
behaviour. In addition, in some structures \CL\ can pay significant
costs, by considering some strategies as not viable too early. In
those structures where \BUL\ performs worse, the difference is
relatively small. 



