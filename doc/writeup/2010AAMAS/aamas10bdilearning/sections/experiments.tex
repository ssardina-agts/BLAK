%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results}\label{sec:experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In order to explore the difference between \BUL\ and \CL, we set up testbed
programs composed of several goals and plans combined in a hierarchical manner
and yielding goal-plan tree structures of different shapes.\footnote{We have
implemented the learning agent system in the \JACK\ BDI platform
\cite{Busetta99jack}. The fact that \JACK\ is a Java-based system and
provides powerful meta-level reasoning capabilities, allows us to integrate \weka\ and
probabilistic plan-selection mechanisms with not much effort. Nonetheless, all
the results are independent on this and any other BDI agent system could
have been used.}
% %
In particular, we crafted goal-plan tree structures representing different
meaningful cases of BDI programs with one main top-level goal, i.e., the event to
be resolved. In addition, each structure enjoys the, generally desirable,
\emph{coverage} property, under which for every possible situation, (i.e., world
state), there is always a way of addressing the main goal, i.e., there is at
least one successful execution of the top-level event, provided the right plan
choices are made. Observe that such successful (plan) choices may be different
for different world states.
% , as know-how information is generally predicated on the situation it is
% applied in. %
When it comes to describing the possible (observable) world states, we have used
a set of logical (binary) propositions, representing the so-called fluent or
features of the environment that are observable to the agent (e.g., fluent
proposition $\mathit{DoorLocked}$ states whether the door is believed open or
not).
% %
Finally, we assume the agent is acting in a non-deterministic environment, in
which actions that are expected to succeed, may still fail with some (small)
probability. In most of our experiments, we assumed a $.1$ probability of
uncontrolled failure for such actions.\footnote{See Discussion section on how our
results generalize to a framework with world state built from non-binary fluents
and with more complex accounts for stochastic actions.}
% %




The experiments consisted in posting the top-level goal repetitively under random
world states, running the corresponding  BDI learning agent, and finally
recording whether the execution terminated successfully or not.
% %
We calculated the average rate of success of the goal every some fixed
number of iterations (in our case $20$), and investigated how such rate evolved
as the agent refined the context condition of plans.
% %%
We ran the tests under both a \BUL-based agent and a \CL-based agent,
ensuring the same sampling of random world states for each agent.

\begin{figure}[t]
\begin{center}
\input{figs/GPtree-T1}
\end{center}
\caption{Goal-plan tree hierarchical structure $\T_1$. To succeed, an agent is thus required to make
two correct choices, including selecting $P$ at the top-level.}
\label{fig:T1}
\end{figure}


From our set of experiments, we have selected three hierarchical structures that
best illustrate the results that we have obtained, namely:
% %
\begin{description}
\item[(Tree $\T_1$; Figure~\ref{fig:T1})] For each world state, the goal has
few plans that can succeed (plans $P_i$), but many other options of comparable
complexity that are bound to fail (plans $P_i'$). 
%%
Under this type of structure, an \CL-based agent will generally perform better 
than an agent using the learning \BUL\ approach.

\item[(Tree $\T_2$; Figure~\ref{fig:T2})] For each world state, the goal has
one plan that can succeed (plan $P$), and a few others that would fail.
However, the plan containing the solution is of substantial higher-complexity. 
%%
In this structure, a \BUL-based agent will outperform an \CL-based one.
% A structure in which \BUL\ is
% expected to have important advantages over \CL, since the latter may wrongly consider a
% top-level plan as a failing plan whereas there is a solution encoded
% under it. 

\item[(Tree $\T_3$; Figure~\ref{fig:T3})] This stands for a ``balanced''
structure that ends up providing different advantages for both \BUL\ and \CL\ in
different parts of the tree.
\end{description}


% In summary, we found that whereas the agent performance under the \BUL\ and \CL\
% approaches is comparable on the first and third cases, the \BUL\ scheme provides
% substantial benefits in the second case. What is more important, if we consider
% agents that may choose not to consider a plan at all when its chances of success
% are believed very low, then the \CL\ approach collapses completely whereas the
% \BUL\ is robust enough to maintain performance.

% For lack of space, we shall only give the form of tree $\T_1$ and informally
% explain the characteristics of the other two.

Let us next discuss each of the plan-goal structures and how the performance of
\BUL-based and \CL-based agents compare.


Under a structure like $\T_1$, the agent basically has several options of the
comparable complexity to resolve the top-level goal $G$ in certain world state,
in our case, $20$ options. However, most of such options---$17$ in our example;
plans $P_i'$---would inevitably lead to failure.
% %
The benefit of using the \CL\ approach comes from the fact that the agent will
decrease the probability of selecting each of those $17$ failing plans as soon as
they fail for the first time. In contrast, \BUL\ would require several failed
experiences of each of those ``bad'' top-level options before decreasing their
probability of selection---to update the decision tree of plan $P_i'$, \BUL\
requires each of their three subgoals to be ``stable.''
% %%
As we expected the \CL\ scheme did perform better in our experiments, in that it
achieved better success rate earlier; see Figure~\ref{fig:T1_result}.
% %
Note then that whereas the agent under the \CL\ approach reaches $50\%$ success
after $1000$ iterations, it takes more than double for \BUL\ to achieve that
performance.
% %
Eventually, both approaches will yield optimal performance. Observe that optimal
performance amounts to a success rate of $81\%$, as the environment fails with
probability $.1$ for every (working) action and each successful execution
involves the performance of two actions (leaf plans consist of single actions).


\begin{figure}[t]
\begin{center}
\input{figs/GPtree-T2}
\end{center}
\caption{Goal-plan tree hierarchical structure $\T_2$.}
\label{fig:T2}
\end{figure}


\begin{figure*}[t]
\begin{center}
\subfigure[Structure $\T_1$]{\label{fig:T1_result}
\input{figs/T1-result}
}
\qquad
\subfigure[Structure $\T_2$]{\label{fig:T2_result}
\input{figs/T2-result}
}
\qquad
\subfigure[Structure $\T_3$]{\label{fig:T3_result}
\input{figs/T3-result}
}
\caption{Agent performance under \BUL\ (circles) and \CL\ (triangles) schemes.
Each point represents results from $5$ experiment runs using a moving average of $100$ samples.}
\end{center}
\end{figure*}


Let us now analyse the goal-plan tree $\T_2$ shown in Figure~\ref{fig:T2}.
% %
Under such structure, a successful execution is encoded in a complex plan, in our
case plan $P$. Other options that the agent may have (e.g., plans $P_i'$) are of
less complexity, but do not lead to solutions for resolving the goal.
% %
Note that because the plan containing the solution, namely $P$, is fairly
complex, the agent needs to make several correct choices in order to arrive to a
successful execution---there are many ways the agent may fail when carrying out
$P$.
% %
Although we expected \BUL\ to yield better agent performance than \CL, the
difference was enormous in our experiments. Figure \ref{fig:T2_result} shows that
while the \BUL\ approach achieves optimal performance, which amounts to slightly
over $40\%$ rate of success, in around $300$ iterations, the \CL\ scheme,
requires close to $2500$ execution experiences. The reason is clear: since there
are more chances to fail plan $P$ initially, \CL\ marks this plan as ``bad,''
compared with the non-working plans $P_i'$. On the other hand, \BUL\ would not
jump to the conclusion that $P$ is a ``bad'' plan even when failing it, since it
is aware that decisions made below $P$ were not informed enough. Consequently,
plan $P$ will continue to be explored with \emph{equal} likelihood to plans
$P_i'$.
% %
Note that this structure shows exactly the problem discussed in the previous
section, namely, the drawbacks of assuming a plan as a bad option just because it
happened to fail, without enough consideration of the confidence on the choices
done below the plan in question.


Finally, consider the hierarchical structure $\T_3$ depicted in Figure
\ref{fig:T3}.
% %
In this case, the top-level goal $G$ has four relevant plans, which are all
``complex,'' that is, they all have several levels and multiple goals and plans.
However, given a world state, only one particular path in this hierarchy will
lead to a successful execution (of course, for different states, different
top-level plans may apply). Among other things, this means that at the top-level
the agent ought to select the right plan given the current world sate, all the
other options plans are bound to eventually fail.
% %
We argue that this is a common feature found in many BDI agent applications, in
that even though the agent has been programmed with several strategies for
resolving a goal, each one is crafted to cover uniquely a particular subset of
states. In other words, these are systems with low \emph{know-how} overlap.
% %
With respect to the two learning approaches we are considering, structure $\T_3$
provides advantages for both of them, in different parts of the tree. The \CL\
scheme is expected to learn faster the inadequacy of the non-working top-level
programs, whereas the \BUL\ would better explore, and find a solution, within the
working top-level plan. This balance was corroborated in our experiments, in
which both type of agents experienced comparable performance; see Figure
\ref{fig:T3_result}.




\subsection{Plan Applicability and Optimality}

So far, we have assumed that the agent, when faced to resolve a goal, considers
all relevant plans as \emph{applicable} too, even for plans with very low chance
of success.
% %
This implies that the, in contrast with standard BDI system, our extended
learning BDI framework will \emph{always} select a plan among the relevant
options.
% %
Because executing a plan is often not cost-free in real systems, it is likely
that an adequate plan selection mechanism would in fact \emph{not} execute plans
with too low probability of success. This, in turn, implies that the system
may, at some point, decide to fail a goal without even trying it if it
considers that the cost of failing at that point is preferable to expected
benefits of executing some available plan.
%%
In fact, this is exactly what standards BDI systems do when no applicable plan
is found for a certain event goal: the event goal is failed right away.


To understand the impact of applicability in our BDI learning framework, we
modified the probabilistic plan selection so that the agent does not consider
plans whose chances of success are deemed below a threshold; in our case we set
such threshold to $0.2$.
% %
For simplicity, we  removed the non-determinism in the model of the
environment---actions either fail or succeed in each world state.



The difference between \CL-based and \BUL-based agents performance when run with
an applicability threshold could be striking
% %
Using the structure $\T_3$ we found that whereas the \BUL\ scheme maintains its
performance (and in fact may slightly improve due to truly failing leaf plans
being ruled out earlier), the \CL\ approach may not able to learn at all and
could end up eventually failing the top-level goal \emph{always}. This is
reported in Figure \ref{fig:T3_result} (dotted lines).

The explanation for this undesirable behavior under \CL\ is as follows.
% %
Initially, the agent tries all top-level plans for the top-level goal, including
the ones containing potential successful executions. Because of their complexity,
their executions tend to fail initially. In each failure, \CL\ decreases the
feasibility of all plans tried, including the top-level ones.  After several
failures, eventually all plans for the top-level goal go below the applicability
threshold of the system (including the ``good'' plans); from now on the system
has no more applicable plans for the goal and will therefore fail it always.
% %
Observe that this behavior does not arise in the original system, because even if
all plans perform very poorly initially, the agent would always pick one anyway,
the successful path would be eventually be found, and the context decision trees
of the plans associated with such path would then start ``recovering.''


The reason why \BUL\ exhibits more robust behaviour here is that false negative
execution runs (i.e., failing executions of a plan that do encode a successful
execution) will \emph{not} be recorded.
% %
The \BUL\ account provides a \emph{confident} test---the stability test---that
will preclude that to happen and which the \CL\ unfortunately lacks.
% %
In the next section, we shall explore an alternative way for providing a
confident test that is indeed compatible with the \CL\ scheme.



