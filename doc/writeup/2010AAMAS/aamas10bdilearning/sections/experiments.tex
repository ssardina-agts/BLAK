%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results}\label{sec:experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{figure*}[t]
%\begin{center}
%\subfigure[Structures $\T_1$ (crosses) and $\T_3$ (circles)]{\label{fig:tree01_result}
%\begin{tikzpicture}[x=0.0075cm,y=4cm]
%	% DRAW AXES LINES
%    \draw[->,xshift=0] (0,0) -- coordinate (x axis mid) (1050,0);
%    \draw[->,xshift=0] (0,0) -- coordinate (y axis mid) (0,1.1);

%	% DRAW AXES NUMBERS
%    \foreach \x/\xtext in {0/0,1.5/200,3/400,4.5/600,6/800,7.5/1000}
%        \draw [xshift=0cm](\x cm,1pt) -- (\x cm,-3pt)
%            node[anchor=north] {$\xtext$};
%    \foreach \y/\ytext in {0/,1/25,2/50,3/75,4/100}
%        \draw (-1pt,\y cm) -- (0pt,\y cm) node[anchor=west] {$\ytext$};

%	% WRITE AXES LABELS
%    \node[above,xshift=3cm] at (x axis mid) {\# Iterations};
%%     \node[below=.5cm] at (x axis mid) {Iterations};
%    \node[above,shift={(0.8cm,2cm)}] at (y axis mid) {\% Success};

%	% PLOT bul DATA TEST 01
%    \draw[-] plot[mark=x,mark size=3,smooth,mark options={color=black}] 
%			file {data/previous/failtest01-bul.data};
%	% PLOT cl DATA TEST 01
%    \draw[dashed,-] plot[mark=x,mark size=3,smooth,mark options={color=black}] 
%			file {data/previous/failtest01-cl.data};

%	% PLOT bul DATA TEST 18
%    \draw[-] plot[mark=*,mark size=1,smooth,mark options={color=black}] 
%			file {data/previous/failtest18-bul.data};
%	% PLOT cl DATA TEST 18
%    \draw[dashed,-] plot[mark=*,mark size=1,smooth,mark options={color=black}] 
%			file {data/previous/failtest18-cl.data};
%\end{tikzpicture}
%}
%\qquad
%\subfigure[Structure $\T_2$]{\label{fig:tree08_result}
%\begin{tikzpicture}[x=0.00121cm,y=4cm]
%	% DRAW AXES LINES
%    \draw[->,xshift=0] (0,0) -- coordinate (x axis mid) (6300,0);
%    \draw[->,xshift=0] (0,0) -- coordinate (y axis mid) (0,1.1);

%	% DRAW AXES NUMBERS
%    \foreach \x/\xtext in {0/0,1.452/1200,2.904/2400,4.356/3600,5.808/4800,7.26/6000}
%        \draw [xshift=0cm](\x cm,1pt) -- (\x cm,-3pt)
%            node[anchor=north] {$\xtext$};
%    \foreach \y/\ytext in {0/,1/25,2/50,3/75,4/100}
%        \draw (-1pt,\y cm) -- (0pt,\y cm) node[anchor=west] {$\ytext$};

%	% WRITE AXES LABELS
%    \node[above,xshift=3cm] at (x axis mid) {\# Iterations};
%%     \node[below=.5cm] at (x axis mid) {Iterations};
%    \node[above,shift={(0.8cm,2cm)}] at (y axis mid) {\% Success};
%%     \node[left=0.5cm,rotate=90,xshift=1cm] at (y axis mid) {Success Income};

%	% PLOT STABLE DATA
%    \draw[-] plot[mark=*,mark size=0,smooth,mark options={color=black}] 
%			file {data/previous/failtest08-bul.data};
%	% PLOT STABLE DATA
%    \draw[dashed,-] plot[mark=x,mark size=0,smooth,mark options={color=black}] 
%			file {data/previous/failtest08-cl.data};
%\end{tikzpicture}
%}
%\caption{Agent performance under \BUL\ and \CL\ schemes. Solid lines
%represent agent performance under the \BUL\ approach; dashed lines represent agent performance under
%the \CL\ approach. Each point stands for the average of the last $20$ executions.}
%\end{center}
%\end{figure*}

	

In order to explore the difference between \BUL\ and \CL, we set up testbed
programs composed of several goals and plans combined in a hierarchical manner
and yielding goal-plan tree structures of different shapes.\footnote{We have
implemented the learning agent system in the \JACK\ BDI platform
\cite{Busetta99jack}. The fact that \JACK\ is a Java-based system and
provides powerful meta-level reasoning capabilities, allows us to integrate \weka\ and
probabilistic plan-selection mechanisms with not much effort. Nonetheless, all
the results are independent on this and any other BDI agent system could
have been used.}
% %
In particular, we crafted goal-plan tree structures representing different
meaningful cases of BDI programs with one main top-level goal, i.e., the event to
be resolved. In addition, each structure enjoys the, generally desirable,
\emph{coverage} property, under which for every possible situation, (i.e., world
state), there is always a way of addressing the main goal, i.e., there is at
least one successful execution of the top-level event, provided the right plan
choices are made. Observe that such successful (plan) choices may be different
for different world states.
% , as know-how information is generally predicated on the situation it is
% applied in. %
When it comes to describing the possible (observable) world states, we have used
a set of logical (binary) propositions, representing the so-called fluent or
features of the environment that are observable to the agent (e.g., fluent
proposition $\mathit{DoorLocked}$ states whether the door is believed open or
not).
% %
Finally, we assume the agent is acting in a non-deterministic environment, in
which actions that are expected to succeed, may still fail with some (small)
probability. In most of our experiments, we assumed a $.1$ probability of
uncontrolled failure for such actions.\footnote{See Discussion section on how our
results generalize to a framework with world state built from non-binary fluents
and with more complex accounts for stochastic actions.}
% %




The experiments consisted in posting the top-level goal repetitively under random
world states, running the corresponding  BDI learning agent, and finally
recording whether the execution terminated successfully or not.
% %
We calculated the average rate of success of the goal every some fixed
number of iterations (in our case $20$), and investigated how such rate evolved
as the agent refined the context condition of plans.
% %%
We ran the tests under both a \BUL-based agent and a \CL-based agent,
ensuring the same sampling of random world states for each agent.

\begin{figure}[t]
\begin{center}
\input{figs/GPtree-T1}
\end{center}
\caption{Goal-plan tree hierarchical structure $\T_1$. To succeed, an agent is thus required to make
two correct choices, including selecting $P$ at the top-level.}
\label{fig:T1}
\end{figure}


From our set of experiments, we have selected three hierarchical structures that
best illustrate the results that we have obtained, namely:
% %
\begin{description}
\item[(Tree $\T_1$; Figure~\ref{fig:T1})] For each world state, the goal has
few plans that can succeed (plans $P_i$), but many other options of comparable
complexity that are bound to fail (plans $P_i'$). 
%%
Under this type of structure, an \CL-based agent will generally perform better 
than an agent using the learning \BUL\ approach.

\item[(Tree $\T_2$; Figure~\ref{fig:T2})] For each world state, the goal has
one plan that can succeed (plan $P$), and a few others that would fail.
However, the plan containing the solution is of substantial higher-complexity. 
%%
In this structure, a \BUL-based agent will outperform an \CL-based one.
% A structure in which \BUL\ is
% expected to have important advantages over \CL, since the latter may wrongly consider a
% top-level plan as a failing plan whereas there is a solution encoded
% under it. 

\item[(Tree $\T_3$; Figure~\ref{fig:T3})] This stands for a ``balanced''
structure that ends up providing different advantages for both \BUL\ and \CL\ in
different parts of the tree.
\end{description}


% In summary, we found that whereas the agent performance under the \BUL\ and \CL\
% approaches is comparable on the first and third cases, the \BUL\ scheme provides
% substantial benefits in the second case. What is more important, if we consider
% agents that may choose not to consider a plan at all when its chances of success
% are believed very low, then the \CL\ approach collapses completely whereas the
% \BUL\ is robust enough to maintain performance.

% For lack of space, we shall only give the form of tree $\T_1$ and informally
% explain the characteristics of the other two.

Let us next discuss each of the plan-goal structures and how the performance of
\BUL-based and \CL-based agents compare.


Under a structure like $\T_1$, the agent basically has several options of the
comparable complexity to resolve the top-level goal $G$ in certain world state,
in our case, $20$ options. However, most of such options---$17$ in our example;
plans $P_i'$---would inevitably lead to failure.
% %
The benefit of using the \CL\ approach comes from the fact that the agent will
decrease the probability of selecting each of those $17$ failing plans as soon as
they fail for the first time. In contrast, \BUL\ would require several failed
experiences of each of those ``bad'' top-level options before decreasing their
probability of selection---to update the decision tree of plan $P_i'$, \BUL\
requires each of their three subgoals to be ``stable.''
% %%
As we expected the \CL\ scheme did perform better in our experiments, in that it
achieved better success rate earlier; see Figure~\ref{fig:T1_result}.
% %
Note then that whereas the agent under the \CL\ approach reaches $50\%$ success
after $1000$ iterations, it takes more than double for \BUL\ to achieve that
performance.
% %
Eventually, both approaches will yield optimal performance. Observe that optimal
performance amounts to a success rate of $81\%$, as the environment fails with
probability $.1$ for every (working) action and each successful plan involves
two actions.


\begin{figure}[t]
\begin{center}
\input{figs/GPtree-T2}
\end{center}
\caption{Goal-plan tree hierarchical structure $\T_2$.}
\label{fig:T2}
\end{figure}


\begin{figure*}[t]
\begin{center}
\subfigure[Structure $\T_1$]{\label{fig:T1_result}
\input{figs/T1-result}
}
\qquad
\subfigure[Structure $\T_2$]{\label{fig:T2_result}
\input{figs/T2-result}
}
\qquad
\subfigure[Structure $\T_3$]{\label{fig:T3_result}
\input{figs/T3-result}
}
\caption{Agent performance under \BUL\ (circles) and \CL\ (triangles) schemes.
Each point represents results from $5$ experiment runs using a moving average of $100$ samples.}
\end{center}
\end{figure*}


The second structure $\T_2$ that we considered amounts to simplifying
each plan $P_i'$ to be just a single action that always fails and
making the goal-tree hierarchy below plan $P$ more complex, that is,
deeper and with more goals.\footnote{For lack of space, we do not show
this structure.}
% \footnote{For lack of space, we do not show
% this structure, but will be included in the final version.}
%
Under such hierarchy, the agent needs to make four correct plan
choices to result in a successful execution; there are also many
chances for the agent to fail under $P$.
%
Although one would expect \BUL\ to yield better agent performance than
\CL, the difference is enormous. Figure \ref{fig:T2_result} shows
that while the \BUL\ approach, achieves optimal performance in around
$100$ iterations, the \CL\ scheme, requires around $5000$ execution
experiences.  
%
The reason is clear: since there are more chances to fail plan $P$
initially, \CL\ marks this plan ``bad,'' causing plans $P_i'$ (which
are all non-working) to be selected at least once in preference to
trying $P$ again. On the other hand, \BUL\ would not consider $P$
``bad'' even when failing it, since it is aware that decisions made 
below were not informed enough. Consequently $P$ will continue to be
explored with equal likelihood to the $P_i'$.
% *** Sentence below could be cut - its an aside...
%
As a matter of fact,
provided adequate parameters are used for checking stability, \BUL\
would only record failed execution traces in plan $P$ if the
environment happened to fail unexpectedly: if the decisions were
informed and the environment cooperated, then the execution is
expected to succeed. 
%
Notice that optimal behavior amounts to less than a $75\%$ rate of
success, since the agent needs to ultimately perform four actions,
each of them having a probability of success of $90\%$ when performed
in the real world. 

Let us now consider the third hierarchical structure $\T_3$, depicted
in Figure \ref{fig:T3}. In this case, the top-level goal $G$ has
five relevant plans, which are all ``complex,'' that is, they all have
several levels and multiple goals and plans. However, only one
particular path in this hierarchy will lead to a successful 
execution for a particular world state. 
Among other things, this means that at the top-level the agent ought
to select the right plan, all the other four plans are bound to fail
eventually. 
%
We argue that this is the typical case in most BDI agent systems, in
that for each goal, the agent may have several strategies, but each
one is crafted to cover uniquely a particular subset of states.
%
From the two learning approaches we are considering, structure $\T_3$
provides advantages for both of them, in different parts of the
tree. The \CL\ scheme is expected to learn faster the inadequacy 
of the four non-working top-level programs, but the \BUL\ would better
explore, and find a solution, within the working top-level plan.
%
This balance is corroborated by the fact that both agents have
comparable performance, with \BUL\ yielding improved behavior slightly
quicker (see Figure \ref{fig:T1_result}, circle marked). 

However, we are currently considering all plans as potentially
applicable and worth trying - even when there is a very low chance of
success. Given that executing a plan is often not cost-free in real
systems, it is likely that the plan selection mechanism would in fact
not execute plans with too low a probability of success. This would
presumably hurt \CL.
% 
In order to demonstrate this we modified the probabilistic plan
selection explained in Section \ref{sec:framework} so that the \JACK\
agent does not consider plans whose chance of success are below 0.2.
In this experiment we also removed the non-determinism---actions
always fail or succeed in each world state.\footnote{Managing the
non-determinism without continuing to, with some probability, try all
plans, requires a more sophisticated mechanism than a simple
probability check, to avoid randomly cutting off options that happen
to have failed on one iteration. However in the interests of
simplicity we deal with the simplified case.}
%
The differences, shown in Figure \ref{fig:T2_result} are striking.

Using the structure 
$\T_3$ we  found that whereas \BUL\ maintains its performance (and in
fact slightly improves, as failing leaf plans are discarded earlier
than before thus reaching optimal performance around $100$
iterations), the \CL\ approach is never able to learn and eventually
is guaranteed to always fail the top-level goal.

The explanation for this wrong behavior under \CL\ is as
follows. Initially, the agent tries all top-level plans for the
top-level goal. Because of their complexity, it is extremely unlikely
that the set of correct choices are made randomly. Thus their
executions fail.
This causes \CL\ to decrease the feasibility of all plans
tried, including the top-level ones. As this will likely happen
for several iterations, eventually all plans for a goal reach a probability of
success lower than required, thus
running out of options, failing the goal in question, and propagating
the failure up in the hierarchy. Eventually, the top-level plans end
up with low expectations of success and are ruled out.
This will give no options to try and the goal will always
fail.\footnote{Such behavior does not arise in the original
system because even if all plans have extremely low success chances,
the agent would pick one anyway. As a result, the successful path
would be eventually be found and plans' context conditions would start
``recovering.''}

We conclude then that overall \BUL\ clearly exhibits more robust
behaviour. In addition, in some structures \CL\ can pay significant
costs, by considering some strategies as not viable too early. In
those structures where \BUL\ performs worse, the difference is
relatively small. 



