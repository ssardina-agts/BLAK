%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Context Learning}\label{sec:context_learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\success}{\mbox{\emph{succ}}}
\newcommand{\failure}{\mbox{\emph{fail}}}

With the classical BDI programming framework extended with decision trees and a
probabilistic plan selection scheme, we are now ready to develop mechanisms for
learning context decision trees and hopefully improving over time the selection
of plans based on experience.
% %
To that end, in this section, we explore two approaches for learning the context
condition of plans.



Recall that our ultimate objective is to learn which plans are best for achieving
a particular goal event in the various world states that may ensue. Given that,
in this work, we have no measure of cost for plans,\footnote{This could also be a
useful addition, but is not part of standard BDI programming languages.} a good
plan for a given world state is simply one which (usually) succeeds in such
state. In order to learn the context decision tree for a plan, the agent keeps
track of previous experiences she has had when running the plan in question. More
precisely, if a plan $P$ is tried in world state $w$ with certain outcome $o \in
\{\success,\failure\}$, the agent may record the tuple $\tuple{w,o}$ as part of
the training set for plan $P$.
% %
Interestingly, while it is always meaningful to record successful executions,
some failures may not be worth recording. Based on this observation, we shall
develop and compare two different algorithms that differ on how past experiences
are taken into account by the agent. Before then, though, let us explain better
this issue by means of an example.
 

\begin{figure}[t]
\begin{center}
\input{figs/GPtree-T3}
\end{center}
\caption{Goal-Plan hierarchy $\T_3$. There are $2^4$ worlds whose solutions are distributed evenly in each of the $4$ top level plans. Successful execution trace is of length $4$. Within each sub-tree $P_i$ \BUL\ is expected to perform better for a given world, but it suffers in the number of worlds. Overall, \CL and \BUL\ perform equally well in this structure.}

% \caption{Goal-plan tree hierarchy. Rounded boxes stand for event goals; rectangles for plans.}
%\caption{Goal-plan tree hierarchy $\T_3$. Rounded boxes stand for event goals; rectangles for plans. Top-level goal $G$ as $k$ relevant plans, each of them with a goal-plan tree structure below them as shown for plan $P_i$.}
%
% \caption{Goal-plan tree hierarchy $\T_3$. Rounded boxes stand for event goals; rectangles for
% plans. Leaf plans are assumed to directly succeed or fail when executed in the environment, and
% are marked accordingly. 
% %
% An edge with a label $\times n$ states that there are $n$ edges of such type (e.g.,  goal $G$ has
% $5$ relevant plans). Indexes are used to represent different goals/plans under such labeled edges.
% For instance, below $P_1$, the indexes are understood as $i \in \{1,2\}$, $j \in \{1,\ldots,3\}$,
% and $k \in \{2,\ldots,5\}$.
% %
% To succeed, an agent should execute $6$ working plans, namely, $P_{1111}^1$, $P_{1112}^1$, and 
% $P_{1113}^1$ to resolve goal $G_1^1$, and $P_{1211}^1$, $P_{1212}^1$, and 
% $P_{1213}^1$ to resolve goal $G_1^2$.}
\label{fig:T3}
\end{figure}



Consider the example in Figure \ref{fig:T3}.
% %
Suppose that in some execution, plan $P_\ell$, for some $\ell \in \{1,\ldots,k\}$
is selected in order to resolve top-goal $G$ in some world state $w_1$. The plan
involves, in turn, the successful resolution of three sequential goals $G_a$,
$G_b$, and finally $G_c$. Suppose further that subgoal $G_1$ has been resolved
successfully, yielding new state $w_2$, and that plan $P_b^2$ has been chosen
next to try and achieve the second subgoal $G_b$.
% %
Suppose next that the execution of plan $P_b^2$, which may involve more subgoals,
happens to \emph{fail}. As we have no failure recovery, this will immediately
cause goal $G_b$ itself to fail, and recursively will cause failure at each level
until top-level goal $G$ itself fails.
% %
Clearly, the failure should be recorded in the decision tree of the plan where
the failure originated, namely, plan $P_b^2$. However, it is not so clear, as
will be discussed next, whether the failure should also be recorded in the
decision trees for plans higher up in the hierarchy, for example, in plan
$P_\ell$ (the one selected to address $G$).


In order to discuss further which data should be recorded where, we define the
notion of an \textit{active execution trace}, as a sequence of the form
$G_0[P_0:w_0] \cdot G_1[P_1:w_1] \cdot \ldots \cdot G_n[P_n:w_n]$, which
represents the sequence of currently active goals, along with the plans which
have been selected to achieve each of them, and the world state in which the
selection was made---plan $P_i$ has been selected in world state $w_i$ in order
to achieve the $i$-th active subgoal $G_i$.
% %
In our example, the trace at the moment of failure is as follows:
\[ \lambda=G[P_\ell:w_1] \cdot G_b[P_b^2:w_2]. \]


So, when the final goal of $\lambda$ fails, namely $G_b$, it is clear that the
decision tree of the plan being used to achieve that goal ought to be updated, in
the example, a failure should be recorded for the world state $w_2$ for the
decision tree attached to plan $P_2^b$.  Although it may be the case that the
plan usually succeeds in the situation in which it was chosen, and failure is
simply due to some non-determinism (or in the general case, actions of other
agents, interactions, etc.), there is no way to determine this and the learning
process will eventually recognise such cases as ``noise.''
% %
However, should the decision trees of plans associated with earlier goals in
$\lambda$ (e.g., the one for $P_\ell$) be updated?  The point is that it is
conceivable that the failure of goal $G_b$ could have been avoided, had an
alternative plan, say $P_b^1$, been chosen instead. If this is the case, then
recording a failure against plan $P_\ell$ would not not be justified.
% %
Informally, one could argue that it is best to wait before recording failures
against a plan until one is reasonably confident that subsequent choices down the
goal-plan tree hierarchy were ``well informed.'' In order words, if the agent
knows that the plan selection for goal $G_b$ was as good and informed as
possible, then recording the failure for world $w_1$ in plan $P_\ell$ would
also be justified.


\newcommand{\procedurefont}[1]{\mathsf{#1}}
\newcommand{\StableGoal}{\procedurefont{StableGoal}}
\newcommand{\RecordTrace}{\procedurefont{RecordFailedTrace}}
\newcommand{\RecordWorldDT}{\procedurefont{RecordWorldDT}}


The judgment as to whether the decisions made were sufficiently ``well
informed,'' is however not a trivial one.  A plan $P$ is considered to be
\emph{stable} for a particular world state $w$ if the rate of success of $P$ in
$w$ is changing below a certain threshold $\epsilon$.
% %
In such a case, the agent can start to build confidence about the applicability
level of $P$.
% % We also allow specification of a minimum number of execution experiences for
% $P$ in $w$, in order to have the change of rate of success be sufficiently
% meaningful. %
The stability notion extends to goals in the expected way: a goal is considered
\emph{stable} for world state $w$ if all its relevant plans are stable for $w$. 
%%
When a goal is stable, we regard the plan selection for such goal as a ``well
informed'' one. Thus, a plan failure is recorded in the plan if the subgoal
that caused the plan failure is is stable for the world it was meant to be
executed. In our example, we shall record the failure if goal $G_b$ is stable
in world state $w_2$---the selection of option $P_b^2$ was an informed one.

The following algorithm shows how a failure run is recorded in an active goal
trace $\lambda$. Function $\StableGoal(G,w,k,\epsilon)$ returns true iff goal $G$
is considered \textit{stable} for world state $w$, under change of success rate
thresholds $0 < \epsilon \leq 1$ and minimal number of executions $k \geq 0$.
 
 \renewcommand{\algorithmiccomment}[1]{\hfill \texttt{\small // #1}}
 \newcommand{\assign}{\mbox{:=\ }}
 \begin{algorithm}[h]
	\caption{$\RecordTrace(\lambda,k,\epsilon)$}\label{algo:record_failed_exec}
	\label{alg:NDS}
  \begin{algorithmic}[1]
    \REQUIRE $\lambda=G_0[P_0:w_0] \cdot \ldots \cdot G_n[P_n:w_n]$; $k\geq0$;
    $\epsilon > 0$ \ENSURE Propagates DT updates for plans

	\STATE $\RecordWorldDT(P_n,w_n,\failure)$

    \IF{$\StableGoal(G_n,w_n,k,\epsilon) \land |\lambda|>1$}
    	 \STATE $\lambda' \assign G_0[P_0:w_0] \cdot \ldots \cdot
    				G_{n-1}[P_{n-1}:w_{n-1}]$
    	\STATE $\RecordTrace(\lambda',k,\epsilon)$ \COMMENT{recursive call}
    \ENDIF
  \end{algorithmic}
\end{algorithm}

 
The algorithm starts by recording the failure against the last plan $P_n$ in the
trace.
% %
Next, if the choice of executing plan $P_n$ to achieve goal $G_n$ was indeed an
informed one (that is, goal $G_n$ was stable for $w_n$), then the procedure
should be repeated for the previous goal-plan nodes, if any.
% %
If, on the other hand, the last goal $G_n$ in the trace is not deemed stable
enough, the procedure terminates and no more failure data is assimilated.
% %
Observe that, in order to update the decision tree of a certain plan that was
chosen along the execution, it has to be the case that the (failed) decisions
taken during execution must have all been informed ones.


So, in the remaining of the paper, we shall consider two learning approaches. The
first, which we call \emph{aggressive concurrent learning} (\CL), corresponds to
the more traditional approach where all data is always assimilated by the
learner, that is, we take $\epsilon = 1$ and $k = 0$. In other words, every plan
and every goal is always considered stable and, as a result, a failure in a plan
is always recorded. The assumption is that misleading information, as
discussed above, will eventually disappear as noise.
% %
The second one, which we refer to as \emph{bottom-up learning} (\BUL), is more
cautious and records a failure execution experience when some stability on
decisions is deemed, that is, $\epsilon > 0$. In our work, we have taken
$\epsilon = 0.3$ and $k = 3$; although these values may seem very close to the
ones used by \CL, they do indeed have a substantial impact on the results.
%%
Note that the lower $\epsilon$ is and the higher $k$ is, the more
conservative the agent will be in considering its decisions ``well informed.'' 
%
% With $\epsilon$ = 1 and
% k = 0 we obtain a more standard learning approach where all information is
% accumulated, and the assumption is made that faulty information will eventually
% disappear as noise.

In the following section, we shall explore these two approaches against
different programs with different structures.




