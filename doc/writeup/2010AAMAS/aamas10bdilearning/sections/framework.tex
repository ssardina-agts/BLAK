%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The BDI Learning Framework}\label{sec:framework}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The problem that we are interested in is as follows: \emph{given past execution
data, determine which plans are applicable in the current world state and which
of those is to be executed}.

To address this ``learnable'' plan-selection task, we start by modeling the
context condition of plans with \emph{decision trees}, rather than with logical
formulas.\footnote{Of course, we can combine both formulas and decisions
tress...}
% %
Decision trees~\cite{Mitchell97:ML}  provide a natural classification mechanism
for our purposes, as they can deal with noise (due to non-deterministic
environments, for example), and they are able to support disjunctive hypotheses.
They are also readily convertible to rules, which are the standard representation
for context conditions in BDI systems.



In a nutshell, each plan in the agent's library is associated with a decision
tree that will classify world states into an expectation of whether the plan will
succeed or fail. By inducing plans' decision tress based on previous executions,
a plan's decision tree will thus give the agent information for each relevant
plan, in a particular world state, regarding how likely it is to succeed/fail
based on the agent's past experiences.


Given this new context for BDI programming, there are two issues that ought to be
addressed.
% %%%
First, one has to decide \emph{when and what kind of execution data the agent should
collect in order to be able to ``learn'' (useful) decision trees for plans}.
Roughly speaking, data is collected regarding whether a plan is considered to
have succeeded or failed in the world state it has been selected for execution.
% %
\textbf{For example, in Figure~\ref{fig:T3}, if plan XXX in selected in world
$w$} %%
% %
Whereas successful executions are always recorded, the recording of failure runs
of a plan may be subject to some analysis; this is the topic of the following section.


The second issue to be addressed is how to use the plans' decision trees for plan
selection. More concretely: \emph{given a goal to be resolved and a set of
relevant plans with their corresponding context decision trees, what plan should
the agent commit for execution?}
% %%
Typical BDI platforms offer various mechanisms for plan selection, including plan
precedence and meta-level reasoning. However, these mechanisms are pre-programmed
and do not take into account the experience of the agent.
% %
In our framework for context learning, we must consider the standard dilemma of
\emph{exploitation} vs \emph{exploration}. To that end, we use a natural plan
selection approach in which plan are selected using a probabilistic function,
which chooses a plan with a probability proportional to its relative expected
success (in the world state of interest). Moreover, in
Section~\ref{sec:coverage}, we discuss how to further enhance such plan selection
account by also talking into account how much each candidate plan has been
explored, relative to its ``complexity.''


For the purpose of our analysis, in this work, we have used algorithm
\textsf{J48}, a version of \textsf{c4.5} \cite{Mitchell97:ML}, from the
well-known \textsf{weka} learning package \cite{weka99}. Consequently, we
recreate decision trees, from scratch, after each new piece of data is collected.
% %
Of course, for a real-world implementation, one should appeal to algorithms for
\emph{incremental} induction of the decision tree, such as those described in
\cite{Swere06:Fast,Utgoff97Decision}.
% %
%Observe that any algorithm for inducing a decision tree from a training set (in
%our case, the set of outcomes for different world states), would struggle for
%both classification accuracy as well as a compact representation (i.e., a smaller
%tree). Thus, algorithms like \textsf{J48}  may trade miss-classification of some
% past execution data (i.e., world states together with their final
%success/failure outcomes) for the sake of yielding a more compact decision tree.
%% %
%Nonetheless, together with the final success/failure classification at each leaf
%node, the number of both correct and incorrect classifications relative to
%the training set is also recorded. This shall provide then an
%estimate of therate of success of the plan for the world states
%classified under that leafnode. 
%

The algorithm for computing the decision tree aims to balance
compactness of representation with accuracy. Information
is maintained in the decision tree about the number of successes and
failures for a given leaf node (representing a set of world states in
our case) that have been encountered.
We then use this information to estimate the rate of success of the
plan to which this decision tree is attached, for the world states
represented by the given leaf node - and in particular the world state
currently under consideration.

Finally, we should point out a number of assumptions that were made in order to
be able to explore the basic issues we are concerned with.
% %
First, we assume that actions in the environment may fail with some probability
($1$ if the action is not possible in a world sate).
% %
Second, we assume no changes happen in the environment, during execution, other
than those made by the agent. Although this may appear too limiting, the fact
that actions may fail with some probability mitigates against the
over-simplification for many cases, by treating other changes as
non-deterministic failures.
% %
Third, we consider the execution of a single intention; learning in the context
of multiple, possibly interacting, intentions poses other extra challenges that
would complicate our study substantially (see \cite{Thangarajah02}).
% % %
Finally, we assume no automated failure handling, whereby the BDI system retries
a goal with alternative options, if the selected plan happens to fail. 
Integrating failure handling would not alter any of our results, but it wouldcomplicate our implementation framework and the understanding of the basicmechanisms.


% These assumptions are,
% namely:
% \begin{enumerate}
% \item
% Actions in the environment may fail with some probability, even in
% world states where they ''should'' succeed. 
% This captures the non-deterministic nature of most real
% world environments. 
% \item
% No changes happen in the environment, during execution, other than
% those made by the agent. (Although this may appear too
% limiting, the previous assumption of non-determinism mitigates against
% the over-simplification for many cases, by treating other changes as
% non-deterministic failures.)
% \item
% The agent is only executing a single intention (i.e., pursuing a single
% goal). 
% %
% Without this assumption it would be necessary for the agent to
% reason about potentially negative interactions between concurrent goals. While such reasoning is
% possible
% \cite{Thangarajah02}, it complicates the situation for this work.
% 
% 
% 
% \item
% There is no automated failure handling, whereby the BDI system retries
% a goal with an alternative plan, if the selected plan fails. This will
% certainly be integrated into the system for real use, but again
% complicates the understanding of the basic mechanisms. In fact,
% its integration may help achieve faster learning.
% %  for real applications. 
% \end{enumerate}



