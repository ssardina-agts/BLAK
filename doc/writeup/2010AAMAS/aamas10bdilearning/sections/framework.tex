%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A BDI Learning Framework}\label{sec:framework}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The problem that we are interested in is as follows: \emph{given past execution
data and the current world state, determine which plan to execute next 
in order to address an event}.


To address this ``learnable'' plan-selection task, we start by modeling the
context condition of plans with \emph{decision trees}, rather than with logical
formulas.\footnote{The logical formulae of the classical BDI framework can of course be combined with \dt s.}
% %
Decision trees~\cite{Mitchell97:ML}  provide a natural classification mechanism
for our purposes, as they can deal well with noise (generally due to partially
observable and predictable environments), and they are able to support
disjunctive hypotheses. They are also readily convertible to rules, which are the
standard representation for context conditions in BDI systems.


We associate each plan in the agent's library with a \dt\ that classifies world
states into an expectation of whether the plan will succeed or fail. Then for
each relevant plan, its \dt\ (induced based on previous executions)
gives the agent information regarding how likely it is to succeed/fail in a
particular world state.


Given this new context for BDI programming, there are two issues that ought to be
addressed.
% %%%
First, one has to decide \emph{when and what kind of execution data the agent should
collect in order to be able to ``learn'' (useful) decision trees for plans}.
Roughly speaking, data is collected regarding whether a plan is considered to
have succeeded or failed in the world for which it was selected.
% %
  Whereas successful executions are always recorded, the recording of failure
runs of a plan may be subject to some analysis; this is the topic of the following section.



The second issue to be addressed is how to use the plans' decision trees for plan
selection. More concretely: \emph{given a goal to be resolved and a set of
relevant plans with their corresponding context decision trees, what plan should
the agent commit for execution?}
% %%
Typical BDI platforms offer various mechanisms for plan selection, including plan
precedence and meta-level reasoning. However, these mechanisms are pre-programmed
and do not take into account the experience of the agent.
% %
In our framework for context learning, we must consider the standard dilemma of
\emph{exploitation} vs \emph{exploration}. To that end, we use an 
approach in which plans are selected with a probability proportional to their relative expected
success (in the world state of interest). Later, in
Section~\ref{sec:coverage}, we discuss how to further enhance such plan selection
by considering how much each candidate plan has been
explored relative to its ``complexity.''



For the purpose of our analysis, we have used algorithm
\propername{J48}, a version of \propername{c4.5} \cite{Mitchell97:ML}, from the
well-known \weka\ learning package \cite{weka99}. Currently we
recreate decision trees from scratch after each new outcome is recorded.
% %
Of course, for a real-world implementation, one should appeal to algorithms for
\emph{incremental} induction of the decision tree, such as those described in
\cite{Swere06:Fast,Utgoff97Decision}.
% %

The \weka\ \propername{J48} algorithm for inducing \dt{}s aims to balance
compactness of representation with accuracy. Consequently, it maintains in each
\dt\ information about the number of instances (or world states in our case) from
the training data correctly and incorrectly classified by each decision tree leaf
node. Subsequently, whenever a plan's \dt\ is used to classify a new instance
(world state), weka returns not only the classification (i.e. success or
failure), but also a classification probability (i.e. to what degree it believes
that the classification is correct). We then use this probability as an estimate
of the plan's chances of success for the world in question.


Finally, we should point out a number of assumptions that were made in order to
focus on the core issues we are concerned with.
% %
First, we assume that actions in the environment may fail with some probability
(if an action is not possible in a world state this probability is $1.0$).
% %
% DSINGH#1: Based on review comments, Dhirendra changed this:
%Second, we assume no external changes in the environment during execution other
%than those made by the agent. Although this may appear too limiting, the fact
%that actions may fail with some probability mitigates against
%over-simplification by capturing external changes as
%non-deterministic failures.
% DSINGH#1: to this:
This is a simple way to capture non-deterministic failures caused either by 
imperfect execution or external changes in the environment. A success on the
other hand is always attributed only to the agent's actions.
% DSINGH#1: end change
% %
Second, we consider the execution of a single intention; learning in the context
of multiple, possibly interacting, intentions poses other extra challenges that
would complicate our study substantially (see \cite{Thangarajah02}).
% % %
Lastly, we assume no automated failure handling, whereby the BDI system retries
a goal with alternative options if the selected plan happens to fail. 
% DSINGH#2: Based on review comments, Dhirendra changed this:
%Integrating failure handling would not alter any of our results, but it would
%complicate our implementation framework and the understanding of the basic
%mechanisms.
% DSINGH#2: to this:
Integrating failure handling would complicate our implementation framework and the understanding of the basic mechanisms. For instance, if an alternative plan were to succeed after the initial failure then care must be taken in propagating this outcome to the parent as the success may have been caused precisely because the first choice failed in a way that enabled the second one to succeed.
% DSINGH#2: end change



