%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A BDI Learning Framework}\label{sec:framework}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The problem that we are interested in is as follows: \emph{given past execution
data and the the current world state, determine which plan to execute next 
in order to address an event}.


To address this ``learnable'' plan-selection task, we start by modeling the
context condition of plans with \emph{decision trees}, rather than with logical
formulas.\footnote{As is the case in the classical BDI framework. Of course, we can combine both formulas and decisions
trees...}
% %
Decision trees~\cite{Mitchell97:ML}  provide a natural classification mechanism
for our purposes, as they can deal with noise (due to non-deterministic
environments, for example), and they are able to support disjunctive hypotheses.
They are also readily convertible to rules, which are the standard representation
for context conditions in BDI systems.




We associate each plan in the agent's library with a \dt
that will classify world states into an expectation of whether the plan will
succeed or fail. Then for each relevant plan, the plan's \dt (induced based on previous executions) will give the agent information regarding how likely it is to succeed/fail in a particular world state.


Given this new context for BDI programming, there are two issues that ought to be
addressed.
% %%%
First, one has to decide \emph{when and what kind of execution data the agent should
collect in order to be able to ``learn'' (useful) decision trees for plans}.
Roughly speaking, data is collected regarding whether a plan is considered to
have succeeded or failed in the world for which it was selected.
% %
\textbf{For example, in Figure~\ref{fig:T3}, if plan XXX in selected in world
$w$} %%
% %
Whereas successful executions are always recorded, the recording of failure runs
of a plan may be subject to some analysis; this is the topic of the following section.



The second issue to be addressed is how to use the plans' decision trees for plan
selection. More concretely: \emph{given a goal to be resolved and a set of
relevant plans with their corresponding context decision trees, what plan should
the agent commit for execution?}
% %%
Typical BDI platforms offer various mechanisms for plan selection, including plan
precedence and meta-level reasoning. However, these mechanisms are pre-programmed
and do not take into account the experience of the agent.
% %
In our framework for context learning, we must consider the standard dilemma of
\emph{exploitation} vs \emph{exploration}. To that end, we use an 
approach in which plans are selected with a probability proportional to their relative expected
success (in the world state of interest). Moreover, in
Section~\ref{sec:coverage}, we discuss how to further enhance such plan selection
account by considering how much each candidate plan has been
explored, relative to its ``complexity.''



For the purpose of our analysis, in this work, we have used algorithm
\propername{J48}, a version of \propername{c4.5} \cite{Mitchell97:ML}, from the
well-known \weka\ learning package \cite{weka99}. Currently we
recreate decision trees from scratch after each new piece of data is collected.
% %
Of course, for a real-world implementation, one should appeal to algorithms for
\emph{incremental} induction of the decision tree, such as those described in
\cite{Swere06:Fast,Utgoff97Decision}.
% %
%Observe that any algorithm for inducing a decision tree from a training set (in
%our case, the set of outcomes for different world states), would struggle for
%both classification accuracy as well as a compact representation (i.e., a smaller
%tree). Thus, algorithms like \textsf{J48}  may trade miss-classification of some
% past execution data (i.e., world states together with their final
%success/failure outcomes) for the sake of yielding a more compact decision tree.
%% %
%Nonetheless, together with the final success/failure classification at each leaf
%node, the number of both correct and incorrect classifications relative to
%the training set is also recorded. This shall provide then an
%estimate of the
%rate of success of the plan for the world states
%classified under that leaf
%node.
 
%

%dsingh-:
%The algorithm for computing the decision tree aims to balance
%compactness of representation with accuracy. Information
%is maintained in the decision tree about the number of successes and
%failures for a given leaf node (representing a set of world states in
%our case) that have been encountered.
%We then use this information to estimate the rate of success of the
%plan to which this decision tree is attached, for the world states
%represented by the given leaf node - and in particular the world state
%currently under consideration.
%dsingh+:
The \weka\ \propername{J48} algorithm for inducing {\dt}s aims to balance compactness of representation with accuracy. Consequently, it maintains in each \dt information about the number of instances (or world states in our case) from the training data correctly and incorrectly classified by each decision leaf node. Subsequently, whenever a plan's \dt is used to classify a new instance (world state), weka returns not only the classification (i.e. success or failure), but also a classification probability (i.e. to what degree it believes that the classification is correct). We then use this probability as an estimate of the plan's chances of success for the world in question.


Finally, we should point out a number of assumptions that were made in order to
focus on the core issues we are concerned with.
% %
First, we assume that actions in the environment may fail with some probability
(if an action is not possible in a world state this probability is $1.0$).
% %
Second, we assume no external changes in the environment during execution other
than those made by the agent. Although this may appear too limiting, the fact
that actions may fail with some probability mitigates against
over-simplification by capturing external changes as
non-deterministic failures.
% %
Third, we consider the execution of a single intention; learning in the context
of multiple, possibly interacting, intentions poses other extra challenges that
would complicate our study substantially (see \cite{Thangarajah02}).
% % %
Lastly, we assume no automated failure handling, whereby the BDI system retries
a goal with alternative options, if the selected plan happens to fail. 
Integrating failure handling would not alter any of our results, but it would
complicate our implementation framework and the understanding of the basic
mechanisms.



% These assumptions are,
% namely:
% \begin{enumerate}
% \item
% Actions in the environment may fail with some probability, even in
% world states where they ''should'' succeed. 
% This captures the non-deterministic nature of most real
% world environments. 
% \item
% No changes happen in the environment, during execution, other than
% those made by the agent. (Although this may appear too
% limiting, the previous assumption of non-determinism mitigates against
% the over-simplification for many cases, by treating other changes as
% non-deterministic failures.)
% \item
% The agent is only executing a single intention (i.e., pursuing a single
% goal). 
% %
% Without this assumption it would be necessary for the agent to
% reason about potentially negative interactions between concurrent goals. While such reasoning is
% possible
% \cite{Thangarajah02}, it complicates the situation for this work.
% 
% 
% 
% \item
% There is no automated failure handling, whereby the BDI system retries
% a goal with an alternative plan, if the selected plan fails. This will
% certainly be integrated into the system for real use, but again
% complicates the understanding of the basic mechanisms. In fact,
% its integration may help achieve faster learning.
% %  for real applications. 
% \end{enumerate}



