\documentclass[preprint,12pt]{elsarticle}
\parindent 0pt
\parskip 0.33ex

\usepackage{color}
\definecolor{gray}{rgb}{0.2,0.2,0.2}
\newcounter{countshout}
\newcommand{\shout}[1]{
\vspace{0.15in}
\noindent
%\begin{center}
%\begin{tabular}{c}
		\fcolorbox{black}{white}{
			\begin{minipage}{0.96\textwidth}
			#1\addtocounter{countshout}{1}%
			\end{minipage}
			}
		\fcolorbox{gray}{gray}{\textcolor
			{white}{\textbf{Action}}
		}
%%%		{white}{\textbf{\ Note \thecountshout\ }}
%\end{tabular}
%\end{center}
\vskip 0.25in
}

\newcommand{\question}[1]{\subsection{#1}}

%\renewcommand{\baselinestretch}{.96}

\begin{document}

\begin{frontmatter}

\title{Extending BDI Plan Selection to Incorporate Learning from Experience\\
(Responses to Reviewers)}

\author{Dhirendra Singh \& Sebastian Sardina \& Lin Padgham}
\ead{\{dhirendra.singh,sebastian.sardina,lin.padgham\}@rmit.edu.au}
\address{RMIT University, Melbourne, Australia}

\begin{abstract}
This document details the responses and changes based on reviewers comments on
the original manuscript submitted to the Special Issue on Hybrid Control of
Autonomous Systems of the Journal of Robotics and Autonomous Systems
\cite{Singh:HYCAS10}.
\end{abstract}

\end{frontmatter}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{General Comments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We would like to point out that our previous work referred to as ``under review''
in the manuscript  is now accepted as a peer-reviewed publication
\cite{Singh:AAMAS10} and we have duly updated the citation. Some concerns raised
by the reviewers have also been addressed in \cite{Singh:AAMAS10} and we have
noted this in our responses where appropriate.

{\it Please note that the page number and Figure references used in this document refer to the revised manuscript  and may be different from those in reviewer comments.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reviewer \#1 Comments and Responses}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question{ The problem is well motivated and is clearly relevant to the topic
of this special issue. The way the above two measures are computed seems plausible,
although this is only partly supported by the experiments. This is because of the
"optimizations" you mention on page 16 for the particular problem at hand. What
would happen if you did not use these optimizations? }

\label{rev1:optimisations}

We see how the details of the optimisations have added to some confusion.
% %
These optimisations were in place for \emph{all} experiments and so impacted both
approaches --- the baseline $ACL$ and the improved $ACL+\Omega$. Our main
motivation for this was to speed-up absolute convergence times for both
approaches during experimentation allowing us to significantly reduce the
run-time for our full suite of experiments.
% %
The impact of not applying the optimisations is that the graphs of Figure 5 and Figure 6 take more iterations to converge to the solutions. This
however does not impact any of our claims that only focus on the effectiveness of the new $ACL+\Omega$ approach ``relative'' to the baseline $ACL$.

\shout{For that reason we have removed the paragraph on
optimisations (was paragraph before start of [4.2 Results] on page 15), as it is not relevant to the discussion and does not bear on the relative results.}


\subsection{You say on p. 12 that $\delta_{Pt}$ can be computed offline. It is
not clear to me how. In your example, you state the equation for $\delta_Pt$ but
not how you obtained it. Was it derived automatically or by hand?
}

For this work, we have calculated $\delta_{Pt}$ in terms of average breadth and
depth of the structure, where the depth of a structure with recursion is the
maximum level of recursion, to provide an approximation of the complexity of the
structure.
% %
Of course, there are other ways of calculating $\delta_{Pt}$ (e.g., in
\cite{Singh:AAMAS10} we have used an accurate calculation of the number of
choices below plans; however this is not feasible anymore when goals and plans
are schemas with possibly infinite instances); the whole idea is to somehow
measure the complexity of a hierarchical structure.

\shout{We have added a footnote to the first paragraph on page 12 to clarify that
$\delta_{Pt}$ is computed in terms of breadth and depth of a structure but that
that other measures may be used with analogous overall results.}




\subsection{You only compare your new approach to your previous work in the experiments. Why did you not include a comparison with the existing BDI program that comes with JACK? This is all the more surprising since you yourself ask: does our learning framework achieve the performance of the existing system. Does the existing program always do the right thing? In any case, it would be good to compare at least some of the hand-crafted context formulas with those learned by the DT approach.}

The original JACK program has perfectly crafted context conditions that work precisely so that all $Solve$ events are always successfully resolved (equivalent to $100\%$ in Figure 5a). While the structured exploration of $ACL+\Omega$ succeeds in resolving the $Solve$ goal for all possible configurations (new Figure 4b crosses) by $12k$ iterations, it nonetheless does not achieve perfect performance (converges to about $90\%$ performance in Figure 5a). The reason is that the decision tree is a compact representation that does not guaranteed 100\% correctness when classifying the training data (we discuss this inherent trade-off in [3.1 Integrating Decision Trees into Context Conditions for Plans]). For instance, a plan that has previously succeeded in a given world may not get selected because the decision tree returns a low probability of success due to misclassification of the related training sample. The only way to guarantee correct selection would be to use the training data directly, for instance using a look-up table. 

We would further like to point out here that the learnt context conditions do not ``look'' like the original ones. When the learnt decision trees are converted to rules, they are quite complex and perhaps not intuitive as the original conditions are. This is mainly due to representational differences. For instance, the real context conditions are relational while the learnt ones are propositional. 

\shout{We have updated the final paragraph of [4.2 Results] on page 18 to clarify this point.}


\subsection{The paper is well written and the relevant literature is discussed/cited as far as I can tell. (I am no BDI expert.)}

\shout{Please note that we have significantly improved the related work discussion in [5. Discussion and Conclusion] on page 20.}

\subsection{Typo: p.20: preiously}

\shout{This has now been corrected.}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reviewer \#2 Comments and Responses}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The overall work seems reasonable and appears to be effective,
although it does not seem to be particularly challenging. The learning
approach is limited by the inabilitiy to take into account goal
inter-dependencies. Nevertheless, the work represents progress towards
endowing BDI agents with learning capabilities.}
\label{rev2:domain_justification}

Although the problem may initially appear non-challenging, it indeed is. There
are many nuances that come up mainly due to the fact that \emph{(i)} the
objects to be learnt (i.e., plan's context conditions) are strongly
inter-related, what happens in one plan is affected by what happens in the plans
below; \emph{(ii)} learning and acting happens interleaved; and \emph{(iii)}
there are typically many more failures than successes.

Providing a \emph{principled} exploration mechanism of the several alternatives
an agent may have in its know-how structure turns out to be non trivial, which
explains why almost no work has been done on the topic.

With respect to the issue of inter-dependencies, we point out that this is not a
limitation of our learning approach itself, but of the overall BDI programming
paradigm. Addressing this would require a substantial modification in the BDI
programming style in terms of representation, which is out of the scope of this
paper. Here we aim to provide a learning account compatible with standard
existing BDI architectures.

\shout{We have updated paragraph 4 on page 20 to clarify this point.}

\subsection{In sec.3, 1st para., what do you mean by "event-goal type"? just a
parametrized event-goal?}

\shout{Yes. We have updated the paper to be consistent and use \textit{parameterised} instead.}

\subsection{The whole section 3.1 is written in a confusing way.}

\shout{We have now significantly reworked this section to help improve understanding.}
 
\subsection{In the 2nd para. pg 9, you talk about "the plan". One can guess that
you mean learning is applyied for each plan in the library. But it
would be better to make it explicit here, e.g., that you need a
training data set for each plan in the library.}

\shout{We have incorporated your suggestion and updated the last paragraph on page 8. We have also made this clear in the first paragraph of [2.2 Learning for BDI Plan Selection] on page 6.}

\subsection{At the end of this paragraph, it says that a classification is learnt "based purely on the subset of attributes in w that are relevant to the contex condition of the plan." This is not clear. What 'w' are you referring to here? The next para. starts again talking about "The attributes in w...", what 'w'?}

\shout{We have prefixed ``world state'' to $w$ throughout and ensured that $w$ is explained clearly at the start of the paragraph when we first mention it (last paragraph of page 8).}


\subsection{Also in the 2nd para. it says "the decision tree will learn a
classification..." isn't the tree itself what is being learnt?}

\shout{We have reworded the last paragraph of page 8 so it is not confusing to the reader.}

\subsection{In the 3rd para. pg 11, it says that "...we may never find the bottom or leaf nodes. This has implication for any bottom-up strategies." You mean top-down strategies?}

\shout{We have rewritten paragraph 1 on page 11 to remove any confusion.}

\newcommand{\tuple}[1]{\langle #1 \rangle}		% tuple

\subsection{In sec. 3.2, goals G are sometimes followed by square brackets, as in
$G[\phi]$, and sometimes buy angle brackets, $G\tuple{\phi}$, which one may
guess represent modalities, but the notation has not been defined. So the reader
is left guessing what the notation means.}

We apologise for the confusing notation. These are not modalities. $G[P:w]$
states that plan $P$ is tried on world $w$ to resolve goal $G$. We have changed
$G\tuple{\phi}$ to $G(\vec{x})$ which denotes goal $G$ with parameters $\vec{x}$
(e.g., $clean(room1)$).

\shout{We have clarified and fixed the notation in Section 3.2. We now
just use $G[P:w]$ and $G(\vec{x})$.}


\subsection{Sec 3.4, 1st para., you refer to a "previous definition of the
learning task (Section 3)", which one can guess refers to the informal
definition at the beginning of Section 3, since there is no definition
anywhere. Btw, the reference itself is in section 3.}

\shout{We have removed the reference and reworded the first paragraph of [3.4 Handling Parameterised Event-Goals] on page 12.}

\subsection{Also in sec 3.4, you define the training data set as tuples
[w U phi, o]. This is strange since I would think the world would be
represented differently from a goal's parameters, but you specify the
first element in a training data tuple as the union of a world and
the goal's parameters. Why not add a ground goal instead? Or use a
triple?}

\shout{Thank you for picking this. We have now updated the last paragraph of [3.4 Handling Parameterised Event-Goals] on page 13 to now use a triple.}

\subsection{At the end of sec. 4.1, you discuss some "optimizations" that are
possible in the case of the towers of hanoi problem. It is not really
clear why you need to make these optimizations (I'd called them
simplifying assumptions rather than optimizations). What if you don't
make them?}

We refer the reviewer to the response in \ref{rev1:optimisations} that addresses this concern.

\subsection*{Typos etc:}

\subsection{pg 8: "a plans ..." $\to$ "a plan ..." or "a plan's ..." (occurs in many places).}

\shout{We have reviewed the manuscript and corrected these cases.}

\subsection{pg 20: "preiously learnt"}

\shout{This has now been corrected.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reviewer \#3 Comments and Responses}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The major weaknesses of the paper is the lack of emperical or formal results and the number of assumptions made which makes the general applicability questionable.}

In general, we believe that while the assumptions mean that the current framework is not ready for practical systems, it nonetheless is a significant step forward in our ongoing work to that end. Please refer to our responses in \ref{rev3:assumptions} and \ref{rev3:convincing} that address specific questions that are related to this comment.

\subsection{ The paper is well written and is easy to understand but some parts
in the sections 2 and 3 would be easier to understand if a good real example
would guide the line of argumentation (e.g. an example from the agents or
robotics domain). $\ldots$ A good example of a domain used in the explanation of
the background in section 2 and in the development of the approach in section 3
would ease the understanding of the paper and would even non-bdi experts to
profit from the paper.}

\label{rev3:bdibackground}

We take the point and agree it may be hard for non-bdi experts to get a good
understanding of the BDI approach by reading Section 2. In particular, it was
difficult to see what plan libraries built from plan-rules, in turn built from
events and actions, were.

\shout{We have added a short example of plan rules that would arise in an
elevator controller. Please see second paragraphs 2 and 3 of [2.1 BDI Agent Systems] on page 4.}


\subsection{ In the middle part of page 7 the authors claim that event types and
recursion is important to handle. But it is not very well explained why these
constructs are important to handle.}

\label{rev3:recursion_examples}

Event types are necessary to be able to compactly represent the domain know-how
information by providing ``schemas'' of plans that can be instantiated with
multiple objects. Recursion is very important as it allows, in these languages,
to express (unbounded) iteration (as in logic programming, there is in general
no imperative while or loop constructs, thus recursion becomes the main
construct for iteration).

\shout{Hopefully the plan rules example for an elevator controller introduced in
[2.1 BDI Agent Systems] will help clarify why schematic plan rules and recursive calls in
programs are important. We have also updated paragraph 2 on page 7
(``A limitation...'') to be more clear on the importance of handling these 2
features.}


\subsection{Also throughout the paper it is not really point out how the event types are handled in fact.}

We handle event types in the learning framework by adding the event parameters as attributes to the decision trees' training data. Please see the last paragraph of [3.4 Handling Parameterised Event-Goals] on page 13 for more information on this.

\subsection{The term "coverage" is used very often starting with page 7. It seems to be an important part in the approach but is neither formally defined nor really explained. A definition and a deeper explanation would help the reader to understand the approach.}

We consider the reliability of a plan's decision tree in a given 
world state to be proportional to the number of sub-plan choices (or paths
below the plan in the goal-plan hierarchy) that have been
explored in that world state. 
%
{\it Coverage} refers to the set of explored paths relative to the set of all possible paths.
%
The greater the coverage, the more we have explored and the greater the confidence in the resulting decision tree.

\shout{We have now updated paragraph 1 on page 7 with this definition.}

\subsection{On page 9 the authors introduce decision trees to represent the chance of a plan to be successful which is used later to define if a plan should be used in a certain situation. To me it is not clear as a decision tree covers both the negative and the positive case of the execution how the information on the negative outcome is used.}
Yes, the decision tree classification is concrete i.e. either {\it success} or {\it failure}. However, since the decision tree induction usually has some trade-off between accuracy of classification and compactness of representation, then some training samples get incorrectly classified in the wrong ``bucket'' (where the bucket name is {\it success} or {\it failure}) when infact the actual outcome class of those training samples is different. So in a given bucket (or class) one may get a total of $m$ training samples out of which $n$ are misclassified. The ratio $1-(n/m)$ gives the likelihood of class membership and is automatically provided by the $weka$ package. This is the fraction we refer to when we talk about the ``chance'' of success (or failure).

\shout{We agree that this was not well explained. We have updated the last paragraph in [3.1. Integrating Decision Trees into Context Conditions for Plans] on page 9 to make this clear.}

\subsection{ On the second half of page 9 the authors claim that the set of
attributes used in the learning of a decision tree are selected by the programmer
of the domain. This means in the first place additional work for the programmer
and might in the second place prevent the system to find a good solution. Would
it be feasible to learn the conditions without this artificial limitation of the
attributes?}

We believe the requirement for the user/programmer to state relevant domain
propositions is a realistic one. It is generally feasible for the domain expert
to state which features of the world might play some role when solving a given
goal. For example, in the travelling domain, airlines schedules and location of
airports are relevant for the goal of buying an airline ticket, whereas domain
propositions describing the humidity or features of car rental companies are
mostly not relevant. See that the requirement is for the domain expert to
enumerate the relevant features for each goal; this is generally a feasible
request and much less demanding than requesting exact plans' preconditions.

Nonetheless, relevant features could be automatically extracted from the plan
library if domain information is available for primitive actions. For example, if
in addressing some goal, it might happen that action ``drive(src,dest)'' is
executed, then the fuel level, being mentioned in the precondition of the action,
should be considered as ``relevant'' for the goal.
% %
Though this automatic extraction is not the subject of this paper, we think that
in a full-fledged learning BDI system, both automatic analysis and domain expert
knowledge would be used to estimate the relevant set of propositions for a goal.

\shout{We have added a footnote to the paragraph 2 of page 9 to highlight this point.}

\subsection{ On page 10 the authors explain recursion and the need for. Could you
give a proper example for a recursive event goal in a real world domain.}

As in logic programming, recursive calls (in our context, recursive
``subgoaling'') are necessary to express iterative procedures.
%%
Please see our response in \ref{rev3:recursion_examples} above.

 
\subsection{In chapter 3.3 the authors explain that the training set is build up incrementally. To my understanding it might be the case that the training set so far and the plan selection learned so far has an influence of the future training data. Therefore, is it possible that a somehow unlucky plan selection and therefore probably bad training set prevent the system to converge to a good or correct set of conditions?}

Yes, absolutely correct. This concern when performing \textit{online} learning (learning while you act) has been central in our previous work as it is here. In the first instance \cite{Airiau:IJAT09} we look at how the notion of informed decisions may be used to do selective recording of results in order to filter out misleading training data. In the second \cite{Singh:AAMAS10} we explore a confidence measure that may be used to decide how much value we will put in the resulting decision tree. These are two orthogonal approaches i.e. how best to collect training data and how best to use it.

\shout{We have updated paragraph 2 of [2.2. Learning for BDI Plan Selection] on page 6 to include this point about ongoing learning impacting future learning.
}

\subsection{Formula 2 on page 14. The construction of the formula looks to me a little bit ad-hoc. Is there a better justification for the formula? What are others or maybe better weight functions.}

The formula is the same as that described in \cite{Singh:AAMAS10} where we also discuss an alternative weighting and how it may be used to adjust the performance of the system to suit different hierarchies. Considereing that this issue has been previously discussed, and due to lack of space here, we have chosen not to include this justification in this paper.

\shout{However, we have added a footnote to paragraph 2 of [3.5. Calculating Plan Selection Weights based on Conﬁdence] on page 13 referring the reader to the origin of the formula.}

\subsection{In the middle part of page 16 the authors state that they do not continuous exploration once a solution is found. Why this is done or why it is sufficient? It seems to me that this assumption is against the authors arguments for the properties of their approach.}

We are sorry this has been a cause of much confusion. We refer the reviewer to the response in \ref{rev1:optimisations} that addresses this concern.

\subsection{In 4.2. k is introduced but no real number for it in the experiments is presented nor the influence of this parameter is described. Please give more information on this.}

The parameter $k$ controls the rate of change of decay $\delta_{Pt}$ and is currently arbitrarily set to $4.0$. Since the decay $\delta_{Pt}$ determines our confidence in the decision tree, then $k$ determines how quickly or slowly we will get there in terms of the number of steps.

\shout{We have updated paragraph 1 of [4.2 Results] on page 16 with this information.}

\subsection{At the end of page 18 the authors that the applicability threshold is set to 20\% in the second experiment. It seems to my a little bit artificial and tuned towards an ad-hoc justification for the approach. Please give further information on the performance of the compared algorithms in the context of this threshold.}

We agree that the threshold setting of $20\%$ is somewhat arbitrary, however it is consistent with that used in \cite{Singh:AAMAS10}. The difference between the default weight ($0.5$) and the cut-off weight (threshold of $0.2$ in this case) determines how much ``give'' we have in the exploration. The closer the threshold is to the default, the higher the likelihood that the agent will give up before achieving the goal.

\shout{We have added a footnote to paragraph 1 on page 18 to help clarify this further.}

\subsection{On the top of page a number of assumptions are stated. These assumption seems to limit the practical general applicability of the approach a lot. Please give a deeper justification for these assumptions and on probably solutions to them.}
\label{rev3:assumptions}
\shout{We assume the reviewer is referring to the assumptions on page 20 of the original submission. We have improved justification of assumptions in [5. Discussion and Conclusion] on page 19. We have also emphasised that while more work is required before our framework is ready for use in practical systems, this work nonetheless constitutes significant progress in this direction.}


\subsection{The coverage of related research seems to me a bit weak. Only three references on such a topic is not much. I guess there is more related work out there. Please rework these section.}

\shout{We have reworked the entire related work coverage in [5. Discussion and Conclusion] and now provide a total of nine related references on pages 20.}

\subsection{The result section is not very much convincing. First to me it is not clear why a simple toy domain like the towers of Hanoi justifies the usefulness of the algorithm. Even the two experiments in this domain are made in a ad-hoc manner. For a journal this is too weak. I like to see a deeper treatment of the toy domain and to see experiments in a more advanced, more relevant or even real world domain. Otherwise I am not completely convinced about the advantages of the approach.}
\label{rev3:convincing}

We acknowledge your overall concern, but hope to strongly convey that the chosen domain (taken from the JACK distribution example) is in fact justified as it captures important characteristics that must first be treated and understood in a principled way in an idealised setting that is not possible by directly adopting a real world example. We reiterate that our end goal {\it is} to allow use of our learning framework in real applications. We further refer the reviewer to our response in \ref{rev2:domain_justification}.

Our experiments were chosen to convey the key benefits of the $ACL+\Omega$ approach using a similar experimental setup as previously \cite{Singh:AAMAS10}. In fact, our testing for this work includes all our earlier experiments (albeit lacking parameterised events and recursion) to ensure that the new approach does not invalidate our earlier work.


The first experiment (Figure 5) captures some key points that we wish to highlight. Firstly, it shows the benefits of the structured exploration of $ACL+\Omega$ over the baseline $ACL$ approach \cite{Airiau:IJAT09,Singh:AAMAS10}. The result also confirms the merits of the exploration-based confidence idea that in it's original form \cite{Singh:AAMAS10} did not scale to recursive programs and formed the motivation for this work. Secondly, the experiment reinforces the intuition that the structured $ACL+\Omega$ approach is likely to show more gains where the solution is more complex. For instance, for solutions requiring five levels of recursion, ACL achieves only $50\%$ success by the end of the experiment at $5k$ iterations whereas $ACL+\Omega$ achieves $95\%$ success within $3.5k$ iterations. For levels three and one, the advantages of $ACL+\Omega$ over $ACL$ are progressively less. 

The first experiment equips us to understand the performance of the two approaches in the full experiment shown in Figure 6a. We have now added a second view of the results of experiment two in Figure 6b that shows the progressive discovery of solutions for each approach. Here the benefit of $ACL+\Omega$ is clear. Figure 6b shows that $ACL+\Omega$ resolves all $52$ goals within $12k$ iterations whereas $ACL$ resolves only $47$ by the end of the experiment at $20k$. At a similar point of comparison, to resolve $47$ goals $ACL+\Omega$ takes around $6.4k$ iterations whereas $ACL$ takes more than twice as long at $17.7k$. 

Perhaps it is the choice of the third experiment with applicability thresholds that seems ad-hoc to the reader in this context, however it is there to emphasise that the improved $ACL+\Omega$ avoids this issue in a manner  (of experiment) similar to the coverage approach \cite{Singh:AAMAS10} it borrows from. 


\shout{We have added Figure 5b and significantly reworded [4.2 Results] to strengthen our case.}

\section*{Final Comments}

We thank all the reviewers for their invaluable comments and the opportunity to address the points raised. We hope to make a valuable contribution to the Journal of Robotics and Autonomous Systems.
 
\begin{thebibliography}{10}

\bibitem{Airiau:IJAT09}
S.~Airiau, L.~Padgham, S.~Sardina, and S.~Sen.
\newblock Enhancing Adaptation in {BDI} Agents Using Learning Techniques.
\newblock In {\em International Journal of Agent Technologies and Systems}
(IJATS), 1(2):1-18, January 2009. 

\bibitem{Singh:AAMAS10}
D.~Singh, S.~Sardina, L.~Padgham, S.~Airiau.
\newblock Learning Context Conditions for {BDI} Plan Selection.
\newblock In {\em Proceedings of Autonomous Agents and Multi-Agent Systems
(AAMAS)}, Toronto, Canada, 2010. To appear.

\bibitem{Singh:HYCAS10}
D.~Singh, S.~Sardina, L.~Padgham.
\newblock Extending BDI Plan Selection to Incorporate Learning from Experience.
\newblock Submission under review for the Special Issue on Hybrid Control of Autonomous Systems (HYCAS), Journal of Robotics and Autonomous Systems, 2010.


\end{thebibliography}

\end{document}
