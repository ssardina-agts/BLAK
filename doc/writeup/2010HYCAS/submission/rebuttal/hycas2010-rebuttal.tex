\documentclass[preprint,12pt]{elsarticle}
\parindent 0pt
\parskip 0.33ex

\usepackage{color}
\definecolor{gray}{rgb}{0.2,0.2,0.2}
\newcounter{countshout}
\newcommand{\shout}[1]{
\vspace{0.15in}
\noindent
%\begin{center}
%\begin{tabular}{c}
		\fcolorbox{black}{white}{
			\begin{minipage}{0.96\textwidth}
			#1\addtocounter{countshout}{1}%
			\end{minipage}
			}
		\fcolorbox{gray}{gray}{\textcolor
			{white}{\textbf{Action}}
		}
%%%		{white}{\textbf{\ Note \thecountshout\ }}
%\end{tabular}
%\end{center}
\vskip 0.25in
}

\newcommand{\question}[1]{\subsection{#1}}

\begin{document}

\begin{frontmatter}

\title{Extending BDI Plan Selection to Incorporate Learning from Experience\\
(Responses to Reviewers)}

\author{Dhirendra Singh \& Sebastian Sardina \& Lin Padgham}
\ead{\{dhirendra.singh,sebastian.sardina,lin.padgham\}@rmit.edu.au}
\address{RMIT University, Melbourne, Australia}

\begin{abstract}
This document details the responses and changes based on reviewers comments on
the original manuscript submitted to the Special Issue on Hybrid Control of
Autonomous Systems of the Journal of Robotics and Autonomous Systems
\cite{Singh:HYCAS10}.
\end{abstract}

\end{frontmatter}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{General Comments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We would like to point out that our previous work referred to as ``under review''
in the manuscript  is now accepted as a peer-reviewed publication
\cite{Singh:AAMAS10} and we have duly updated the citation. Some concerns raised
by the reviewers have also been addressed in \cite{Singh:AAMAS10} and we have
noted this in our responses where appropriate.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reviewer \#1 Comments and Responses}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% {\em The paper is concerned with learning plan selection for parametrized goals
% with sub-goal recursion in the context of a BDI architecture. The work is based
% on an earlier approach by the same authors applying decision-tree (DT) learning
% to this problem. Previously, goals were not parametrized and hence no recursion
% needed. Technical contributions include equations to compute a measure of
% confidence in the DT classification based on both plan and domain complexity and
% a likelihood measure for selecting a plan for execution. Experimental results
% using the Towers of Hanoi problem are presented and discussed.}

\question{ The problem is well motivated and is clearly relevant to the topic
of this special issue. The way the above two measures are computed seems plausible,
although this is only partly supported by the experiments. This is because of the
"optimizations" you mention on page 16 for the particular problem at hand. What
would happen if you did not use these optimizations? }

\label{rev1:optimisations}

We see how the details of the optimisations have added to some confusion.
% %
These optimisations were in place for \emph{all} experiments and so impacted both
approaches --- the baseline $ACL$ and the improved $ACL+\Omega$. Our main
motivation for this was to speed-up absolute convergence times for both
approaches during experimentation allowing us to significantly reduce the
run-time for our full suite of experiments.
% %
The impact of not applying the optimisations is that the graphs of Figure 5 and Figure 6 take more iterations to converge to the solutions. This
however does not impact any of our claims that only focus on the effectiveness of the new $ACL+\Omega$ approach ``relative'' to the baseline $ACL$.

\shout{For that reason we have removed the paragraph on
optimisations (final paragraph of [Section 4.1 Experimental Setup] on page 16), as it is not relevant to the discussion does not bear on the relative results.}


\subsection{You say on p. 12 that $\delta_{Pt}$ can be computed offline. It is
not clear to me how. In your example, you state the equation for $\delta_Pt$ but
not how you obtained it. Was it derived automatically or by hand?
}

% The idea of $\delta_Pt$ is a decay factor that matches the ``complexity'' of the
% goal-plan structure. This factor would be smaller for a less complex structure
% than for a more complex one. The goal-plan structure itself is nothing more than
% the BDI program and is given upfront in the form of a plan library that may be 
% parsed offline to compute a structure similar to that shown in Figure 2. We may
% further parse this structure to compute a plan's complexity relative to another.
% Previously \cite{Singh:AAMAS10} we have used an accurate calculation of the
% number of choices below the plan to measure this. However, the same calculation
% is not possible with parameterised goals --- we cannot compute offline the full
% set of possibilities as it depends on the runtime values of the parameters. 

For this work, we have calculated $\delta_{Pt}$ in terms of average breadth and
depth of the structure, where the depth of a structure with recursion is the
maximum level of recursion, to provide an approximation of the complexity of the
structure.
% %
Of course, there are other ways of calculating $\delta_{Pt}$ (e.g., in
\cite{Singh:AAMAS10} we have used an accurate calculation of the number of
choices below plans; however this is not feasible anymore when goals and plans
are schemas with possibly infinite instances); the whole idea is to somehow
measure the complexity of a hierarchical structure.

\shout{We have updated the related paragraph on page 12 to clarify that
$\delta_{Pt}$ is computed in terms of breadth and depth of a structure but that
that other measures may be used with analogous overall results.}




\subsection{You only compare your new approach to your previous work in the experiments. Why did you not include a comparison with the existing BDI program that comes with JACK? This is all the more surprising since you yourself ask: does our learning framework achieve the performance of the existing system. Does the existing program always do the right thing? In any case, it would be good to compare at least some of the hand-crafted context formulas with those learned by the DT approach.}

The original JACK program has perfectly crafted context conditions that work precisely so that all $Solve$ events are always successfully resolved (equivalent to $100\%$ in Figure 6a). While the structured exploration of $ACL+\Omega$ succeeds in resolving the $Solve$ goal for all possible configurations (new Figure 6b crosses) by $12k$ iterations, it nonetheless does not achieve perfect performance (converges to about $90\%$ performance in Figure 6a). The reason is that the decision tree is a compact representation that does not guaranteed 100\% correctness when classifying the training data (we discuss this inherent trade-off in [Section 3.1 Integrating Decision Trees into Context Conditions for Plans]). For instance, a plan that has previously succeeded in a given world may not get selected because the decision tree returns a low probability of success due to misclassification of the related training sample. The only way to guarantee correct selection would be to use the training data directly, for instance using a look-up table. 

We would further like to point out here that the learnt context conditions do not ``look'' like the original ones. When the learnt decision trees are converted to rules, they are quite complex and perhaps not intuitive as the original conditions are. This is mainly due to representational differences. For instance, the real context conditions are relational while the learnt ones are propositional. 

\shout{We have updated the final paragraph of [Section 4.2 Results] on page 18 to clarify this point.}


\subsection{The paper is well written and the relevant literature is discussed/cited as far as I can tell. (I am no BDI expert.)}

\shout{Please note that we have further updated and reworked the related work discussion in [Section 5. Discussion and Conclusion] on pages 20 and 21.}

\subsection{Typo: p.20: preiously}

\shout{This has now been corrected.}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reviewer \#2 Comments and Responses}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% {\em This paper presents work that aims at adding learning capabilities to
% BDI agent systems. In particular, the authors introduce learning of
% the context conditions of BDI agent programs, that is, the state
% conditions that are required to hold in the current state of execution
% in order for an agent program to be considered for execution.
% The approach is based on replacing the context formulae of programs by
% decision trees. The work builds on previous work by the authors by
% generalizing the learning scheme to work with parametrized goal/events
% and (bounded) recursive programs. An experimental evaluation of the
% learning approach is presented showing the improvement over their
% previous work.}


\subsection{The overall work seems reasonable and appears to be effective,
although it does not seem to be particularly challenging. The learning
approach is limited by the inabilitiy to take into account goal
inter-dependencies. Nevertheless, the work represents progress towards
endowing BDI agents with learning capabilities.}
\label{rev2:domain_justification}

Although the problem may initially appear non-challenging, it indeed is. There
are many nuances that come up mainly due to the fact that \emph{(i)} the
objects to be learnt (i.e., plan's context conditions) are strongly
inter-related, what happens in one plan is affected by what happens in the plans
below; \emph{(ii)} learning and acting happens interleaved; and \emph{(iii)}
there are typically many more failures than successes.

Providing a \emph{principled} exploration mechanism of the several alternatives
an agent may have in its know-how structure turns out to be non trivial, which
explains why almost no work has been done on the topic.

With respect to the issue of inter-dependencies, we point out that this is not a
limitation of our learning approach itself, but of the overall BDI programming
paradigm. Addressing this would require a substantial modification in the BDI
programming style in terms of representation, which is out of the scope of this
paper. Here we aim to provide a learning account compatible with standard
existing BDI architectures.

\shout{We have updated the first paragraph on page 20 to clarify this point.}

\subsection{In sec.3, 1st para., what do you mean by "event-goal type"? just a
parametrized event-goal?}

\shout{Yes. We have updated the paper to be consistent and use \textit{parameterised} instead.}

\subsection{The whole section 3.1 is written in a confusing way.}

\shout{We have further reviewed and updated this section to help improve understanding.}
 
\subsection{In the 2nd para. pg 9, you talk about "the plan". One can guess that
you mean learning is applyied for each plan in the library. But it
would be better to make it explicit here, e.g., that you need a
training data set for each plan in the library.}

\shout{We have incorporated your suggestion and updated paragraph 1 on page 9.}

\subsection{At the end of this paragraph, it says that a classification is learnt "based purely on the subset of attributes in w that are relevant to the contex condition of the plan." This is not clear. What 'w' are you referring to here? The next para. starts again talking about "The attributes in w...", what 'w'?}

\shout{We have prefixed ``world state'' to $w$ throughout and ensured that $w$ is explained clearly at the start of the paragraph when we first mention it.}


\subsection{Also in the 2nd para. it says "the decision tree will learn a
classification..." isn't the tree itself what is being learnt?}

\shout{We have reworded paragraph 1 on page 9 so it is not confusing to the reader.}

\subsection{In the 3rd para. pg 11, it says that "...we may never find the bottom or leaf nodes. This has implication for any bottom-up strategies." You mean top-down strategies?}

\shout{We have rewritten paragraph 2 on page 11 to remove any confusion.}

\newcommand{\tuple}[1]{\langle #1 \rangle}		% tuple

\subsection{In sec. 3.2, goals G are sometimes followed by square brackets, as in
$G[\phi]$, and sometimes buy angle brackets, $G\tuple{\phi}$, which one may
guess represent modalities, but the notation has not been defined. So the reader
is left guessing what the notation means.}

We apologise for the confusing notation. These are not modalities. $G[P:w]$
states that plan $P$ is tried on world $w$ to resolve goal $G$. We have changed
$G\tuple{\phi}$ to $G(\vec{x})$ which denotes goal $G$ with parameters $\vec{x}$
(e.g., $clean(room1)$).

\shout{We have clarified and fixed the notation in Section 3.2. We now
just use $G[P:w]$ and $G(\vec{x})$.}


\subsection{Sec 3.4, 1st para., you refer to a "previous definition of the
learning task (Section 3)", which one can guess refers to the informal
definition at the beginning of Section 3, since there is no definition
anywhere. Btw, the reference itself is in section 3.}

\shout{We have removed the reference and reworded paragraph 1 of [Section 3.4 Handling Parameterised Event-Goals] on page 12.}

\subsection{Also in sec 3.4, you define the training data set as tuples
[w U phi, o]. This is strange since I would think the world would be
represented differently from a goal's parameters, but you specify the
first element in a training data tuple as the union of a world and
the goal's parameters. Why not add a ground goal instead? Or use a
triple?}

\shout{We have now updated the last paragraph of [Section 3.4 Handling Parameterised Event-Goals] on page 13 to now use a triple.}

\subsection{At the end of sec. 4.1, you discuss some "optimizations" that are
possible in the case of the towers of hanoi problem. It is not really
clear why you need to make these optimizations (I'd called them
simplifying assumptions rather than optimizations). What if you don't
make them?}

We refer the reviewer to the response in \ref{rev1:optimisations} that addresses this concern.

\subsection*{Typos etc:}

\subsection{pg 8: "a plans ..." $\to$ "a plan ..." or "a plan's ..." (occurs in many places).}

\shout{We have reviewed the manuscript and corrected these cases.}

\subsection{pg 20: "preiously learnt"}

\shout{This has now been corrected.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reviewer \#3 Comments and Responses}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{The major weaknesses of the paper is the lack of emperical or formal results and the number of assumptions made which makes the general applicability questionable.}

In general, we believe that while the assumptions means that the current framework is not ready for practical systems, it nonetheless is a significant step forward in our ongoing work to that end. Please refer to our responses in \ref{rev3:assumptions}, \ref{rev3:convincing}, and \ref{rev3:experiments} that address specific questions that are related to this comment.


\subsection{The paper is well written and is easy to understand but some parts in the sections 2 and 3 would be easier to understand if a good real example would guide the line of argumentation (e.g. an example from the agents or robotics domain). $\ldots$ A good example of a domain used in the explanation of the background in section 2 and in the development of the approach in section 3 would ease the understanding of the paper and would even non-bdi experts to profit from the paper.}
\label{rev3:recursion_examples}

\shout{SEBASTIAN TODO: We have now included references to several logistics domains used in the planning literature that are also relevant to our discussion. Where?...}


\subsection{In the middle part of page 7 the authors claim that event types and recursion is important to handle. But it is not very well explained why these constructs are important to handle.} 

\shout{SEBASTIAN TODO: In procedural languages that do not provide constructs for looping, one mechanism for achieving this behaviour is through recursion. For this reason parameterised events and recursion are essential features that are heavily used in real BDI systems.}


\subsection{Also throughout the paper it is not really point out how the event types are handled in fact.}

We handle event types in the learning framework by adding the event parameters as attributes to the decision trees' training data. Please see the last paragraph of [Section 3.4 Handling Parameterised Event-Goals] on page 13 for more information on this.

\subsection{The term "coverage" is used very often starting with page 7. It seems to be an important part in the approach but is neither formally defined nor really explained. A definition and a deeper explanation would help the reader to understand the approach.}

We consider the reliability of a plan's decision tree in a given 
world state to be proportional to the number of sub-plan choices (or paths
below the plan in the goal-plan hierarchy) that have been
explored in that world state. 
%
{\it Coverage} refers to the set of explored paths relative to the set of all possible paths.
%
The greater the coverage, the more we have explored and the greater the confidence in the resulting decision tree.

\shout{We have now updated paragraph 2 on page 7 with this definition.}

\subsection{On page 9 the authors introduce decision trees to represent the chance of a plan to be successful which is used later to define if a plan should be used in a certain situation. To me it is not clear as a decision tree covers both the negative and the positive case of the execution how the information on the negative outcome is used.}
Yes, the decision tree classification is concrete i.e. either {\it success} or {\it failure}. However, since the decision tree induction usually has some trade-off between accuracy of classification and compactness of representation, then some training samples get incorrectly classified in the wrong ``bucket'' (where the bucket name is {\it success} or {\it failure}) when infact the actual outcome class of those training samples is different. So in a given bucket (or class) one may get a total of $m$ training samples out of which $n$ are misclassified. The ratio $1-(n/m)$ gives the likelihood of class membership and is automatically provided by the $weka$ package. This is the fraction we refer to when we talk about the ``chance'' of success (or failure).

\shout{We have updated to last paragraph in [Section 3.1. Integrating Decision Trees into Context Conditions for Plans] on page 9 to make this clear.}

\subsection{On the second half of page 9 the authors claim that the set of attributes used in the learning of a decision tree are selected by the programmer of the domain. This means in the first place additional work for the programmer and might in the second place prevent the system to find a good solution. Would it be feasible to learn the conditions without this artificial limitation of the attributes?}

\shout{SEBASTIAN TODO: We have updated the second last paragraph in [Section 3.1. Integrating Decision Trees into Context Conditions for Plans] on page 9 to address this further.}

\subsection{On page 10 the authors explain recursion and the need for. Could you give a proper example for a recursive event goal in a real world domain.}

\shout{SEBASTIAN TODO: This is related to the response in \ref{rev3:recursion_examples}.}

\subsection{In chapter 3.3 the authors explain that the training set is build up incrementally. To my understanding it might be the case that the training set so far and the plan selection learned so far has an influence of the future training data. Therefore, is it possible that a somehow unlucky plan selection and therefore probably bad training set prevent the system to converge to a good or correct set of conditions?}

Yes, this is correct. This very concern has been the subject of our previous work. In the first instance \cite{Airiau:IJAT09} we look at how the notion of informed decisions may be used to do selective recording of results in order to filter out misleading training data. In the second \cite{Singh:AAMAS10} we explore a confidence measure that may be used to decide how much value we will put in the resulting decision tree. These are two orthogonal approaches i.e. how best to collect training data and how best to use it.

\shout{We have updated paragraph 2 of [Section 2.2. Learning for BDI Plan Selection] on page 6 to clarify this point just before our previous work is discussed.}

\subsection{Formula 2 on page 14. The construction of the formula looks to me a little bit ad-hoc. Is there a better justification for the formula? What are others or maybe better weight functions.}

The formula is the same as that described in \cite{Singh:AAMAS10} where we also discuss an alternative weighting and how it may be used to adjust the performance of the system to suit different hierarchies. Considereing that this issue has been previously discussed, and due to lack of space here, we have chosen not to include this justification in this paper.

\shout{We have added a note referring the reader to the origin of the formula.}

\subsection{In the middle part of page 16 the authors state that they do not continuous exploration once a solution is found. Why this is done or why it is sufficient? It seems to me that this assumption is against the authors arguments for the properties of their approach.}

We refer the reviewer to the response in \ref{rev1:optimisations} that addresses this concern.

\subsection{In 4.2. k is introduced but no real number for it in the experiments is presented nor the influence of this parameter is described. Please give more information on this.}

The parameter $k$ controls the rate of change of decay $\delta_{Pt}$ and is currently arbitrarily set to $4.0$. Since the decay $\delta_{Pt}$ determines our confidence in the decision tree, then $k$ determines how quickly or slowly we will get there in terms of the number of steps.

\shout{We have updated paragraph 1 of [Section 4.2 Results] on page 16 with this information.}

\subsection{At the end of page 18 the authors that the applicability threshold is set to 20\% in the second experiment. It seems to my a little bit artificial and tuned towards an ad-hoc justification for the approach. Please give further information on the performance of the compared algorithms in the context of this threshold.}

Yes, the threshold setting of $20\%$ is somewhat arbitrary, but consistent with that used in \cite{Singh:AAMAS10}. The difference between the default weight ($0.5$) and the cut-off weight (threshold of $0.2$ in this case) determines how much ``give'' we have in the exploration. The closer the threshold is to the default, the higher the likelihood that the agent will give up before achieving the goal. However, we do not believe the chosen threshold impacts the justification of the approach. The main requirement is that it be in the range $0.0 \leq threshold < 0.5$.

\shout{We have added a footnote on page 18 to help clarify this further.}

\subsection{On the top of page a number of assumptions are stated. These assumption seems to limit the practical general applicability of the approach a lot. Please give a deeper justification for these assumptions and on probably solutions to them.}
\label{rev3:assumptions}
\shout{We assume the reviewer is referring to the assumptions on page 20. We have improved justification of assumptions in [Section 5. Discussion and Conclusion] on pages 19-20. We have also emphasised that while more work is required before our framework is ready for use in practical systems, this work nonetheless constitutes significant progress in the right direction.}


\subsection{The coverage of related research seems to me a bit weak. Only three references on such a topic is not much. I guess there is more related work out there. Please rework these section.}

\shout{We have reworked the entire related work coverage in [Section 5. Discussion and Conclusion] and have provided further related references on pages 20-21.}

\subsection{The result section is not very much convincing. First to me it is not clear why a simple toy domain like the towers of Hanoi justifies the usefulness of the algorithm.}
\label{rev3:convincing}

We reiterate that our end goal is to allow use of our learning framework in real applications. Nonetheless, as a first step this toy domain captures several key issues while providing an idealised setting for the extension of our previous work from using synthetic hierarchies \cite{Airiau:IJAT09,Singh:AAMAS10} to ones with real meaning. We further refer the reviewer to our response in \ref{rev2:domain_justification}.


\subsection{Even the two experiments in this domain are made in a ad-hoc manner. For a journal this is too weak. I like to see a deeper treatment of the toy domain and to see experiments in a more advanced, more relevant or even real world domain. Otherwise I am not completely convinced about the advantages of the approach.}
\label{rev3:experiments}

Our experiments were chosen to convey the key benefits of the $ACL+\Omega$ approach using a similar experimental setup as previously \cite{Singh:AAMAS10}.

The first experiment (Figure 5) captures some key points that we wish to highlight. Firstly, it shows the benefits of the structured exploration of $ACL+\Omega$ over the baseline $ACL$ approach \cite{Airiau:IJAT09,Singh:AAMAS10}. The result also confirms the merits of the exploration-based confidence idea that in it's original form \cite{Singh:AAMAS10} did not scale to recursive programs and formed the motivation for this work. Secondly, the experiment reinforces the intuition that the structured $ACL+\Omega$ approach is likely to show more gains where the solution is more complex. For instance, for solutions requiring five levels of recursion, ACL achieves only $50\%$ success by the end of the experiment at $5k$ iterations whereas $ACL+\Omega$ achieves $95\%$ success within $3.5k$ iterations. For levels three and one, the advantages of $ACL+\Omega$ over $ACL$ are progressively less. 

The first experiment equips us to understand the performance of the two approaches in the full experiment shown in Figure 6a. We have now added a second view of the results of experiment two in Figure 6b that shows the progressive discovery of solutions for each approach. Here the benefit of $ACL+\Omega$ is clear. Figure 6b shows that $ACL+\Omega$ resolves all $52$ goals within $12k$ iterations whereas $ACL$ resolves only $47$ by the end of the experiment at $20k$. At a similar point of comparison, to resolve $47$ goals $ACL+\Omega$ takes around $6.4k$ iterations whereas $ACL$ takes more than twice as long at $17.7k$. 

Perhaps it is the choice of the experiment with applicability thresholds that seems ad-hoc to the reader in this context, however it is there to emphasise that the improved $ACL+\Omega$ avoids this issue in a manner  (of experiment) similar to the coverage approach \cite{Singh:AAMAS10} it lends from.

\shout{We have added Figure 6b and significantly reworded [Section 4.2 Results] to strengthen our case.}


\begin{thebibliography}{10}

\bibitem{Airiau:IJAT09}
S.~Airiau, L.~Padgham, S.~Sardina, and S.~Sen.
\newblock Enhancing Adaptation in {BDI} Agents Using Learning Techniques.
\newblock In {\em International Journal of Agent Technologies and Systems}
(IJATS), 1(2):1-18, January 2009. 

\bibitem{Singh:AAMAS10}
D.~Singh, S.~Sardina, L.~Padgham, S.~Airiau.
\newblock Learning Context Conditions for {BDI} Plan Selection.
\newblock In {\em Proceedings of Autonomous Agents and Multi-Agent Systems
(AAMAS)}, Toronto, Canada, 2010. To appear.

\bibitem{Singh:HYCAS10}
D.~Singh, S.~Sardina, L.~Padgham.
\newblock Extending BDI Plan Selection to Incorporate Learning from Experience.
\newblock Submission under review for the Special Issue on Hybrid Control of Autonomous Systems (HYCAS), Journal of Robotics and Autonomous Systems, 2010.


\end{thebibliography}

\end{document}
