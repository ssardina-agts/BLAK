%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Conclusion}\label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This paper builds on earlier work that extends
the typical BDI programming framework to use \dt{}s as (part of) a
plan's context condition, with a probabilistic plan selection
mechanism that caters for both exploration and exploitation of plans.
We have shown in earlier work that due to the structure of BDI
programs, care must be taken in how learning is used, to avoid
problems in certain situations. In some cases these problems lead to
failure to learn at all, as we also show here.

In this paper we extend previous work to allow for parameterised
goals such as {\it Travel(\$from, \$to)} and also for recursion, both
of which are necessary for real applications. In doing this our
previous confidence measure which relied on a finite goal-plan tree
did not scale, so we have provided a more approximate measure, relying
on the principles that have been shown correct, but without the
limitations. This paper also takes an existing BDI program involving
parameterised goals and recursion, and evaluates our approach using
this program. By removing the existing context conditions, and then
learning the correct behaviour, we show that we are able to obtain
good (although not perfect\footnote{We would hope that when learning
is combined with programmer provided context conditions, the problems
preventing perfect learning here would be avoided.}) performance. We
also demonstrate that the naive approach to learning, that does not
account for the BDI program structure fails to learn given
some program structures and an applicability threshold.

The work still contains a number of simplifying assumptions which will
need to be addressed before being able to develop {\it practical} on-line
learning for situated agents. Currently the work has not been
integrated with standard BDI failure handling and recovery. Clearly
this will be needed, but we do not expect that it will undermine any
of our results described here. In fact a careful integration of
failure handling could improve the speed of learning as multiple
attempts could be made to achieve a (sub)-goal. However care will need
to be taken regarding changes to world state and possible interactions
between failed attempts and eventually successful ones.

%
Our current approach does not detect and learn interactions between
sibling goals in the context of a particular parent; each
subgoal is treated ``locally.'' To handle such interactions, the
selection of a plan for resolving a sub-goal should also be predicated
on the goals higher than the sub-goal, that is, it should take into
account the ``reasons'' for the sub-goal.

%An issue with our use of a coverage-based confidence measure is
%that it has a defined end state, namely $c_p=1$. However, in a system
%where the environment is always changing, re-learning will need to occur 
%indefinitely and the framework must allow for this.
% as the agent continually tries to
%\emph{adapt}. One option is to have an explicit resetting of the
%confidence factor when the agent is told, or deduces, that the
%environment has changed. Deduction could use a number of approaches
%such as noticing a lack of correlation in previously correlated
%features, 
%or noticing when a plan that has
%previously suceeded reliably begins to fail, concluding that the
%world has changed in a way that is relevant for the achievement of the
%goal related to that plan.

Finally, we note that our current use of \dt s is naive. 
For instance, in our framework all execution data is maintained forever and 
\dt s re-built after each plan execution. Furthermore, we learn using actual 
world states, whereas an improvement would be to learn using relational world information.
While not ideal, our approach nevertheless allows us to focus on 
understanding the complexities of learning in BDI programs first 
without worrying about the underlying techniques.

The issue of combining learning and deliberative approaches for online 
decision making in BDI-like systems has not been widely addressed.
% %
In~\cite{Riedmiller01}, \textit{preiously learnt} low level robot soccer skills are 
used in the deliberative decision making process once deployed.
% %
Hern\'andez et al. \cite{Hernandez04:Learning} give a preliminary
account of how decision trees may be induced on plan failures in order
to find alternative logical context conditions in a deterministic
paint-world example. 
% %
A closely related area to BDI is that of hierarchical task network (HTN)
systems where task decompositions used are similar to BDI goal-plan hierarchies.
Recently, in similarly motivated work to ours, \cite{Zhuo09:Learning} 
proposed a method for learning HTN method preconditions under partial
observations. There, a set of constraints are constructed from
observed decomposition trees that are then solved \emph{offline} using
a constraint solver.
% %
In contrast, in our work learning and deliberation are fully integrated in a
way that one impacts the other and the classical
exploration/exploitation dilemma applies. 

% %
Although there is still work to do before we can expect learning to be
successfully integrated into a fully autonomous BDI agent, the
work reported here is significant in that it provides a solid
foundation for adding new capabilities to BDI agents to allow them to
learn and adapt based on experience.

