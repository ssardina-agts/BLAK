%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Conclusion}\label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This paper builds on previous work where we have shown how to extend
the typical BDI programming framework to use \dt{}s as (part of) a
plan's context condition, with a probabilistic plan selection
mechanism that caters for both exploration and exploitation of plans.
We have shown in earlier work that due to the structure of BDI
programs, care must be taken in how learning is used, to avoid
problems in certain structures. In some cases these problems lead to
failure to learn at all, as was also illustrated in this paper.

In this paper we extend previous work to allow for parameterised
goals such as {\it Travel(\$from, \$to)} and also for recursion, both
of which are necessary for real applications. In doing this our
previous confidence measure which relied on a finite goal-plan tree
did not scale, so we have provided a more approximate measure, relying
on the principles that have been shown correct, but without the
limitations. This paper also takes an existing BDI program involving
parameterised goals and recursion, and evaluates our approach using
this program. By removing the existing context conditions, and then
learning the correct behaviour, we show that we are able to obtain
good (although not perfect\footnote{We would hope that when learning
is combined with programmer provided context conditions, the problems
preventing perfect learning here would be avoided.}) performance. We
also demonstrate that the naive approach to learning, which does not
take account of the BDI program structure will fail to learn given
some program structures, and an applicability threshold.

The work still contains a number of simplifying assumptions which will
need to be addressed before being able to develop {\it practical} on-line
learning for situated agents. Currently the work has not been
integrated with standard BDI failure handling and recovery. Clearly
this will be needed, but we do not expect that it will undermine any
of our results described here. In fact a careful integration of
failure handling could improve the speed of learning as multiple
attempts could be made to achieve a (sub)-goal. However care will need
to be taken regarding changes to world state and possible interactions
between failed attempts and eventually successful ones.

%
Also our current approach will not detect and learn interactions between
sibling goals in the context of a particular parent; each
subgoal is treated ``locally.'' To handle such interactions, the
selection of a plan for resolving a sub-goal should also be predicated
on the goals higher than the sub-goal, that is, it should take into
account the ``reasons'' for the sub-goal.

One issue with the use of a coverage-based confidence measure used is
that it has a defined end state, namely $c_p=1$. However, in a system
where the environment may actually be changing, then 
re-learning will need to occur indefinitely, as the agent continually tries to
\emph{adapt}. One option is to have an explicit resetting of the
confidence factor when the agent is told, or deduces, that the
environment has changed. Deduction could use a number of approaches
such as noticing a lack of correlation in previously correlated
features, 
or noticing when a plan that has
previously suceeded reliably begins to fail, concluding that the
world has changed in a way that is relevant for the achievement of the
goal related to that plan.

The issue of combining learning and deliberative approaches for decision making
in autonomous systems has not been widely addressed.
% %
In~\cite{Riedmiller01}, learning is used \emph{prior to deployment}
for acquiring low level robot soccer skills that are then treated as
fixed methods in the deliberative decision making process once deployed.
% %
Hern\'andez et al. \cite{Hernandez04:Learning} give a preliminary
account of how decision trees may be induced on plan failures in order
to find an alternative logical context conditions in a deterministic
paint-world example. 
% %
More recently, \cite{Zhuo09:Learning} proposes a method for learning
hierarchical task network (HTN) method preconditions under partial
observations. There, a set of  constraints are constructed from
observed decomposition trees that are then solved \emph{offline} using
a constraint solver. Despite HTN systems being automated planning
frameworks, rather than execution frameworks, these are highly 
related to BDI agent systems when it comes to the \emph{know-how} information
used---learning methods' preconditions amounts to learning plan's context
conditions.
% %
In contrast, in our work, learning and deliberation are fully integrated in a
way that one impacts the other and the classical
exploration/exploitation dilemma applies. 

% %
Although there is still work to do before we can expect learning to be
successfully integrated into a fully autonomous BDI agent, the
work reported here is significant in that it provides a solid
foundation for adding new capabilities to BDI agents to allow them to
learn and adapt based on experience.

