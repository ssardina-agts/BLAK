%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Informed Plan Selection}\label{sec:coverage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\notems{new intro}
A middle-ground between the two extreme learning approaches \CL\ and \BUL\ can be
obtained if the ``confidence'' test is realized at plan selection time, rather
than at learning time.
% %
The idea is that confidence in a plan's \dt\ increases as more of the possible
choices below the plan in the goal-plan structure are explored.

\notems{changed $T$ to $P$}
So, with each plan in the goal-plan tree hierarchy, we identify its set of
potential \textit{choices} as the set of all potential execution paths
\textit{below} the plan in the hierarchy. This can easily be computed offline.
% %
Intuitively, a plan's \dt\ is more \textit{informed} for a world state if it is
based on a larger number of choices having been explored in that state. We say
that a plan has a higher degree of \emph{coverage} as more of its underlying
choices are explored and accounted for in the corresponding \dt. Technically,
given a \dt\ for a certain BDI plan $P$, we define its coverage for the world
state $w$ as $c_P(w) \in [0,\ldots,1]$.
% %
Initially, when the plan has not yet been executed in a world $w$, its coverage
in such state is $c_P(w) = 0$ and the agent has no basis for confidence in the
likelihood of success estimated by $P$'s \dt\ for world/belief state $w$.
% %
As the different ways of executing the plan in state $w$ are explored, the value
of $c_P(w)$ approaches $1$. When all choices have been tried, $c_P(w)=1$ and the
agent may rely fully on the \dt\ estimation of success.
% %
In this way, coverage can provide a confidence measure for the \dt\
classification.


Each time a plan execution result is recorded, the coverage
$c_P(w)$ for a world $w$ is calculated and stored.
% %
It requires, in principle, $\tau \times |S|$ \emph{unique} executions of
a plan for it to reach \emph{full} coverage, where $\tau$ is the total number of
choices below the plan and $|S|$ is the number of possible worlds. Practically,
however, it takes significantly less since choices below a plan are effectively
an AND/OR tree, and each time an AND node fails, the subsequent
nodes are not tried and are counted as covered for the world in question.
% %
Also, a plan is generally not executed in every world state, so in
practice it will only need to be assessed in the subset of the world
states that is relevant to it.






\notems{changed $p_T(\cdot)$ to $\kappa_P(\cdot)$}
So, we next construct a probabilistic plan selection function that includes the
coverage-based confidence measure.
% %
Formally, we define the plan selection \emph{weight} $\Omega_P(w)$ as a function
of the \dt\ determined success expectation $\kappa_P(w)$ and the degree of
coverage $c_P(w)$:\footnote{For the \CL\ and \BUL\ schemes discussed above,
$\Omega_P(w)=p_T(w)$, that is, the weight of a plan is exactly its \dt\
estimation of success.}
% %
\begin{equation*}\label{eqn:coverage}   
\Omega_P(w) = 0.5 + \left[  c_P(w)^{1/\alpha} *  \left( \kappa_P(w) - 0.5 \right)  \right], 
\end{equation*}
%%	
where $\alpha \in [0,\ldots,\infty)$ is the coverage amplification factor, with
default value $\alpha=1$.
%%	
Initially, the selection weight of the plan for a previously unseen world state
$w$ takes the default value of $0.5$.
% %
Over time, as the various execution paths below the plan are tried in $w$, its
coverage increases and the selection weight approaches the true value estimated
by the plan's \dt.




Interestingly, the coverage-based account provides a flexible mechanism for
``tuning'' the behavior of the agent depending on application characteristics.
% %
As $\alpha \approx 0$, the $\CLSELB$ framework will behave more like the original
$\BULSELA$ system: $c_P(w)^{1/\alpha}$ transitions directly from $0$ to $1$ when
$c_P(w)$ reaches $1$ (and remains zero otherwise).
% %
On other hand, when $\alpha \approx \infty$, a $\CLSELB$ based agent will behave
more like an $\CLSELA$ based agent: $c_P(w)^{1/\alpha}$ transitions from $0$ to
$1$ faster and $\Omega'(w) \approx \kappa_P(w)$. With $\alpha=1$ we get our
initial equation.
% %
It follows then that \CLSELB\ provides a principled \emph{middle ground} between
the \CLSELA\ and \BULSELA\ schemes.













\subsection{Experiments}


We experimented with the alternative plan selection scheme by studying its impact
with the two learning approaches \CL\ and \BUL\ from the previous section.
% %
We will refer as \CLSELB\ and \BULSELB\ to the learning frameworks obtained when
using \CL\ and \BUL\ approaches, respectively, together with the new
coverage-based probabilistic selection.
% %
Thus, $\CL$ and $\BUL$ correspond to the cases where the \emph{original}
selection weighting using only the \dt{}s' expectation of success (i.e.,
$\Omega_P(w) = \kappa_P(w)$) is used.



So, we set up testbed programs composed of several goals and plans combined in a
hierarchical manner and yielding goal-plan tree structures of different shapes.
% %%
The experiments consisted in posting the top-level goal repetitively under random
world states, running the corresponding  BDI learning agent, and finally
recording whether the execution terminated successfully or not.\footnote{We have
implemented the learning agent system in the \JACK\ BDI platform
\cite{BusettaRHL:AL99-JACK} and used used algorithm \propername{J48}, a version
of \propername{c4.5} \cite{Mitchell97:ML}, from the well-known \weka\ learning
package \cite{weka99} for representation and induction of \dt{}s. The fact that
\JACK\ is a Java-based system and provides powerful meta-level reasoning
capabilities, allows us to integrate \weka\ and probabilistic plan-selection
mechanisms with not much effort. Nonetheless, all the results are independent on
this and any other BDI agent system could have been used.}
% %
We calculate the average rate of success of the goal by first averaging the
results at each time step over $5$ runs of the same experiment, and then
smoothing using a moving average of the previous $100$ time steps to get the
trends reported in the figures.


Our first observation is that the $\BULSELA$ and $\BULSELB$ approaches show
similar performance.
% %
This is not surprising, as the stability test performed by these agents at each
plan node inherently results in close to full coverage. Indeed, for a plan to
become ``stable,'' the agent needs to (substantially) explore (i.e., cover) all
possible ways of executing it. The stability check, then, effectively reduces
$\kappa_P(w)$ to $\Omega_P(w)$.
% %
% So, for simplicity, we shall not give a further account of the $\BULSELB$
% approach in this section.



We now focus on the \CL\ approach.
% %
First of all, for the cases where $\CLSELA$ performs reasonably well compared to
$\BULSELA$-based systems, the new $\CLSELA$ approach maintains comparable
performance.
% %
The benefit of the coverage-based approach is apparent, though, when one
considers the goal-plan structure $\T_2$ in which the $\CLSELA$ performed
poorly (cf. Figure \ref{fig:T2_result}).
% %
Here, the $\CLSELB$ scheme showed a dramatic improvement over $\CLSELA$. Figure
\ref{fig:T2_result2} shows this change with the results for the new approach to
plan selection $\CLSELB$ superimposed over the original results
from Figure \ref{fig:T2_result}.
% %
The reason why the new plan selection mechanism improves the \CL\ learning scheme
is that even though the success estimation $\kappa_P(w)$ for a given plan $P_i$ would
still be low initially (remember that \CL, in contrast with \BUL, would record
all initial failure outcomes for $P_i$), the agent would not be very confident in
such estimation until the plan's coverage increases; therefore the
selection weight $\Omega'_T(w)$ will initially bias towards the default weight of
$0.5$. In other words, the false negative outcomes collected by the agent for
plan $P_i$ would not be considered so seriously due to low plan coverage. As full
coverage is approached, one would expect the agent to have discovered the success
execution encoded in $P_i$.


\begin{figure}[t]
\begin{center}
\input{figs/T2-result2}
\caption{Performance of $\CLSELB$ (solid crosses) in structure $\T_2$ compared against
the earlier results for $\CLSELA$ and $\BULSELA$ (both in dotted grey).}
\label{fig:T2_result2}
\end{center}
\end{figure}



Even more interesting is the the impact of the new plan selection mechanism on
agents that work with an applicability threshold, i.e., agents that may not
select plans that are deemed unlikely to succeed.
% %%%
Here, the original $\CLSELA$ approach completely fails, as it collects many
negative experiences early on, quickly causing plans' success expectation to fall
below the selection threshold. For $\CLSELB$, even if a plan is deemed with very
low expectation of success, its selection weight would be biased towards the
default value of $0.5$ if it has not been substantially ``covered.''
% %
Hence, provided that the applicability threshold is lower than the default plan
selection weight, then $\CLSELB$ is indeed able to find the solution(s).
% %
Figure~\ref{fig:performance-applicability} shows the $\CLSELB$ performance in
goal-plan structure $\T_2$ for an applicability threshold of $0.2$.


\begin{figure}[t]
   \centering
   \input{figs/T4-result.tex}
   \caption{Performance of $\CLSELB$ (solid crosses) compared against $\CLSELA$ and $\BULSELA$ (both in dotted grey) in structure $\T_2$ using an applicability threshold of $0.2$.}
   \label{fig:performance-applicability}
\end{figure}


The above results show that the coverage-based confidence weighting can improve
the performance of the \CL\ approach in those cases where it performed poorly due
to false negative experiences, i.e., failure runs for a plan that includes
successful executions.
% %


Finally, we note that coverage-based selection weights encourage the agent to
explore all available options. This further ensures that all solutions are
systematically found, allowing the agent to decide which solution is optimal
faster. For some domains this may be an important feature.








