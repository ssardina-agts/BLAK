% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\notems{new para}
Agents are an important technology that have the potential to take over
contemporary methods for analysing, designing, and implementing complex software
systems suitable for domains such as telecommunications, industrial control,
business process management, transportation, logistics, and aeronautics
\cite{Jennings:COMACM01,Belecheanu:AAMAS06}.
% %
The BDI model of agency \cite{Pollack:AIJ92-IRMA,Bratman88} is a popular and
well-studied approach with substantial theoretical and practical work. It has its
roots in philosophy with Bratman's \cite{Bratman87:Intentions} theory of practical
reasoning and Dennett's theory of intentional systems \cite{Dennet97:IntentionalStance}.
% %
A recent industry study \cite{Benfield:AAMAS06} analysing several applications
claimed that the use of BDI (Belief-Desire-Intention) agent technology in complex
business settings can improve overall project productivity by an average 350\% to
500\%. Also the agent approach allowed the business to change and extend
solutions quickly helping to bridge the semantic gap between the business side
and IT development.




In this paper, we are concerned with one of the key aspects of the BDI
agent-oriented programming paradigm, namely, that of \emph{intelligent plan
selection} \cite{Pollack92-IRMA,Georgeff89-PRS}.
% %
Specifically, we explore the details of how effective plan selection can be
learnt based on ongoing experience.


There are a plethora of agent programming languages and development platforms in
the BDI tradition, such as  \PRS\ \cite{Georgeff89-PRS},
\JACK~\cite{Busetta99jack}, \TAPL~\cite{Hindriks99:Agent} and
\DAPL~\cite{Dastani:JAAMAS08-2APL}, \JASON~\cite{jasonbook}, and SRI's
\SPARK~\cite{MorelyM:AAMAS04-SPARK}, among others. %%
% %
\notems{added ``reactively''}
Generally speaking, these systems enable \emph{abstract plans} written by
programmers to be combined and used \emph{reactively} in real-time, in a way that
is both flexible and robust. Concretely, a BDI agent is built around a
\textit{plan library}, a collection of pre-defined \textit{hierarchical plans}
indexed by goals and representing the standard operational procedures of the
domain (e.g., landing a plane).
% %
The so-called \emph{context condition} attached to each plan states the
conditions under which the plan is a sensible strategy to address the
corresponding goal in a given situation (e.g., it is not raining). The execution
of a BDI system relies then entirely on \textit{context sensitive subgoal
expansion}, allowing agents to ``act as they go'' by making \emph{plan choices}
at each level of abstraction with respect to the current situation.



The fact that both the actual behaviours (the plans) and the situations for which
they are appropriate (their context conditions) are fixed at design time has
important implications for the whole programming approach.
% %
First, it is often difficult or impossible for the programmer to craft the
\emph{exact} conditions under which a plan would succeed. Second, once deployed,
the plan selection mechanism is fixed and may not adapt to potential variations
of different environments.
% %
Finally, since plan execution often involves interaction with a \emph{partially
observable} external world, it is desirable to measure success in terms of
probabilities rather than boolean values.


%Following~\cite{Airiau:IJAT09}, we propose a BDI programming framework where plans'
%context conditions can be learnt over time, by exploring the plan library and
%suitably incorporating the outcomes into plans' context conditions.
%% %
%To that end, we first extend the standard BDI programming architecture by
%modeling context conditions of plans with \emph{decision trees}
%\cite{Mitchell97:ML} and developing a \emph{probabilistic plan selection scheme}
%compatible with the new representation (Section~\ref{sec:framework}). %%
%% %
%We then  develop and empirically investigate an aggressive approach (called \CL) and a
%conservative one (called \BUL) to learning such context decision trees, and
%showed that they provide different advantages depending on the implicit structure
%of the plan library (Sections~\ref{sec:context_learning} and
%\ref{sec:experiments}). 
In \cite{Airiau:IJAT09} the authors detail why it may be problematic for
learning if the BDI hierarchical structure is not taken into account.
They showed why it can be problematic to assume a mistake at a
higher level in the hierarchy, when a poor outcome may have been
related to a mistake in selection further down.  Their
empirical work suggested that perhaps the advantages of an aggressive
approach, where all failures were considered meaningful, outweighed
the advantages of careful consideration.
%
In this work, we show that the aggressive scheme, though simpler, can at times lead to a complete inability to learn, thus yielding a much less robust performance than the conservative alternative.
% %
Informally, the difference between the two approaches is that whereas the
aggressive approach accounts for every execution failure, the conservative
approach instead considers failures only when decisions made during the execution
are deemed sufficiently ``informed.''


In order to obtain a flexible middle-ground between the above two approaches, we
devise an extended probabilistic plan selection that includes a \emph{confidence}
measure based on how much the agent has explored the space of possible
executions of a given plan (Section~\ref{sec:coverage}). The more this space has
been ``covered'' by previous executions, the more the agent ``trusts'' the
estimation of success provided by the plan's decision tree.
% %
It turns out that an agent based on the aggressive learning scheme together
with the enhanced plan selection mechanism provides a flexible and simple
compromise between the two extreme approaches.



We point out that our new account of plans' context conditions is
\emph{compatible} with the standard formula-based one.
% %
Indeed, our approach can be easily combined with the standard plan selection
mechanism, by allowing the agent programmer to provide initial context conditions
that could be later automatically ``refined'' by the agent system. By doing so,
one can effectively take a BDI program and ``tune it'' using our learning
framework.
% %
For simplicity, though, context conditions are learnt from scratch in our
experimental work.





% The rest of the paper is organized as follows. In the next section, we
% provide a quick overview of typical BDI-style agents.
% %
% We then discuss modifications to the BDI framework to accommodate learning capabilities by
% extending context conditions and the agent plan-selection function. 
% %
% After that, we describe two approaches to learning the context
% condition of plans, aggressive and careful consideration.
% %
% We report on the empirical results obtained and analyse situations in
% which each is advantageous or problematic.
% %
% We then develop the notion of coverage assessment to influence plan
% selection and show that this addresses many of the issues that can
% otherwise arise when using the aggressive learning approach.
% %
% We conclude that care must be taken to
% achieve robust and successful learning in a typical BDI hierarchical
% program, and that in the absence of specialised information about
% program structure, an aggressive learning approach with coverage
% assessment during plan selection, is most likely to perform best.
%
%We conclude with a brief summary and discussion.
