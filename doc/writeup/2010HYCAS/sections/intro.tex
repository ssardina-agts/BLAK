% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\notems{new para; new refs}
Agents are an important technology that have the potential to take over
contemporary methods for analysing, designing, and implementing complex software
systems suitable for domains such as telecommunications, industrial control,
business process management, transportation, logistics, and aeronautics
\cite{Jennings:COMACM01,Belecheanu:AAMAS06,Ljungberg:PRICAI92-OASIS,Ziming:AAC07}.
% %
The BDI model of agency \cite{Pollack:AIJ92-IRMA,Bratman88} is a popular and
well-studied approach with substantial theoretical and practical work. It has its
roots in philosophy with Bratman's \cite{Bratman87:Intentions} theory of practical
reasoning and Dennett's theory of intentional systems \cite{Dennet97:IntentionalStance}.
% %
A recent industry study \cite{Benfield:AAMAS06} analysing several applications
claimed that the use of BDI (Belief-Desire-Intention) agent technology in complex
business settings can improve overall project productivity by an average 350\% to
500\%. Also the agent approach allowed the business to change and extend
solutions quickly helping to bridge the semantic gap between the business side
and IT development.
%%
BDI systems have built into them an ability to balance pro-actively
pursuing a goal, with reactively responding to the environment. They
also have a well developed failure recovery mechanism. This makes them
very suitable for robotics applications operating in a physical world
which is often more error prone than a software domain.

BDI systems, despite their strengths, do not however incorporate any
ability to learn from experience. Our work makes a start at addressing
this issue, focussing specifically on learning which plan to select,
given a particular goal and world state.

There are many agent programming languages and development platforms
in the BDI tradition, including
%such as \PRS\ \cite{Georgeff89-PRS},
\JACK~\cite{BusettaRHL:AL99-JACK}, 
\JADEX~\cite{PokahrBL:EXP03-JADEX}, and
%\TAPL~\cite{Hindriks99:Agent} and
%\DAPL~\cite{Dastani:JAAMAS08-2APL}, 
\JASON~\cite{jasonbook}
%, and SRI's \SPARK~\cite{MorelyM:AAMAS04-SPARK}, 
among others. 
%% 
All of them
follow a similar basic architecture, whereby \emph{abstract plans}
written by programmers are combined and used \emph{reactively} in
real-time, in a way that is both flexible and robust. Concretely, a
BDI agent is built around a \textit{plan library}, a collection of
pre-defined \textit{hierarchical plans} indexed by goals and
representing the standard operational procedures of the domain (e.g.,
landing a plane).
%
A \emph{context condition} attached to each plan states the
conditions under which the plan is a sensible strategy to address the
corresponding goal in a given situation (e.g., it is not raining). The execution
of a BDI system then relies on \textit{context sensitive subgoal
expansion}, allowing agents to ``act as they go'' by making \emph{plan choices}
at each level of abstraction with respect to the current situation.
%
Although this is quite flexible and effective, an important drawback
is the lack of ability to learn from ongoing experience. An ability to
{\it learn} regarding the selection of plans in particular situations,
adds a whole new layer of robustness and flexibility. Firstly there
may be situations where it is difficult to determine in advance the
exact situation under which a particular approach is likely to
succeed. This is especially the case when it involves complex combinations of
values of environmental variables. Secondly, an environment may change
over time, or be slightly different in different deployment
locations. The ability for the agent system to observe and learn
from its performance is obviously a very desirable property.

In our work we achieve this by replacing (or augmenting) the usual
boolean formula for representation of context conditions, by a
decision tree \cite{Mitchell97:ML} which is learnt based on
experience. Our plan selection is then based on a probabilistic
approach, usually choosing the plan which has the highest likelihood
of success, based on experience. This probabilistic approach is also
more suitable than the standard boolean approach for complex, often
partially observable worlds, where various plans may be worth trying,
but have different chances of succeeding.

There are however various nuances that must be addressed in using
decision trees. There is the usual balance between exploration and
exploitation evident in all learning. There are also particular issues
to do with the hierarchical representation of BDI programs. 
%
We have explored these issues in previous papers
\cite{Airiau:IJAT:09,Singh:AAMAS10}, looking at various approaches to the
issues. In this paper we add the ability to deal with paramaterised
goals and with recursive calls, both of which are essential for real
applications. Unfortunately, once we add this expressivity our
previous preferred approach does not scale. Consequently we develop a
simplified approximation to achieve the same basic intuition which we
have previously shown to be correct in principle. We then take an existing BDI
program from the JACK tutorial, remove the given context conditions,
and show how we can learn the appropriate use of the plans provided
using our new approach.

Our approach can easily be combined with the standard plan selection
mechanism, by allowing the agent programmer to provide initial context conditions
that could later be automatically ``refined'' by the agent system. By doing so,
one can effectively take a BDI program and ``tune it'' using our learning
framework.
% %
For simplicity, though, context conditions are learnt from scratch in our
experimental work.

In the next section we provide an introduction to and overview of both
BDI programming and our learning framework, as well as an overview of
our previous approaches. We then describe in detail
the learning framework that incorporates the additional aspects of
parameterised goals and recursive calls, with our revised approach to
address previously identified issues.
We show empirical evaluation on an example program developed by
Agent Oriented Software for their JACK tutorial, by removing the context
conditions and applying our learning approach.  We finish with a
discussion of outstanding issues and related work.

% The rest of the paper is organized as follows. In the next section, we
% provide a quick overview of typical BDI-style agents.
% %
% We then discuss modifications to the BDI framework to accommodate learning capabilities by
% extending context conditions and the agent plan-selection function. 
% %
% After that, we describe two approaches to learning the context
% condition of plans, aggressive and careful consideration.
% %
% We report on the empirical results obtained and analyse situations in
% which each is advantageous or problematic.
% %
% We then develop the notion of coverage assessment to influence plan
% selection and show that this addresses many of the issues that can
% otherwise arise when using the aggressive learning approach.
% %
% We conclude that care must be taken to
% achieve robust and successful learning in a typical BDI hierarchical
% program, and that in the absence of specialised information about
% program structure, an aggressive learning approach with coverage
% assessment during plan selection, is most likely to perform best.
%
%We conclude with a brief summary and discussion.
