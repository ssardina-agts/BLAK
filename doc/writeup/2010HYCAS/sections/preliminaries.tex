%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}\label{sec:preliminaries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\success}{\mbox{\emph{succ}}}
\newcommand{\failure}{\mbox{\emph{fail}}}

\newcommand{\procedurefont}[1]{\mathsf{#1}}
\newcommand{\StableGoal}{\procedurefont{StableGoal}}
\newcommand{\RecordTrace}{\procedurefont{RecordFailedTrace}}
\newcommand{\RecordWorldDT}{\procedurefont{RecordWorldDT}}



\subsection{BDI Agent-Oriented Programming}\label{sec:bdi_programming}

\notems{new para}
BDI agent-oriented programming is a popular, well-studied, and practical paradigm
for building intelligent agents situated in complex and dynamic environments with
(soft) real-time reasoning and control requirements
\cite{Georgeff89-PRS,Benfield:AAMAS06}.
% %
Generally speaking, BDI agent-oriented programming languages are built around an
explicit representation of propositional attitudes (e.g., beliefs, desires,
intentions, etc.). A BDI architecture addresses how these components are
represented, updated, and processed to determine the agent's actions.
% .
There are a number of agent programming languages and development platforms in
the BDI tradition, such as \PRS~\cite{IngrandGR:IEEE92-PRS} and
\dMARS~\cite{Inverno:JAAMAS04-dMARS}, \AgentSpeak\ and
\JASON~\cite{Rao:LNCS96-AgentSpeak,Bordini:07-JASONBOOK},
\JADEX~\cite{PokahrBL:EXP03-JADEX},
\TAPL~\cite{Hindriks:JAAMAS99-3APL,DastaniRM:05} and
\DAPL~\cite{Dastani:JAAMAS08-2APL}, \GOAL~\cite{BoerHHM:JAPL07-GOAL},
\JACK~\cite{BusettaRHL:AL99-JACK}, SRI's \SPARK~\cite{MorelyM:AAMAS04-SPARK}, and
\JAM~\cite{Huber:AGENTS99-JAM}.



\begin{figure}[t]
\begin{center}
\resizebox{.9\textwidth}{!}{\input{figs/figbdiarch}}
\end{center}
\caption{A typical BDI-style architecture}
\label{fig:bdiarch}
\end{figure}

In a BDI-style system, an agent consists, basically, of a belief base (akin to a
database), a set of recorded pending goal events, a plan library, and an
intention base.
% %
Figure~\ref{fig:bdiarch} depicts a typical BDI architecture.
% %
While the belief base encodes the agent's knowledge about the world, the pending
events stand for the \emph{goals} the agent wants to achieve/resolve.
% %
The \textit{plan library}, in turn, contains \emph{plan rules}, or simply
\emph{plans}, of the general form $e: \psi \leftarrow \delta$ encoding the
standard domain operational procedure $\delta$ (that is, a program) for achieving
the event-goal $e$ when the so-called \textit{context condition} $\psi$ is
believed true---program $\delta$ is a reasonable strategy to resolve event $e$
whenever $\psi$ holds.
% %
\notems{UAV example}
For example, a BDI controller for an unmanned aerial vehicle (UAV)
\cite{Karim:AAMAS05,Ziming:AAC07} may include a certain procedure $\delta$ for
achieving the goal of \emph{landing the aircraft} (the event) under \emph{normal
weather conditions} (the context).
% %
Among other operations, the plan-body program $\delta$ will typically include the
execution of actions ($act$) in the environment and subgoal events ($!e$) that
ought to be in turn resolved by selecting suitable plans for that subgoal event.
Lastly, the intention base accounts for the current, partially instantiated,
plans that the agent has already committed to in order to handle or achieve some
event-goal.


The basic \emph{reactive goal-oriented behavior} of BDI systems involves the
system responding to events, the inputs to the system, by committing to handle
one pending event-goal, selecting a plan from the library, and placing its
program body  into the intention base.
% %%
A plan may be selected if it is \textit{relevant} and \textit{applicable}, that
is, if it is a plan designed for the event in question and its context condition
is believed true, respectively.
% %
In contrast with traditional planning, execution happens at each step. The
assumption is that the use of plans' context-preconditions to make choices as
late as possible, together with the built-in goal-failure mechanisms, ensures
that a successful execution will eventually be obtained while the system is
sufficiently responsive to changes in the environment.

\notems{fixes in para}
For the purposes of this paper, we shall mostly focus on the plan library, as we
investigate ways of learning how agents can make a better use of it over time.
% %
It is not hard to see that, by grouping together plans responding to the same
event type, the plan library can be seen as a set of \emph{goal-plan tree}
templates: a goal (or event) node has children representing the alternative
relevant plans for achieving it; and a plan node, in turn, has children nodes
representing the subgoals (including primitive actions) of the plan.
% %%
These structures can be seen as AND/OR trees: for a plan to succeed all the
subgoals and actions of the plan must be successful (AND); for a subgoal to
succeed one of the plans to achieve it must succeed (OR).




\begin{figure}[t]
\begin{center}
\input{figs/GPtree-T3}
\end{center}
\caption{Goal-plan hierarchy $\T_3$. There are $2^4$ worlds whose solutions are
distributed evenly in each of the $4$ top level plans. Successful execution
traces are of length $4$. Within each sub-tree $P_i$, \BUL\ is expected to
perform better for a given world, but it suffers in the number of worlds. Overall, \CL\ and \BUL\
perform equally well in this structure.}
\label{fig:T3}
\end{figure}


Consider, for instance, the hierarachicaly structure sown in Figure~\ref{fig:T3}.
% %%
A link from a goal to a plan means that this plan is relevant (i.e., potentially
suitable) for achieving the goal (e.g., $P_1 \ldots P4$ are the relevant plans
for event goal $G$); whereas a link from a plan to a goal means that the plan
needs to achieve that goal as part of its (sequential) execution (e.g., plan
$P_A$ needs to achieve goal $G_{A1}$ first and then $G_{A2}$).
% %
For compactness, an edge with a label $\times n$ states that there are $n$ edges
of such type.
% %
Leaf plans directly interact with the environment and so, in a given world state,
they can either succeed or fail when executed; this is marked accordingly in the
figure \emph{for some particular world} (of course, in other states, such plans
may behave differently).
% %
In some world, given successful completion of $G_A$ first, the agent may achieve
goal $G_B$ by selecting and executing $P_B$, followed by selecting and executing
$2$ leaf working plans to resolve goals $G_{B1}$ and $G_{B2}$. If the agent
succeeds with goals $G_{B1}$ and $G_{B2}$, then it succeeds for plan $P_B$,
achieving thus goal $G_B$ and the top-level goal $G$ itself. There is no possible
successful execution, though, if the agent decides to carry on any of the three
plans labelled $P_{B2}'$ for achieving the low-level goal $G_{B2}$.





As one can easily observe, the problem of \textit{plan-selection} is at the core
of the whole BDI approach:
\emph{which plan should the agent commit to in order to achieve a certain goal?}
% %
This problem amounts, at least partly, to what has been referred to as
\emph{means-end analysis} in the agent foundational literature
\cite{Pollack92-IRMA,Bratman88}, that is, the decision of \textit{how} goals are
achieved.
% %%
To tackle the plan-selection task, state-of-the-art BDI systems leverage domain
expertise by means of the context conditions of plans. However, crafting fully
correct context conditions at design-time can, in some applications, be a
demanding and error-prone task. In addition, fixed context conditions do not
allow agents to adapt to changing environments.
% %%
\notems{changed last sentence} Thus, in this work, we shall be interested in
exploring how an agent, while acting, can \emph{learn} how to better select the
plans available in order to improve goal achievability over time.



















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Learning Context Preconditions in BDI Systems}\label{sec:BDI_learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In~\cite{Airiau:IJAT:09}, a framework for incorporating learning into typical BDI
agent-oriented programming was proposed.
% %
Generally speaking, the idea is to allow agents to determine which plan to
execute next in order to address/resolve an event by relying on past execution
data and the current world state.



Technically, instead of representing context conditions of plans with belief
(boolean) formulas (as standard practice in BDI programming), decision
trees~\cite{Mitchell97:ML} are used to ``judge'' how reasonable a strategy is if
applied in a certain world state.
% %%
More concretely, each plan in the agent's library is associated with a \dt\ that
is meant to classify belief states into an expectation of whether the plan will
succeed or fail, if executed in such state. In that way, for each relevant plan,
the plan's \dt\ will give the agent information regarding how likely it is to
succeed/fail in a particular world state.
% %
The issue then is how to effectively \emph{induce} such \dt{}s from previous
executions. More importantly, that learning has to happen \emph{while the agent is
executing in the domain of concern}. As a result, how the agent system
\emph{uses} the decision trees for plan selection also becomes and important
and non-trivial issue to address.


To provide an integrated account for \emph{learning} and \emph{using} plans'
context decision trees, the BDI learning framework proposed in
\cite{Airiau:IJAT:09} explored two learning approaches and a probabilistic plan
selection scheme.
% %
The \emph{probabilistic plan selection} scheme states that, given an event-goal
$e$ to be resolved in a world/belief state $w$, the agent chooses a plan $P_i$
among the relevant plans for $e$ with a probability directly proportional to its
estimation of success in state $w$ (by $P_i$'s decision tree) and normalized
among all the success estimation of all relevant options. An default success
estimation of $.5$ is used for plans that have never been executed.
% %
Using such a probabilistic plan selection, the BDI learning system allows for both
\emph{exploitation} and \emph{exploration} of the given know-how information
encoded in the plan library.
% %
Observe that typical BDI platforms do offer various advanced mechanisms for plan
selection, including plan precedence and meta-level reasoning. However, these
mechanisms are pre-programmed and do not take into account the experience of the
agent.

Integrated with the probabilistic plan selection scheme, the aforementioned work
proposed and empirically evaluated two approaches for learning the decision trees
of plans, that is, for \emph{inducing} plans' \dt\ based on previous executions
of such plans.
% %
The two approaches differ on which situations a failed execution of a certain
plan is considered meaningful for the learning process:
% %
\begin{itemize}
  
  \item An aggressive approach, referred as \CL\ (aggressive concurrent learning),
  considers every execution of a plan as meaningful; hence, every execution of
  a plan is ``recorded'' as part of training set for its corresponding \dt.

  
  \item A conservative approach, referred as \BUL\ (bottom-up learning),
  considers failed executions of a plan meaningful only when decisions taken
  during its execution were deemed ``well-informed;'' only such executions 
  are ``recorded'' against the corresponding \dt.
\end{itemize}


Since BDI programs (programs $\delta$ in BDI plans $P$) generally involve the
achievement of sub-goal events, which in turn can involve further sub-goal
events, the rationale behind the \BUL\ approach is that failed executions of a
plan may in fact be due to \emph{wrong choices} during its execution.



For example, consider the structure in Figure \ref{fig:T3} and suppose that plan
$P_3$ is selected in order to resolve top-goal $G$ in world state $w_1$. The plan
involves, in turn, the successful resolution of sequential goals $G_A$ and $G_B$.
Suppose further that subgoal $G_A$ has been resolved successfully, yielding new
state $w_2$, and that plan $P_B$ has been chosen next to try and achieve the
second subgoal $G_B$.
% %
Suppose next that the first subgoal of plan $P_B$, namely $G_{B1}$ has been
successfully resolved, yielding new state $w_3$, but that the non-working plan
$P_{B2}'$ for subgoal $G_{B2}$ is selected in $w_3$ and execution thus
\emph{fails}.
% %
Such failure will be propagated upwards in the hierarchy, causing goals $G_{B2}$
as well as $G_B$ and top-level goal $G$ itself to fail.\footnote{No failure
recovery was assumed.}
% %
Although the failure must be recorded in the decision tree of the plan where the
failure \emph{originated}, namely, plan $P_{B2}'$ world state $w_3$, it is not
clear whether the failure should also be recorded in the decision trees for plans
higher up in the hierarchy.
% %
The \CL\ learning approach will indeed do so and hence record the failure
execution against plans $P_B$ and $P_3$, in worlds $w_2$ and $w_1$ respectively.
% %
On the other hand, the more conservative \BUL\ scheme will not do so if it
considers that the failure of subgoal $G_{B2}$ in plan $P_B$, for instance, might
have been avoided, had the alternative plan $P_{B2}$ been chosen instead.
% % Therefore, recording a failure against plans $P_B$ and $P_3$ would not be
% justified---it is not true that plan $P_{B}$ is a ``bad'' option in world state
% $w_2$. %
So, the \BUL\ approach will \emph{wait} before recording failures against a plan
until it is reasonably confident that subsequent choices down the goal-plan tree
hierarchy were ``well informed'' (e.g., until the plan selection for goals
$G_{B2}$ and $G_B$ are deemed informed).\footnote{Whereas the work in
\cite{Airiau:IJAT:09} uses a simplistic account based on plans being executed a
minimum number of times for judging ``informed'' decisions, the more recent work
\cite{Singh:AAMAS10} uses a substantially more grounded account based on the
notion of plan and goal ``stability.''}

% 
% Whereas the work in \cite{Airiau:IJAT:09} uses a simplicit account for judging
% ``informed'' decisions, the more recent work \cite{Singh:AAMAS10} uses a
% substantially more grounded account. The former requires plans to be executed a
% \emph{fixed} number of times for failure executions to be recorded; the latter
% requires plans and goals to reach certain ``stability'' degree for decisions to
% have deemed informed. A plan is stable if its estimation of success is changing
% below a certain threashold $\epsilon$.
% 

\notems{this could be moved as intro of section 3}
The \CL\ approach is simpler in nature, as no special reasoning is required to
judge whether an execution experience of a plan should be considered as
``training'' data for its decision tree learning.
% %
On the other hand, the \BUL\ account exhibits more robust behaviour since false
negative executions---failing executions of plans that do encode successful
runs---are \emph{not} taken into account for the learning process.
% %
In fact, it was shown in \cite{Singh:AAMAS10} that, when applicability
threasholds are used in the system (i.e., a plan is not tried if its success
estimation is below a certain level), as standard in any realistic application,
the \CL\ scheme may lead to a complete inability to learn, while the \BUL\ scheme
would still be able to learn and eventually yield optimal performance.
% \footnote{That result in fact contradicts the conjecture in
% \cite{Airiau:IJAT09} suggesting that perhaps the advantages of an aggressive
% approach outweighed the advantages of careful consideration.}


The learning approach discussed above suffers from some limitations, especially
when it comes to its applicability in real world settings.
% %
First, it is not clear which of the two extreme learning approach should applied
in a particular setting, the simpler \CL\ agressive approach or the more robust,
though involved, \BUL\ scheme.
% %
Second, plan libraries with event \emph{types} and \emph{recursive} structures
were not considered.
% %
Finally, experiments were based on synthetic examples.
% %
In the rest of the paper, we aim to address all these three issues.









