%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}\label{sec:preliminaries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\success}{\mbox{\emph{succ}}}
\newcommand{\failure}{\mbox{\emph{fail}}}

\newcommand{\procedurefont}[1]{\mathsf{#1}}
\newcommand{\StableGoal}{\procedurefont{StableGoal}}
\newcommand{\RecordTrace}{\procedurefont{RecordFailedTrace}}
\newcommand{\RecordWorldDT}{\procedurefont{RecordWorldDT}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{BDI Agent Systems}\label{sec:bdi_programming}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\notems{new para}
BDI agent-oriented programming is a popular, well-studied, and practical paradigm
for building intelligent agents situated in complex and dynamic environments with
(soft) real-time reasoning and control requirements
\cite{Georgeff89-PRS,Benfield:AAMAS06}.
% %
Generally speaking, BDI agent-oriented programming languages are built
around an 
explicit representation of propositional attitudes (e.g., beliefs, desires,
intentions, etc.). A BDI architecture addresses how these components are
represented, updated, and processed to determine the agent's actions.
% .
%There are a number of agent programming languages and development platforms in
%the BDI tradition, some examples of which include
% \PRS~\cite{IngrandGR:IEEE92-PRS} and
% \dMARS~\cite{Inverno:JAAMAS04-dMARS}, \AgentSpeak\ and
%\JASON~\cite{Bordini:07-JASONBOOK},
%Rao:LNCS96-AgentSpeak,
%\JADEX~\cite{PokahrBL:EXP03-JADEX}, and
% \TAPL~\cite{Hindriks:JAAMAS99-3APL,DastaniRM:05} and
% \DAPL~\cite{Dastani:JAAMAS08-2APL}, \GOAL~\cite{BoerHHM:JAPL07-GOAL},
%\JACK~\cite{BusettaRHL:AL99-JACK}.
%, SRI's \SPARK~\cite{MorelyM:AAMAS04-SPARK}, and
%\JAM~\cite{Huber:AGENTS99-JAM}.
%ABOVE SAID IN INTRO


\begin{figure}[t]
\begin{center}
\resizebox{.7\textwidth}{!}{\input{figs/figbdiarch}}
\end{center}
\caption{A typical BDI-style architecture}
\label{fig:bdiarch}
\end{figure}

A BDI agent consists, basically, of a belief base (akin to a
database) which stores the agent's knowledge about the world, a set of
pending events, which includes both external percepts or messages and
internal goals, a plan library and an intention structure.
% %
Figure~\ref{fig:bdiarch} depicts a typical BDI architecture.
% %
The plan library contains rules of the form $e: \psi \leftarrow
\delta$ indicating that $\delta$ is a suitable procedure for achieving
event-goal $e$ when context condition $\psi$ is true.
% %
%\notems{UAV example}
%For example, a BDI controller for an unmanned aerial vehicle (UAV)
%\cite{Karim:AAMAS05,Ziming:AAC07} may include a certain procedure $\delta$ for
%achieving the goal of \emph{landing the aircraft} (the event) under \emph{normal
%weather conditions} (the context).
% %
Among other operations, the plan body procedure $\delta$ will
typically include the 
execution of actions ($act$) in the environment and subgoal events ($!e$) that
are in turn resolved by selecting suitable plans for that subgoal
event.
%
The intention structure contains the current, partially instantiated,
plans that the agent has committed to in order to handle or
achieve some event-goals.

The basic \emph{reactive goal-oriented behavior} of BDI systems involves the
system responding to events by 
selecting an appropriate plan from the library, and placing its
program body into the intention base.
% %%
A plan is appropriate if it is designed for the event in question
(relevant) and its context
condition is believed true (applicable).
% %
In contrast with traditional planning, execution happens at each step. The
use of plans' context-preconditions to make choices as
late as possible, together with the built-in goal-failure mechanisms, ensures
that the system is responsive to changes in the environment.

%\notems{fixes in para}
In this paper we focus on the plan library to
investigate ways of learning appropriate or better plan selection
based on experience. 
% %
By grouping together plans responding to the same
event type, the plan library can be seen as a set of \emph{goal-plan tree}
templates: a goal (or event) node has children representing the alternative
relevant plans for achieving it; and a plan node, in turn, has children nodes
representing the subgoals (including primitive actions) of the plan.
% %%
These structures can be seen as AND/OR trees: for a plan to succeed all the
subgoals and actions of the plan must be successful (AND); for a subgoal to
succeed one of the plans to achieve it must succeed (OR).

\begin{figure}[t]
\begin{center}
\input{figs/gpt-example}
\end{center}
\caption{An Example BDI Goal-Plan Hierarchy.}
\label{fig:T3}
\end{figure}

Consider, for instance, the hierarachical structure shown in Figure~\ref{fig:T3}.
% %%
A link from a goal to a plan means that this plan is relevant (i.e., potentially
suitable) for achieving the goal (e.g., $P_1 \ldots P4$ are the relevant plans
for event goal $G$); whereas a link from a plan to a goal means that the plan
needs to achieve that goal as part of its (sequential) execution (e.g., plan
$P_A$ needs to achieve goal $G_{A1}$ first and then $G_{A2}$).
% %
For compactness, an edge with a label $\times n$ states that there are $n$ edges
of such type.
% %
Leaf plans directly interact with the environment and so, in a given world state,
they can either succeed or fail when executed; this is marked accordingly in the
figure \emph{for some particular world} (of course, in other states, such plans
may behave differently).
% %
In some world, given successful completion of $G_A$ first, the agent may achieve
goal $G_B$ by selecting and executing $P_B$, followed by selecting and executing two working leaf plans to resolve goals $G_{B1}$ and $G_{B2}$. If the agent
succeeds with goals $G_{B1}$ and $G_{B2}$, then it succeeds for plan $P_B$,
achieving thus goal $G_B$ and the top-level goal $G$ itself. There is no possible
successful execution though, if the agent decides to carry on any of the three
plans labelled $P_{B2}'$ for achieving the low-level goal $G_{B2}$.


As can be seen \textit{plan-selection} is critically important.
Standard BDI systems leverage domain
expertise by means of the context conditions of plans.
% However, crafting fully
%correct context conditions at design-time can, in some applications, be a
%demanding and error-prone task. In addition, fixed context conditions do not
%allow agents to adapt to changing environments.
% %%
%\notems{changed last sentence} 
In this work, we are interested in exploring how a situated agent may
\emph{learn} plan selection based on experience, in order to improve goal
achievement. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Learning for BDI Plan Selection} \label{subsec:bdi_learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In order to facilitate learning regarding which plan should be
executed for a given goal in a particular world state, we first replace the
boolean formulae that are the standard representation for context
conditions in BDI programming languages, with decision
trees~\cite{Mitchell97:ML} that provide a judgement as to whether the
plan is likely to succeed or fail for the given situation. 

To select a plan, based on information in the decision trees, we use a
probabilistic selection method whereby we choose a plan with
probability proportional to its believed chance of success in the
particular world state. This approach provides a balance between
exploitation (we choose plans with relatively higher success expectations
more often), and exploration (we choose plans randomly to get better
confidence in their believed success by trying them in more 
situations).

The resolution of goals in BDI execution results in 
the invocation of plans that in turn may post sub-goals that are further
handled by sub-plans in a hierarchical manner. In a programming context,
this is equivalent to making a function call that in turn calls sub-functions.
%The key difference is that the choice of sub-plans in the BDI context is
%not determined by the designer 
%% ABOVE IS NOT CORRECT
However a sub-goal may have a number of possible plans for achieving
it, some of which will work better in particular situations than
others. In our learning context, where we do not yet know which plans
work well in which situations, a plan may fail not because the plan
was a bad choice in the given situation, but instead because the
run-time choice of sub-plans was incorrect for the situation.

%As we have reported in earlier work~\cite{Airiau:IJAT:09} there are
%potential problems with learning the wrong information if a plan fails
%due to a wrong decision further down in the goal plan
%hierarchy. Consider the goal-plan tree in figure \ref{fig:bdiarch}: if
%as discussed previously we picked one of the  plans indicated by
%$P_{B2}'$ to address goal 
%$G_{B2}$ (in the world which this tree is annotated for), then we
%would fail. This failure of $G_{B2}$\footnote{We are not currently
%using BDI failure recovery.} would then cause $P_B$ to fail, failing
%$G_B$, and in turn $P_i$. As $P_{B2}'$ is a leaf node it is clearly
%reasonable to learn that this plan fails in the particular world
%state. However it is not so clear that one wants to learn that the
%parent plans $P_B$ and $P_i$ fail. In fact, as we can see in this
%annotated figure, had we made a different choice for achieving
%$G_{B2}$ they would have succeeded. If one does not take account of
%the hierarchical nature of the goal plan in some way, then it may be
%that learning is severely impacted or even made impossible.

%Our experimentation has shown that the degree to which the learning is
%negatively affected due to wrong decisions is very dependent on
%the particular structure of the goal-plan tree (i.e. the
%program). In particular, if there is a threshhold below which a plan
%is not considered worth trying (for example, if belief of success is
%only 10\%, then the cost of trying the plan may not be justified),
%then storing failure information when its reliability is unceratin,
%can lead the agent to be unable to ever learn the successful
%execution. 

Our first approach~\cite{Airiau:IJAT:09} to address this problem was that 
of careful consideration whereby failures are recorded for learning purposes 
only when we are sufficiently sure that the failure was not due to poor
sub-plan choices. 
%This requires recording which choices we have made at each
%point, for every world situation. 
We have shown that this conservative approach is more robust, 
though often slower, than a more agressive
approach which records all experiences, but can in some particular cases
completely fail to learn.

Our second approach reported in~\cite{Singh:AAMAS10} was to
adjust the plan selection probability based on some measure of 
our confidence in the decision tree classification.
%
We consider the reliability of a \dt\ for a given plan in a given 
world state to be proportional to the number of sub-plan choices (or paths
below the plan in the goal-plan hierarchy) that have been
\textit{covered} in that  
world state. The greater the coverage, the more we have explored and the
greater the confidence in the resulting \dt.
%
By biasing the plan selection probability with a coverage-based confidence
measure we achieved the same robustness as that of conservative recording of 
failure cases. The coverage approach, however, is more 
flexible as the extent to which this is used can be
readily adjusted by parameters in the selection formula. 

%A limitation with both the above
%approaches is that we have not considered parameterised goals. 
%A robot goal to move, for instance, 
%would typically have some parameters, either a destination (where the
%world state contains the current position), or a direction. Clearly a
%goal to move from A to B will require different choices than a goal to
%move from A to C (assuming B and C are not close). The learning
%process must take account of this. A second limitation is that we
%have not allowed for recursion, which again is heavily used in many
%real applications. 

A limitation with the previous approaches is that 
events were assumed to be propositional atoms, that is, event
types were not considered. By \emph{event type} we mean an event that may
contain ``data'' as part of its definition. For instance, event
$\textmath{travelTo(?dest)}$ may represent the goal to travel to location
$\textmath{?dest}$. Presumably a goal to move to location $A$ would 
require different choices than a goal to move to $B$, and the learning
must account for this.
% % 
Another limitation is the assumption that the agent's plan library is
non-recursive, so that the goal-plan tree structure induced is always
\emph{finite}.
% %
Clearly both limitations would preclude the applicability of the approach in
many practical domains where hierarchies are usually expressed
using parameterised goal events and plans, and often make
use of (direct or indirect) recursive procedures.


%Unfortunately, when we applied our previous
%coverage measures to unfolded recursive structures (with a fixed
%recursion depth), the method, which relied on exact calculation of
%numbers of paths, was no longer viable computationally, and even had
%problems conceptually. Consequently, in this and ongoing work we adopt
%an approximate measure of complexity of the goal-plan tree beneath a
%given plan and use this, combined with a graduated reliance on the
%decision tree depending on both experience and complexity of the tree
%- a more complex tree requiring more experience before we should feel
%confident of learned results.

Furthermore, the coverage approach of~\cite{Singh:AAMAS10} does not scale
when recursive structures and event-goal types are
introduced. Conceptually we can unfold the recursive structure to a
specified depth. However, the number of paths is exponential in the
recursion number and the problem is further compounded by event-goal
types and the number of possible world states.
%
An additional limitation with the
coverage-based confidence measure is that it does not consider domain
complexity. For instance, a leaf plan that has no sub-goals will
achieve full coverage when it is tried once, after which selection
will be fully biased towards the plans \dt\ classification. However,
the \dt\ that at this point has only witnessed one world will
generalise the outcome to all as-yet-unseen worlds leading to
misclassification errors. 

In the following section we present the details of our learning
approach, incorporating both parametrised goals, and recursion, as
well as a new simpler confidence measure that is based on the 
general idea of coverage but does not suffer from its limitations.







