%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}\label{sec:preliminaries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\success}{\mbox{\emph{succ}}}
\newcommand{\failure}{\mbox{\emph{fail}}}

\newcommand{\procedurefont}[1]{\mathsf{#1}}
\newcommand{\StableGoal}{\procedurefont{StableGoal}}
\newcommand{\RecordTrace}{\procedurefont{RecordFailedTrace}}
\newcommand{\RecordWorldDT}{\procedurefont{RecordWorldDT}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{BDI Agent Systems}\label{sec:bdi_programming}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\notems{new para}
BDI agent-oriented programming is a popular, well-studied, and practical paradigm
for building intelligent agents situated in complex and dynamic environments with
(soft) real-time reasoning and control requirements
\cite{Georgeff89-PRS,Benfield:AAMAS06}.
% %
Generally speaking, BDI agent-oriented programming languages are built around an
explicit representation of propositional attitudes (e.g., beliefs, desires,
intentions, etc.). A BDI architecture addresses how these components are
represented, updated, and processed to determine the agent's actions.
% .
There are a number of agent programming languages and development platforms in
the BDI tradition, such as \PRS~\cite{IngrandGR:IEEE92-PRS} and
\dMARS~\cite{Inverno:JAAMAS04-dMARS}, \AgentSpeak\ and
\JASON~\cite{Rao:LNCS96-AgentSpeak,Bordini:07-JASONBOOK},
\JADEX~\cite{PokahrBL:EXP03-JADEX},
\TAPL~\cite{Hindriks:JAAMAS99-3APL,DastaniRM:05} and
\DAPL~\cite{Dastani:JAAMAS08-2APL}, \GOAL~\cite{BoerHHM:JAPL07-GOAL},
\JACK~\cite{BusettaRHL:AL99-JACK}, SRI's \SPARK~\cite{MorelyM:AAMAS04-SPARK}, and
\JAM~\cite{Huber:AGENTS99-JAM}.



\begin{figure}[t]
\begin{center}
\resizebox{.9\textwidth}{!}{\input{figs/figbdiarch}}
\end{center}
\caption{A typical BDI-style architecture}
\label{fig:bdiarch}
\end{figure}

In a BDI-style system, an agent consists, basically, of a belief base (akin to a
database), a set of recorded pending goal events, a plan library, and an
intention base.
% %
Figure~\ref{fig:bdiarch} depicts a typical BDI architecture.
% %
While the belief base encodes the agent's knowledge about the world, the pending
events include the \emph{goals} the agent wants to achieve/resolve.
% %
The \textit{plan library}, in turn, contains \emph{plan rules}, or simply
\emph{plans}, of the general form $e: \psi \leftarrow \delta$ encoding the
standard domain operational procedure $\delta$ (that is, a program) for achieving
the event-goal $e$ when the so-called \textit{context condition} $\psi$ is
believed true---program $\delta$ is a reasonable strategy to resolve event $e$
whenever $\psi$ holds.
% %
%\notems{UAV example}
For example, a BDI controller for an unmanned aerial vehicle (UAV)
\cite{Karim:AAMAS05,Ziming:AAC07} may include a certain procedure $\delta$ for
achieving the goal of \emph{landing the aircraft} (the event) under \emph{normal
weather conditions} (the context).
% %
Among other operations, the plan-body program $\delta$ will typically include the
execution of actions ($act$) in the environment and subgoal events ($!e$) that
ought to be in turn resolved by selecting suitable plans for that subgoal event.
Lastly, the intention base accounts for the current, partially instantiated,
plans that the agent has already committed to in order to handle or achieve some
event-goal.


The basic \emph{reactive goal-oriented behavior} of BDI systems involves the
system responding to events, the inputs to the system, by committing to handle
one pending event-goal, selecting a plan from the library, and placing its
program body  into the intention base.
% %%
A plan may be selected if it is \textit{relevant} and \textit{applicable}, that
is, if it is a plan designed for the event in question and its context condition
is believed true, respectively.
% %
In contrast with traditional planning, execution happens at each step. The
assumption is that the use of plans' context-preconditions to make choices as
late as possible, together with the built-in goal-failure mechanisms, ensures
that a successful execution will eventually be obtained while the system is
sufficiently responsive to changes in the environment.

%\notems{fixes in para}
In this paper we focus on the plan library to
investigate ways of learning how agents can make a better use of it over time.
% %
It is not hard to see that, by grouping together plans responding to the same
event type, the plan library can be seen as a set of \emph{goal-plan tree}
templates: a goal (or event) node has children representing the alternative
relevant plans for achieving it; and a plan node, in turn, has children nodes
representing the subgoals (including primitive actions) of the plan.
% %%
These structures can be seen as AND/OR trees: for a plan to succeed all the
subgoals and actions of the plan must be successful (AND); for a subgoal to
succeed one of the plans to achieve it must succeed (OR).




\begin{figure}[t]
\begin{center}
\input{figs/GPtree-T3}
\end{center}
\caption{Goal-plan hierarchy $\T_3$. There are $2^4$ worlds whose solutions are
distributed evenly in each of the $4$ top level plans. Successful execution
traces are of length $4$. Within each sub-tree $P_i$, \BUL\ is expected to
perform better for a given world, but it suffers in the number of worlds. Overall, \CL\ and \BUL\
perform equally well in this structure.}
\label{fig:T3}
\end{figure}


Consider, for instance, the hierarachical structure sown in Figure~\ref{fig:T3}.
% %%
A link from a goal to a plan means that this plan is relevant (i.e., potentially
suitable) for achieving the goal (e.g., $P_1 \ldots P4$ are the relevant plans
for event goal $G$); whereas a link from a plan to a goal means that the plan
needs to achieve that goal as part of its (sequential) execution (e.g., plan
$P_A$ needs to achieve goal $G_{A1}$ first and then $G_{A2}$).
% %
For compactness, an edge with a label $\times n$ states that there are $n$ edges
of such type.
% %
Leaf plans directly interact with the environment and so, in a given world state,
they can either succeed or fail when executed; this is marked accordingly in the
figure \emph{for some particular world} (of course, in other states, such plans
may behave differently).
% %
In some world, given successful completion of $G_A$ first, the agent may achieve
goal $G_B$ by selecting and executing $P_B$, followed by selecting and executing
$2$ leaf working plans to resolve goals $G_{B1}$ and $G_{B2}$. If the agent
succeeds with goals $G_{B1}$ and $G_{B2}$, then it succeeds for plan $P_B$,
achieving thus goal $G_B$ and the top-level goal $G$ itself. There is no possible
successful execution, though, if the agent decides to carry on any of the three
plans labelled $P_{B2}'$ for achieving the low-level goal $G_{B2}$.





As one can easily observe, the problem of \textit{plan-selection} is at the core
of the whole BDI approach:
\emph{which plan should the agent commit to in order to achieve a certain goal?}
% %
This problem amounts, at least partly, to what has been referred to as
\emph{means-end analysis} in the agent foundational literature
\cite{Pollack92-IRMA,Bratman88}, that is, the decision of \textit{how} goals are
achieved.
% %%
To tackle the plan-selection task, state-of-the-art BDI systems leverage domain
expertise by means of the context conditions of plans. However, crafting fully
correct context conditions at design-time can, in some applications, be a
demanding and error-prone task. In addition, fixed context conditions do not
allow agents to adapt to changing environments.
% %%
%\notems{changed last sentence} 
Thus, in this work, we shall be interested in
exploring how an agent, while acting, can \emph{learn} how to better
select among the
plans available in order to improve goal achievement over time.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Learning for BDI Plan Selection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In order to facilitate learning regarding which plan should be
executed for a given goal, in a particular world state, we replace the
boolean formulae that are the standard representation for context
conditions in BDI programming languages, with decision
trees~\cite{Mitchell97:ML} that provide a judgement as to whether the
plan is likely to succeed or fail, given a particular situation. 

To select a plan, based on information in the decision trees, we use a
probabilistic selection method whereby we choose a plan with
probability proportional to its believed chance of success in the
particular world state. This approach provides a balance between
exploitation (we more often choose a plan with a higher belief of
success), and exploration (we sometimes choose a plan which currently
does not have a strong belief of success: perhaps the failure was
unusual, or maybe we made a later wrong choice and there is another
path from this plan which will succeed).

As we have reported in earlier work~\cite{Airiau:IJAT:09} there are
potential problems with learning the wrong information if a plan fails
due to a wrong decision further down in the goal plan
hierarchy. Consider the goal-plan tree in figure \ref{fig:bdiarch}: if
as discussed previously we picked one of the  plans indicated by
$P_{B2}'$ to address goal 
$G_{B2}$ (in the world which this tree is annotated for), then we
would fail. This failure of $G_{B2}$\footnote{We are not currently
using BDI failure recovery.} would then cause $P_B$ to fail, failing
$G_B$, and in turn $P_i$. As $P_{B2}'$ is a leaf node it is clearly
reasonable to learn that this plan fails in the particular world
state. However it is not so clear that one wants to learn that the
parent plans $P_B$ and $P_i$ fail. In fact, as we can see in this
annotated figure, had we made a different choice for achieving
$G_{B2}$ they would have succeeded. If one does not take account of
the hierarchical nature of the goal plan in some way, then it may be
that learning is severely impacted or even made impossible.

Our experimentation has shown that the degree to which the learning is
negatively affected due to wrong decisions is very dependent on
the particular structure of the goal-plan tree (i.e. the
program). In particular, if there is a threshhold below which a plan
is not considered worth trying (for example, if belief of success is
only 10\%, then the cost of trying the plan may not be justified),
then storing failure information when its reliability is unceratin,
can lead the agent to be unable to ever learn the successful
execution. 

There are two different approaches we can take to try to address this
problem. The first approach, which we explored
in~\cite{Airiau:IJAT:09} is to be careful about recording of failure
examples for the decision tree, recording them only when we are
sufficiently sure that the failure was not due to a later wrong
choice. This requires recording which choices we have made at each
point, for every world situation. We have shown that this conservative
approach is more robust, though often slower, than a more agressive
approach which records all experience, but can in some cases be
severely impacted.

The second approach that we have tried, reported in~\cite{Singh:AAMAS10} is to
consider how likely the decision tree is to be reliable, at the point
we make our plan selection.  We consider the reliability of a decision
tree for a particular plan, in a particular world state, to be
relative to how many executions of the plan (or paths in the goal plan
tree) have been explored in that world state. We refer to this measure
as coverage. The greater the coverage, the more we can rely on what we
have observed.  By biasing the selection probability number towards
the default value of 0.5 in cases where the coverage, and therefore
reliability is low, we achieved the same robustness as that
achieved by conservative recording of failure cases. The coverage
approach is more flexible as the extent to which this is used can be
readily adjusted by parameters in the selection formula. 

A limitation of the work that we have done with both the above
approaches is that we have not considered parameterised goals. These
are clearly necessary for any real application.  A robot goal to move
would typically have some parameters, either a destination (where the
world state contains the current position), or a direction. Clearly a
goal to move from A to B will require different choices than a goal to
move from A to C (assuming B and C are not close). The learning
process must take account of this.  A second limitation is that we
have not allowed for recursion, which again is heavily used in many
real applications. Unfortunately, when we applied our previous
coverage measures to unfolded recursive structures (with a fixed
recursion depth), the method, which relied on exact calculation of
numbers of paths, was no longer viable computationally, and even had
problems conceptually. Consequently, in this and ongoing work we adopt
an approximate measure of complexity of the goal-plan tree beneath a
given plan and use this, combined with a graduated reliance on the
decision tree depending on both experience and complexity of the tree
- a more complex tree requiring more experience before we should feel
confident of learned results.
%
In the following section we present the details of our learning
approach, incorporating both parametrised goals, and recursion, as
well as our new approach to estimating the confidence we should have in
the decision tree for a particular plan, given a certain number of
experiences. 









