%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The BDI Learning Framework}\label{sec:framework}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our learning task may be summarised as follows: \textit{Given past execution data and the current world state, determine which plan to execute next in order to best address the event-goal in question}. In the BDI sense, our task is to learn the context condition of each plan in the goal-plan hierarchy. In this section we describe our BDI Learning Framework that enables such learning. In particular we describe the use of \dt s for learning context conditions, together with the confidence-based probabilistic plan selection that uses the learning output, with a focus on learning with event-goal types and recursive event-goals.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Integrating Decision Trees into Context Conditions for Plans}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In a BDI system, a plans context condition is a logical formula that is constructed at design time and evaluated against an event-goal at run time to determine if the plan is applicable in the given world state\footnote{Context formulas may reference internal beliefs as well as environment states, and for this study we treat both as inclusive in the world state.}. As reported in \cite{Airiau:IJAT:09}, in order to allow the context condition to be learnt over time, we annotate each plans context formula with a \textit{\dt}\footnote{It is perfectly feasible to combine the existing logical formula with the \dt\ classification, but to aid our understanding of the \dt\ learning in this study we always use an empty initial formula.}. The idea is that the agent starts with some \textit{necessary but possibly insufficient} conditions for each plan (provided by the designer), and over time and in the course of trying various plans in various world states will be able to \textit{refine} each plans context condition using the learnt \dt\ classification of the world states encountered.

The choice of \dt s as the learning module is motivated by several factors. Firstly, \dt s support hypotheses that are a disjunction of conjunctive terms, and since context formulas are generally expressed in this form then \dt s are readily applicable. Secondly, \dt s can be converted to \textit{if-then} rules that are human readable and can therefore be verified by a domain expert. Finally, \dt s are robust against training data that may contain errors. This is specially relevant in a stochastic domain where perfectly applicable plans may nevertheless fail due to unforeseen circumstances.

The input for the \dt\ learning is a training set of data points of the form $[w,o]$, where $w$ is the world state in which the plan was executed and $o$ was the boolean outcome (success or failure). Initially the training set is empty and grows over time as the agent tries the plan in various world states and samples the result. The world state $w$ itself is a set of discrete attributes that together represent the state of affairs. The idea is that over time the \dt\ will learn a classification based purely on the subset of attributes in $w$ that are relevant to the context condition of the plan. 

The attributes in $w$ determine the quality of the final classification, and their number and possible values has a bearing on the size of the training set required to correctly learn the context condition. The choice of attributes to include in the world state $w$ is a design decision and dependent on domain knowledge. Importantly, the attributes in $w$ should be a superset of the necessary and sufficient attributes relevant to the context condition. For instance, for a plan to pick up an object using a robotic arm, \textit{objectSurface} is a relevant attribute, \textit{gripperWet} possibly is, but \textit{dayOfWeek} likely is not. For the purpose of our study we assume that the designer provides a set of all attributes that are considered possibly relevant to the context condition of the plan. In the worst case, this set is the full set of attributes of the world. 

The decision tree inductive bias is a preference for smaller trees. In other words, the induction of \dt s will trade-off some accuracy in classification for compactness of representation. In fact such inductive bias is necessary if the \dt\ is to generalise to as yet unseen world states. Once a \dt\ is induced from the training set, it may be used to classify any new world state $w$. In the strict sense the classification is an outcome $o$ (failure or success). However, several \dt\ implementations including \propername{J48} in \weka\footnote{In our study we use algorithm \propername{J48}, a version of \propername{c4.5} \cite{Mitchell97:ML}, from the \weka\ learning package \cite{weka99}.} annotate a likelihood of class membership (that is indicative of the inductive bias) to the returned classification. For the given world state $w$ then, we treat the returned likelihood of membership to the $success$ class as the expected likelihood of success of the plan.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Support for Recursive Event-Goals}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Recursion in the context of event-goals refers to the case where the resolution of an event-goal instance $G[\varphi_1]$ involves first the resolution of another goal-event instance $G[\varphi_2]$ of the same type. The result is a growing stack of pending $G[\varphi_i]$ event-goals that eventually terminate in $G[\varphi_n]$ whose parameters satisfy the termination conditions i.e. where a non-recursive plan choice is made.

In order to better understand the impact of recursion on context learning, we use the notion of an \textit{execution trace} that is a sequence of the form $G_0\langle\varphi_0\rangle[P_0:w_0] \cdot G_1\langle\varphi_1\rangle[P_1:w_1] \cdot \ldots \cdot G_n\langle\varphi_1\rangle[P_n:w_n]$, that represents a sequence of event-goals along with the plans selected to handle them and the world state in which the selections were made. So $G_i\langle\varphi_i\rangle[P_i:w_i]$ captures the case where plan $P_i$ was selected in world state $w_i$ in order to achieve the goal-event $G_i\langle\varphi_i\rangle$.

\begin{figure}[t]
\begin{center}
\resizebox{0.8\textwidth}{!}{
\input{figs/gpt-unfolding}
}
\end{center}
\caption{Goal-plan hierarchy containing a goal type $G\langle\rangle$ handled by three plans $P_1$, $P_2$ and $P_3$. Here plan $P_2$ posts two instances of $G\langle\rangle$ resulting in recursion. Two levels of recursive unfolding are shown. Dashed $P_2$ nodes indicate unexplored recursive sub-trees.}
\label{fig:unfolding}
\end{figure}

Consider the example BDI goal-plan hierarchy of Figure \ref{fig:unfolding}. The structure has just a single event-goal type $G\langle\rangle$ and three options to handle it, one of which ($P_2$) in turn posts two instances of the same event-goal type $G\langle\rangle$. In this way, the only plans that take an action in the environment are $P_1$ and $P_3$. The figure highlights an execution trace as follows: \[
\lambda=G\langle\varphi_1\rangle[P_2:w_1] \cdot G\langle\varphi_2\rangle[P_1:w_1] \cdot G\langle\varphi_3\rangle[P_2:w_2] \cdot G\langle\varphi_6\rangle[P_1:w_2] \cdot G\langle\varphi_7\rangle[P_3:w_3].
\]

The first choice in the execution results in the selection of plan $P_2$ to handle event-goal instance $G\langle\varphi_1\rangle$ in a given world $w_1$. Plan $P_2$ in turn immediately posts the event-goal instance $G\langle\varphi_2\rangle$ that is successfully handled by the non-recursive node $P_1$. Plan $P_2$ then posts the second event-goal instance $G\langle\varphi_3\rangle$, which then is handled by itself in a recursive manner.  The outcome is that $\lambda$ traces a path that involves the successive execution of leaf plan $P_1$ for event-goal $G\langle\varphi_2\rangle$ followed by another execution of $P_1$ this time for event-goal $G\langle\varphi_6\rangle$, and finally terminates in the failure of leaf plan $P_3$ for event-goal $G\langle\varphi_7\rangle$. 

Note that if plan $P_2$ had instead been selected to handle $G\langle\varphi_7\rangle$ then a deeper recursive call would have ensued. Similarly if earlier in the execution trace plan $P_2$ was selected to handle event-goal $G\langle\varphi_2\rangle$ then a different recursive sub-tree (shown in Figure \ref{fig:unfolding} as dotted nodes under $G\langle\varphi_2\rangle$) would have unfolded.

The immediate implication of a recursive goal-plan structure is that the size of the hierarchy is no longer static but instead unfolds in a dynamic manner. The issue stems from the fact that the recursion is \textit{unbounded} because the context conditions that cause the recursion to terminate are initially unknown. So in order to know the context conditions we must recursively explore, but in doing so we risk an infinite recursive call because the context conditions that ought to guide the recursive exploration towards termination are unknown. This means that we may never find the ``bottom'' or leaf nodes. This has implications for any \textit{bottom-up} strategies. For instance, our conservative recording approach of \cite{Airiau:IJAT:09} and the coverage-based confidence measure of \cite{Singh:AAMAS10} both suffer from this problem. Incidentally, the simpler aggressive recording approach is not impacted by recursion as it does not consider the goal-plan structure.

One way to resolve this issue is to treat all recursive goals simply as sub-trees in a static structure and limit the recursive \textit{unfolding}  to a maximum allowed depth. In this study we use this bounded recursion approach for handling recursive structures. It follows then that wherever a recursive structure applies, a maximum recursion value must always be supplied. This may not be an unrealistic requirement given that the domain expert will usually have some idea of how much recursion is sufficient for an event-goal type.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Calculating Confidence in the Decision Tree Classification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The typical use of \dt s lies in the \textit{offline} induction from a complete training set. In that sense, the use of \dt s in our framework is unorthodox since the training set is built incrementally by recording samples after each new execution. This results in the training set being incomplete\footnote{Training data is incomplete in the sense that the agent has only collected a portion of the full data set required to learn the correct classification.} in the early stages of learning leading to misclassification errors. A confidence measure in the \dt\ classification is therefore desirable to address this issue. Previously in \cite{Singh:AAMAS10} we showed how the \textit{coverage} of possible execution paths below the plan in the goal-plan hierarchy may be used to build such a measure. Here we propose a new confidence measure that still takes from the notion of coverage but that does not suffer from its limitations (Section \ref{subsec:bdi_learning}).

Our requirement for the confidence measure is that it be a monotonic function whose values transition from no confidence ($0.0$) to full confidence ($1.0$) based on experience. Specifically, the experiences we are interested in should constitute coverage of the plan complexity (number of sub-plan choices) and the domain complexity (number of world states in which the plan applies). Since an exact calculation of such coverage does not scale for all practical purposes then we are interested in an \textit{approximate} coverage that is still representative of the state of affairs but is simpler to compute.

One way to achieve this is to use a monotonic decay function\footnote{This technique is frequently applied in machine learning algorithms for balancing between exploration of choices and exploitation of learning.} (for instance $\epsilon_i = \epsilon_{i-i} * \delta$ where $\delta < 1.0$) but where the decay factor $\delta$ is tied to the complexity involved. This way, a plan that has a larger number of sub-plan choices will utilise a slower decay factor $\delta$ taking longer to reach full confidence $(1-\epsilon)$ than another pan that has less choices to make. For goal-plan complexity this decay may be calculated offline by analysing the goal-plan hierarchy. A similar treatment is possible for domain complexity although the subtlety there is that the decay factor cannot be pre-determined since the number of world states is not known upfront and is dependent on the domain. In this case it is reasonable to treat this as a parameter defined by the domain expert.

\begin{equation*}\label{eqn:confidence}   
c_P = (1.0 - \epsilon_{Pt}) * (1.0 - \epsilon_{Pd}).
\end{equation*}

Equation \ref{eqn:confidence} shows how the final confidence $c_P$ is calculated for a given plan $P$. Here $\epsilon_{Pt}$ is the plans tree complexity decay while $\epsilon_{Pd}$ is the plans domain complexity decay. The actual updates to the decay values are performed each time the plan $P$ is executed. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Handling Event-Goal Types}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our BDI learning framework account as presented earlier in \cite{Airiau:IJAT:09} and \cite{Singh:AAMAS10} did not account for event-goal types. In practical BDI systems, it is often the case that a single plan will handle all instances of an \textit{event-goal type}. Furthermore event-goal instance parameters will generally be included in the context logical formula. 

Recall our previous definition of the learning task: Given past execution data and the current world state, determine which plan to execute next in order to best address the event-goal in question. The simplifying assumption in this definition is that the event-goal in question is an \textit{event-goal instance}. 

Consider once more the goal-plan structure in Figure \ref{fig:unfolding} and the highlighted solution path terminating in the leaf plans indicated by the $\surd$ symbol. The key point here is that the indicated solution applies to the event-goal \textit{instance} $G\langle\varphi_1\rangle$ and to that instance alone. For a different instance $G\langle\varrho_1\rangle$ the solution path would likely be different (one way to visualise this in the Figure \ref{fig:unfolding} is to think of it as an animation where the event-goal parameters and the placement of the $\surd$ symbols changes on each frame).  This means that event-goal instance parameters must also be considered as input for a plans \dt\, if we are to have any hope of learning the solutions \textit{per event-goal instance}.

We include such an account by augmenting the training samples for the \dt\ with the event-goal parameters. As such, the training set now contains samples of the form $[w \cup \varphi,o]$ where the world state $w$ is the initial set of all relevant attributes that represent the state of affairs, $\varphi$ is the set of all event-type parameters, and $o$ is the outcome class (success or failure). Incorporating the event-goal parameter set $\varphi$ in the training data is sufficient for learning with event-goal types, and no fundamental change to the framework is warrented.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Calculating Plan Selection Weights based on Confidence}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Typical BDI platforms offer several mechanisms for plan selection from a set of applicable plans, such as plan precedence and meta-level reasoning. However, since these mechanisms are pre-programmed and do not take into account the experience of the agent, we provide a new \textit{probabilistic plan selection} function for this purpose. 

For each plan, given its expectation of success (as determined by its \dt\ learning) and a confidence measure in this expectation (based on coverage), we calculate a final \textit{selection weight} that is indicative of the likelihood of the plan being selected for execution.

\begin{equation*}\label{eqn:weight}   
\Omega_P(w) = 0.5 + \left[  c_P *  \left( \kappa_P(w) - 0.5 \right)  \right].
\end{equation*}

Equation \ref{eqn:weight} shows how the final plan selection weight $\Omega_P(w)$ is calculated for a given world state $w$. Initially, the confidence $c_P$ is zero and the selection weight takes the default value of $0.5$. Over time, as the confidence improves towards the final value of $1.0$, the selection weight approaches the value $\kappa_P(w)$ estimated by the plan's \dt.

Given the set of applicable plans for resolving event-goal $G$ in world state $w$ then, our probabilistic plan selection mechanism chooses a plan $P_i$ with a probability directly proportional to its selection weight $\Omega_{P_i}(w)$. Such selection ensures a balance between the \emph{exploitation} of current know-how and the \textit{exploration} of new choices that is necessary for online learning tasks. 


%Simplifications: Only single level goal recursion allowed ie G1->P1->G1 and not  G1->P1->G2->P2->G. Also no information of Gs in the system so only the parent goal recursion is tracked.

