##### General comments:

We thank the three reviewers for their invaluable feedback.

Reviewers #1 and #2 have both suggested a comparison with Singh et al.'s one-off learning. Indeed we have evaluated this with some rigour. Overall, our approach performs similarly to theirs in a monotonic setting, and may be used as a replacement for their technique as suggested in the paper. Comparison with Singh-like learning in an environment with changing dynamics, however, is not possible as their technique was not designed for this and will require "hand-tuning" of the learning rate for the particular hierarchy, as well as a notification of the change in the environment. Neither are required in our approach. Regardless, we agree with the reviewers that this comparison should be made clear in the paper.


##### Review #1:

In general, "n" dictates the moving average and therefore the sensitivity of the confidence measure to changes in performance: smaller values of "n" will make it more responsive to change, whereas larger values will lead to more stability. The value n=5 was used in all experiments (Section 4.1). Currently this is fixed upfront and must be decided by the programmer after some initial trials (in our tests values between 3-8 were reasonable). We agree that it would be valuable to remove this dependence, say using a reduced weighting scheme as suggested, in the future. Meanwhile, we will aim to make this intuition clear in the paper.

We note the reviewer #1's remaining comments around the abstract and text clarifications, and will address these appropriately as suggested.

##### Review #2:

Significance/Evaluation/Comparison:

1. The reviewer raises a valid point regarding exploration. Indeed we have not discussed the exploration strategy explicitly. Our exploration strategy is inherent in the confidence-based probabilistic selection weight Omega (Section 3.3). Intuitively, when confidence is low, the probability of choosing the believed high success option is still low, leading to a strong chance of choosing something else i.e. exploring. We agree that it will be very useful to describe this clearly. Thank you for pointing this out.

2. We note the reviewer's comment regarding learning times. We agree that there is work to be done before our results can be presented within a theoretical framework to a machine learning audience. As the reviewer has duly noted, our focus has been on the principled integration of learning with BDI agent programming. Our goal is to see deployed agents that keep-on-learning as it were. Our hope is to also encourage such consideration within the machine learning community.

Technical:

1. The stability degree is recorded for the plan, both for failures (calculated) and successes (assumed 1), as and when they occur temporally. The average degree of stability is calculated over this sequence using the last n values. 

2. Yes the reviewer has understood this correctly. Degree 1 is recorded against all plans (including non-leaf) in the trace as we take an optimistic view of successes. We will clarify this in the text.

3. We see that the wording of this is too strong. Extreme alpha values will indeed make a difference. Overall, where the world is relatively complex but the structure of choices relatively simple you would want a stronger weighting on the world-based value, and vice versa. We will clarify this point in the paper.

4. Thanks, we see how the RecordDegreeStability may be confusing. We will reword this as suggested.  

5. Sorry for this mistake and thanks for picking it. The description for NewStates should say "not". 

Clarity:

We note the reviewer's comments on clarity and will aim to improve this in the areas highlighted.

##### Review #3:

We thank the reviewer for their generous and favourable comments.
