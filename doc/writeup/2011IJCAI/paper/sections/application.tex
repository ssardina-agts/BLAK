%!TEX root = ../ijcai11storage.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{An Embedded Battery System Controller}\label{sec:application}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%Energy storage enables increasing levels of renewable energy in our electricity system, and the rapidly maturing supply chains for several battery technologies encourages electricity utilities, generators, and customers to consider using large battery systems. 
Large battery systems enable increasing levels of renewable energy in our electricity system. 
%
Such installations usually comprise of multiple modules that can be controlled independently~\cite{norris02:grid}. 
%Often it is necessary to operate the modules in different states, such as if it is undesirable to change the direction of power flow too frequently, or due to battery technology requirements such as zinc-bromine flow batteries needing complete discharges at regular intervals. 
%Often it is necessary to operate the modules in different states, for instance due to battery technology requirements such as zinc-bromine flow batteries needing complete discharges at regular intervals. 
Since battery performance is susceptible to changes over time (e.g. modules may lose actual capacity) an \emph{adaptable} control mechanism is desirable that accounts for such changes.
%
Here, we describe an {\em embedded BDI controller for a modular battery system}, that regulates overall energy consumption of a smart office building comprising of loads (e.g., appliances) and generators (e.g. solar panels), by suitably ordering the battery to charge (i.e., act as a load) or discharge (i.e., act as a generator) at determined rates (the overall rate being the sum over the modules' rates).

Figure~\ref{fig:gptree} shows our implemented controller for an overall system capacity of $c$ (module capacity) $\times$ $k$ (number of modules). Top-level goal-event $G(r,k,s)$ requests a battery charge/discharge (normalized) rate of $r \in [-1.0:+1.0]$, where $-1.0$ ($1.0$) indicates maximum discharge (charge) rate, $s$ is the current battery state, and $k$ is (initially) the number of modules in the system. 
%%
The controller works by recursively configuring each module using the plans $\pSetCharge$ (charging at rate $+c$), $\pSetDischarge$ (discharging at rate $-c$), and $\pSetNotUsed$ (disconnected i.e. rate $0$), and finally, after all modules have been configured, physically operating the battery for one period using the $\pExecute$ plan. 
%%

Observe that the first three plans contain known domain constraints for applicability using condition $\cSatisfies_X(\cdot,\cdot,\cdot)$. For instance, plan $\pSetCharge$ may not apply if the module is already charged; $\pSetDischarge$ may be ruled out if discharging the module means that no matter how the remaining modules are configured, the response will fall short of the request. When none of the plans apply, then BDI failure recovery is employed to backtrack up and select a different configuration path until all constraints are satisfied or all options exhausted. 

Plan $\pExecute$ is therefore run only with functionally correct configurations. It first operates the whole battery for the period ($\aOperate$) and then evaluates the result via a sensor ($\aEvaluate$). If the evaluation \emph{succeeds}, then the desired rate $r$ has been met and the whole execution is deemed successful. Otherwise, the evaluation step \emph{fails} and so does the whole execution of the program, since no BDI failure recovery can be used after the battery system has been physically operated. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Results}\label{sec:results}

All experiments used a battery system with five modules (i.e., $k=5$). 
%
Each module had a charge state in the range $0$ (fully discharged) to $3$ (fully charged), as well as an assigned configuration for the period from $\{+c, 0, -c\}$, where $c=1/k$.
%
%For each module, the current charge state is described by a discrete value in the range $[0:3]$, where zero indicates a fully discharged state and three indicates a fully charged state. In addition, each module has an assigned configuration for the current period from the set $\{+c, 0, -c\}$, where $c=1/k$. 
%
Charging adds $+c$ while discharging adds $-c$ to a module's charge state, otherwise the state is unchanged for the period (i.e., there is no charge loss). Thus, the net battery response is in the (normalized) range $[-1.0:+1.0]$ in discrete intervals of $\pm c$. The state space for the problem is given by the modules ($5$), the requests ($11$), the charge states ($4^5$), and the assigned configurations ($3^5$), i.e. $5 \times 11 \times 4^5 \times 3^5 \approx 13.7$ million. The agent does not learn over the full space, however, since the filtering of nonsensical configurations by the plans' context conditions $\cSatisfies_X(\cdot,\cdot,\cdot)$ reduces it substantially (to $\approx1.5$ million).
%
Each episode corresponds to one $G(r,5,s)$ goal-event request: achieve overall battery response of $r$ given current module charge states $s$.  For simplicity of analysis, we generate only satisfiable requests
% i.e. a solution always exists. The outcome of each episode is either no response (no configuration was executed), or execution of the $\pExecute$ plan for operating (and evaluating) the battery. 
The threshold for stability calculation is set to $0.5$. We use an averaging window of $n=5$ for both the stability-based metric $\C_s$ and domain metric $\C_d$, and a (balanced) weighting of $\alpha=0.5$ for the final confidence measure $\C$. Each experiment is run five times and the reported results are averages from these runs. 
%
%Finally, in normal BDI operation, only plans that are deemed applicable as per their context condition are considered for execution. For our learning framework, where applicability is additionally defined by a plan's decision tree, this means that only plans with a reasonable likelihood of success should be allowed. To represent this, we used an \emph{applicability threshold} for plan selection of $40\%$, meaning that plans with a likelihood of success below this value are removed from consideration.\footnote{While this feature {\em does not alter the overall learning performance} of the battery controller, it does preclude the battery from being operated (i.e., plan $\pExecute$ being called) under module configurations that are likely to be unsuccessful. In fact, we found that the threshold used reduces the number of battery operations by $12\%$, which is substantial when considering battery life.}
Finally, since in normal BDI operation only plans deemed applicable as per their context condition are considered for execution, then for our learning framework we used an \emph{applicability threshold} of $40\%$, meaning that plans with a selection weight below this value are not considered.\footnote{While this feature does not alter overall performance, it does preclude the battery from being operated under module configurations that are likely to be unsuccessful. In fact, we found that the threshold reduces the number of battery operations by $12\%$, which is substantial when considering battery life.}

The first experiment (Figure~\ref{fig:experiment1}) represents the traditional (one-off) learning setting. A deterioration in module capacities (at $\approx 5\kilo$ episodes) causes a rapid drop in performance (to $\approx 76\%$) corresponding to the set of programmed/learnt solutions that no longer work. Performance is subsequently rectified by learning to avoid configurations that no longer work. 
%The first experiment (Figure~\ref{fig:experiment1}) represents the traditional (one-off) learning setting where the agent recovers functionality against deterioration in module capacities (at around $5\kilo$ episodes). 
%
%%Typically this would occur over years of use, however to show the response to substantial change, we force this deterioration to occur instantaneously (at around $5\kilo$ episodes). 
%
%The resulting rapid drop in performance (to $\approx 76\%$ success) corresponding to the set of programmed/learnt solutions that no longer work, is subsequently rectified by the controller by learning to avoid configurations that no longer work. 
%
The next two experiments demonstrate adaptive behaviour when the environment dynamics is continually changing.
%
In Figure~\ref{fig:experiment2}, the agent is exposed to partial failures from modules malfunctioning (the first module fails during $[0:20\kilo]$ and is then restored, the second during $[20\kilo:40\kilo]$, and so on), but the battery remains capable of responding to requests using alternative configurations. The agent successfully learns to operate the battery without the failing module by always configuring it to not-in-use.
%~\footnote{The apparent difference between performance drops at $0\kilo$ and $20\kilo$ is not meaningful in any way; it just happens that more ``bad'' cases occurred in the first failure.} 
%
Finally, in Figure~\ref{fig:experiment3}) we evaluate response in the extreme case of complete failure (during $[0:5\kilo]$) after which the system is restored.~\footnote{Around $2\kilo$ episodes, the selection weight of {\em all} plans drops below the applicability threshold. To ensure that learning can still progress, we use as a ``soft'' applicability threshold mechanism: the $40\%$ threshold applies $90\%$ of the time.}
%
The key point is that the learner does not require explicit notification of changes: it re-learns by adjusting exploration based on its performance (that invariably reflects any harmful changes).
