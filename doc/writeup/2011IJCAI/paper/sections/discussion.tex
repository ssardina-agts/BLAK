%!TEX root = ../ijcai11storage.tex
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}\label{sec:discussion}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We proposed a plan-selection learning mechanism for BDI agent-oriented programming languages and architectures that is able to \emph{adapt} when the dynamics of the environment in which the agent is situated changes over time.
%%
Specifically, the proposed \emph{dynamic confidence measure} provides a simple way for the agent to judge how much it should trust its current understanding (of how well available plans can solve goal-events). 
%% 
% Specifically, we proposed a \emph{dynamic confidence measure} that combines ideas of plan stability~\cite{airiau09:enhancing} and plan coverage-based confidence~\cite{singh10:extending,singh10:learning} from previous approaches, with a sense of the rate at which new worlds are being witnessed. This new confidence measure provides a simple way for the agent to judge how much it should trust its current understanding (of how well available plans can solve goal-events). 
%%
In contrast with previous proposals, the confidence in a plan may \emph{not} increase monotonically, and it will drop whenever the learned behavior becomes less successful in the environment, thus allowing for new plan exploration to recover goal achievability. 
%
The new learning mechanism subsumes previous approaches in that it still preserves the traditional monotonic convergence in environments with fixed dynamics.
%
Furthermore, unlike in~\cite{singh10:extending,singh10:learning}, it does not require any account of the number of possible choices below a plan in the hierarchy, and hence scales up for any general goal-plan structure irrespective of its complexity. 
%%
We demonstrated the effectiveness of the proposed BDI learning framework using a simplified energy storage domain whose dynamics is intrinsically changing.

Perhaps the most more severe limitation of our learning framework is that it still cannot account for interactions between a plan's subgoals. If an agent plan involves two sub-goals (e.g., book flight and book hotel), these are learnt independently of each other. This ``local'' learning may then yield sub-optimal behavior. One way of addressing this may be to use extended notions of execution traces that take into account \emph{all} subgoals that led to the final execution outcome.


The issue of combining learning within BDI deliberation has not been widely addressed in the literature.
%%
\citet{hernandez04:learning} reported preliminary results on learning the context condition for a \emph{single} plan using, and hence they do not consider the issue of learning in plan hierarchies.
%%
Other work has focused on integrating \emph{offline} (rather than online) learning within deliberation in BDI systems, such as the work of \citet{lokuge07:improving} for ship berthing logistics and that of ~\citet{riedmiller01:karlsruhe} within a robotic soccer domain.
% {riedmiller01:karlsruhe,brusey02:learning} within a robotic soccer domain.
%%
The recent approach of \citet{broekens10:reinforcement} on integrating reinforcement learning to improve plan selection in GOAL agent language complements ours, in that it may be integrated as ``meta-level'' learning to influence the plan selection calculation given by the weighting function $\Omega$ (see Section~\ref{sec:confidence}). Indeed, we plan to investigate this integration in our future work.







