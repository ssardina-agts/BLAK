%!TEX root = ../ijcai11storage.tex
%\newpage 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Integrated BDI Learning Framework}\label{sec:framework}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%We begin by reviewing the basic agent programming framework that will be used throughout the paper~\cite{airiau09:enhancing,singh10:extending,singh10:learning}, which is a seamless integration of standard Belief-Desire-Intention (BDI) agent-oriented programming~\cite{Rao96:AgentSpeak,WooldridgeBook} with decision tree learning~\cite{Mitchell97:ML}. 

We now describe the BDI learning framework~\cite{airiau09:enhancing,singh10:extending,singh10:learning}, which is a seamless integration of standard Belief-Desire-Intention (BDI) agent-oriented programming~\cite{WooldridgeBook} with decision tree learning~\cite{Mitchell97:ML}. 
%
Within the framework, three mechanisms become crucial. First is a principled approach to learning context conditions based on execution experiences (Section~\ref{sec:context}). Second is a dynamic confidence measure in ongoing learning to decide how much trust to put in the current understanding of the world (Section~\ref{sec:confidence}). Finally, an adequate plan selection scheme compatible with the new type of plans' preconditions is required (Section~\ref{sec:selection}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Learning Context Conditions from Experience}\label{sec:context}
To that end, it was proposed to generalize the account for plans' context conditions to decision trees~\cite{Mitchell97:ML} that can be learnt over time~\cite{airiau09:enhancing,singh10:extending,singh10:learning}. The idea is simple: \emph{the decision tree of an agent plan provides a judgement as to whether the plan is likely to succeed or fail for the given situation.}
%%
By suitably \emph{learning} the structure of and adequately \emph{using} such decision trees, the agent is able to improve its performance over time 
%and release 
, lessening the burden on
the domain modeller to encode ``perfect'' plan preconditions. Note that the classical boolean context conditions provided by the designer could (and generally will) still be used as initial necessary but possibly insufficient requirements for each plan that will be further \emph{refined} in the course of trying plans in various world states.

When it comes to the learning process, the training set for a given plan's decision tree contains samples of the form $[w, e, o]$, where $w$ is the world state---a vector of discrete attributes---in which the plan was executed, $e$ is the vector of parameters for the goal-event that this plan handles, and $o$ is the execution outcome, namely, success or failure. Initially, the training set is empty and grows as the agent tries the plan in various world states for different event-goal parameter values and records each execution result. 
%%
Since the decision tree inductive bias is a preference for smaller trees, one expects that the learnt decision tree consists of only those world attributes and event-goal parameters that are relevant to the plan's (real) context condition.
%%

The typical {\em offline} use of decision trees assumes availability of the complete training set. In our case, however, learning and acting are interleaved in an {\em online} manner: the agent uses its current learning to make plan choices, gaining new knowledge that impacts subsequent choices. This raises the issue of deciding how ``good'' the intermediate generalisations are. Previous approaches have addressed this issue in two different ways. The first~\cite{airiau09:enhancing} is to be selective in recording experiences based on how sensible the related decisions were. For instance, in Figure~\ref{fig:confidence}, consider the case where plan selection results in the failed execution trace $\lambda = G[P:w] \cdot G_2[P_f:w_2] \cdot G_5[P_n:w_5]$.\footnote{Note that trace $\lambda$ assumes the successful resolution of subgoal $G_1$ and the resulting world state $w'$, as described by $\lambda_1$ previously.} Here, should the negative outcome be recorded for training our decision tree at non-leaf nodes $P_f$ and $P$? The concern is that these non-leaf plans failed not because they were a bad choice for world $w$ but because a bad choice ($P_n$) was made further down in the hierarchy. To resolve this issue, a {\em stability} filter is used in \cite{airiau09:enhancing} to record failures only for those plans whose outcomes are considered to be stable, or ``well-informed.'' The second approach is to record always, but instead adjust a plan's selection probability based on some measure of {\em confidence} in its decision tree. In~\cite{singh10:extending,singh10:learning}, the reliability of a plan's decision tree is considered to be proportional to the number of sub-plan choices (or paths below the plan in the goal-plan hierarchy) that have been explored currently: the more choices that have been explored, the greater the confidence in the resulting decision tree. 

The approach of using confidence to adjust plan selection weights is shown to be more generally applicable to learning in BDI hierarchies~\cite{singh10:extending,singh10:learning}, and is the one we use in this study. However, the traditional assumption that learning is a one-off process is no longer applicable for embedded learning agents that operate over long time frames. In fact, they must often un-learn and re-learn as necessary, due to ongoing changes in the environment dynamics. The measure of confidence---that effectively defines the exploration strategy---is therefore no longer a monotonically increasing function, and must dynamically adjust accordingly if previously learnt solutions no longer work. Our new confidence measure for such environments with changing dynamics is presented next. Moreover, it subsumes the functionality of the former methods (it may be used as a direct replacement for them), since it behaves monotonically in environments where the dynamics are fixed. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Determining Confidence in Ongoing Learning}\label{sec:confidence}

\newcommand{\ds}{\zeta}
\newcommand{\app}{\mathname{app}}
\newcommand{\stable}{\mathname{stable}}


We now describe our confidence measure for learning in environments with continually changing dynamics and where the agent must invariably ``keep on learning'' as it were. It uses the notion of {\em stability} introduced in~\cite{airiau09:enhancing}, and defined in ~\cite{singh10:learning} as follows:
%Conceptually, the value of the confidence measure relates to the degree of trust that the agent has in its current understanding of the world (from a learning perspective). Technically, recall that the confidence metric informs the agent about how much it should trust the outcome estimate provided by its current decision tree.
%
%\begin{quote}
\emph{``A failed plan $P$ is considered to be stable for a particular world state $w$ if the rate of success of $P$ in $w$ is changing below a certain threshold.''}
%\end{quote} 
Here the rate is simply the ratio of successful to total experiences in $w$ and stability is measured against the difference between two consecutive rates.
%
Our aim is to use this notion to judge how  well-informed the decisions the agent has made within a particular execution trace were. This is particularly meaningful for \emph{failed} execution traces: low stability suggests that we were not well-informed and more exploration is needed before assuming that no solution is possible (for the trace's top goal in question).
%%
To capture this, we define the \emph{degree of stability} of a (failed) execution trace $\lambda$, denoted $\ds(\lambda)$ as the ratio of stable plans to total applicable plans in the active execution trace below the top-level goal event in $\lambda$. Formally, when $\lambda= G_1[P_1:w_1] \cdots G_n[P_n:w_n]$ we define 
%%
\[
\ds(\lambda) = 
	\frac{ 
			\card{ \bigcup_{i=1}^n \set{P \mid P \in \Delta_{\app}(G_i,w_i),\, \stable(P,w_i)} } 
		}
		{
			\card{	\bigcup_{i=1}^n \Delta_{\app}(G_i,w_i) } 
		},
\]

\noindent
where  $\Delta_{\app}(G_i,w_i)$ denotes the set of all applicable plans (i.e., whose boolean context conditions hold true) in world $w_i$ for goal event $G_i$, and $\stable(P,w_i)$ holds true if plan $P$ is deemed stable at world $w_i$, as defined in~\cite{singh10:learning}.

For instance, take the failed execution trace $\lambda = G[P:w] \cdot G_2[P_f:w_2] \cdot G_5[P_n:w_5]$ from before and suppose that the applicable plans are $\Delta_{\app}(G,w) = \{P\}$, $\Delta_{\app}(G_2,w_2) = \{P_d,P_f\}$, and $\Delta_{\app}(G_5,w_5) = \{P_m,P_n,P_o\}$. Say also that $P_d$ and $P_n$ are the only plans deemed stable (in worlds $w_2$ and $w_5$ respectively). 
%%
Then the degree of stability for the whole trace is $\ds(\lambda)= 2/6$.
%%
Similarly, for the two subtraces $\lambda'= G_2[P_f:w_2] \cdot G_5[P_n:w_5]$ and $\lambda'' =G_5[P_n:w_5]$ of $\lambda$, we get $\ds(\lambda') = 2/5$ and $\ds(\lambda'') = 1/3$.



\newcommand{\StablePlan}{\mathname{StablePlan}}
\newcommand{\SetDegreeStability}{\mathname{RecordDegreeStability}}
\newcommand{\UpdateDegreeStability}{\mathname{RecordDegreeStabilityInTrace}}

The idea is that every time the agent reaches a failed execution trace, the stability degree of each subtrace is stored in the plan that produced that subtrace.
%%
So, for plan $P$ we store degree $\ds(\lambda')$ whereas for plan $P_f$ we record degree $\ds(\lambda'')$. Leaf plans, like $P_n$, make no choices so their degree is assigned $1$.
%%
Intuitively, by doing this, we record against each plan in the (failed) trace, an estimate of how informed the current (active) choices made for the plan were.  
%%
Algorithm~\ref{alg:degree} describes how this (hierarchical) recording happens given an active execution trace $\lambda$. Observe how the stability measure is recorded against each plan in the trace: $\SetDegreeStability(P, w, d)$ records (i.e. saves to memory) degree $d$ for plan $P$ in world state $w$.

\begin{algorithm}[h]
\KwData{$\lambda=G_1[P_1:w_1] \cdot \ldots \cdot G_n[P_n:w_n]$, with $n \geq 1$.}
\KwResult{Records degree of stability for plans in $\lambda$.}
\If{$(n > 1)$}{
	$\lambda'=G_2[P_2:w_2] \cdot \ldots \cdot G_n[P_n:w_n]$\;
	$d = \ds(\lambda')$\;  
	$\SetDegreeStability(P_1, w_1, d)$\;
	$\UpdateDegreeStability(\lambda')$\;
}
\Else{$\SetDegreeStability(P_1, w_1, 1)$\;}
\caption{$\UpdateDegreeStability(\lambda)$}
\label{alg:degree}
\end{algorithm}


As plan execution produces new failed experiences, the calculated degree of stability is appended against it each time. When a plan finally succeeds, we take an optimistic view and record degree $1$ against it. 
%
In other words, choices that led to success are considered well-informed, but for failures we want to know just how good the decisions were.
%
The fact that all plans do eventually become stable or succeed, means that $\ds(\lambda)$ is guaranteed to converge to $1$ (for fixed dynamics). 


To aggregate the different stability recordings for a plan over time, we use the \emph{average degree of stability} over the last $n \geq 1$ executions of plan $P$ in $w$, denoted $\C_s(P,w,n)$. 
%%
This provides us with a measure of confidence in the decision tree for plan $P$ in state $w$. Intuitively, $\C_s(P,w,n)$ tells us how ``informed" the decisions taken when performing $P$ in $w$ were over the $n$ most recent executions.
%%
Notice that if the environment dynamics are constant, this measure monotonically increases from $0$, as plans below $P$ start to become stable (or succeed); it reaches $1$ when all tried plans below $P$ in the last $n$ executions are considered stable. This is what one might expect in the typical learning setting. However, if the environment dynamics were to change and plans start to fail or become unstable, then the measure behaves non-monotonically and adjusts confidence accordingly.

\newcommand{\neww}{\mathname{NewStates}}
Normally, generalising using decision trees is useful, if one has ``enough'' data. For us, this amounts to trying a plan in a given world in meaningful ways (captured by $\C_s(P,w,n)$), as well as trying it in different worlds where it is reachable in the hierarchy. To approximate the latter, we monitor {\em the rate at which new worlds are being witnessed by a  plan $P$}. During early exploration, most worlds that a plan is selected for will be unique, thus yielding a high rate (corresponding to low confidence). Over time, the plan would get selected in all worlds in which it is reachable and the rate of new worlds would approach zero (corresponding to full confidence). Given this, we define our second confidence metric $\C_d(P,n) = 1 - |\neww(P,n)|/n$, where $\neww(P,n)$ is the set of world states in the last $n$ executions of $P$ that have also been witnessed before that. Clearly, $\C_d$ is guaranteed to converge to $1.0$ after all worlds where the plan might be considered are eventually witnessed, although note that it may not do this monotonically.

We now define our final (aggregated) confidence measure $\C(P,w,n)$ using the stability-based metric $\C_s(P,w,n)$ that reflects how well-informed the last $n$ executions of plan $P$ in world $w$ were, and the domain metric $\C_d(P,n)$ that captures how well-known the worlds in the last $n$ executions of plan $P$ were compared with what we had experienced before:
%%
\[
	\C(P,w,n) = \alpha\C_s(P,w,n) + (1-\alpha)\C_d(P,n),
\label{eqn:confidence}
\]

\noindent
where $\alpha$ is a weighting factor used to set a preference bias between the two component metrics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Using Learning and Confidence to Select Plans}\label{sec:selection}

\newcommand{\aSet}{\mathname{set}}
\newcommand{\aOperate}{\mathname{operate}}
\newcommand{\aEvaluate}{\mathname{evaluate}}

\newcommand{\pSet}{\mathname{Set*}}
\newcommand{\pSetCharge}{\mathname{SetCharge}}
\newcommand{\pSetDischarge}{\mathname{SetDischarge}}
\newcommand{\pSetNotUsed}{\mathname{SetNotUsed}}
\newcommand{\pExecute}{\mathname{Execute}}

\newcommand{\cSatisfies}{\psi}

\input{figs/fig-experiments}

Finally, we show how this confidence measure is to be used within the plan selection mechanism of a BDI agent. Remember that for a given goal-event that needs to be resolved, a BDI agent may have several applicable plans from which one ought to be selected for execution. We do this by choosing probabilistically among these options in a way proportional to some given weight per plan---the more weight a plan is assigned, the higher the chances of it being chosen. 
%%
Following~\cite{singh10:extending,singh10:learning}, we define this selection weight for plan $P$ in world $w$ relative its last $n$ executions as follows: 
%%
\[
	\Omega(P,w,n) = 0.5 + \left[  \C(P,w,n) \times  \left( \P(P,w) - 0.5 \right)  \right],
\label{eqn:omega}   
\]

\noindent 
where $\P(P,w)$ is the probability of success of plan $P$ in world $w$ as given by $P$'s decision tree. 
%
Indeed, this weight definition is basically that of~\cite{singh10:extending,singh10:learning}, except for the use of our new confidence metric $\C$ as defined above. The idea is to combine the likelihood of success of plan $P$ in world $w$ (the term $\P(P,w)$) with a confidence bias (in this case $\C \in [0.0:1.0]$) to determine a final plan selection weight $\Omega$. 
When the confidence is maximum i.e. $1.0$, then $\Omega = \P(P,w)$, and the final weight equals the likelihood reported by the decision tree; when the confidence is zero, then $\Omega=0.5$, and the decision tree has no bearing on the final weight (a default weight of $0.5$ is used instead).

