%!TEX root = ../ijcai11storage.tex
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:introduction}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The Belief-Desire-Intentions (BDI) agent programming paradigm
\cite{Georgeff89-PRS,Rao96:AgentSpeak,WooldridgeBook} is a
successful and popular approach to developing complex systems that
will behave robustly in situations where the environment is
interacting with the agent system.
However they do not incorporate learning, and thus, once deployed,
have no ability to adjust to changes in the dynamics of an environment
that cause previously successful approaches to fail and vice versa.
%%
In this work we maintain the structure and advantages of a BDI agent
program, but integrate learning, adapting and adjusting machine
learning techniques that are generally used to learn behaviour in a
static environment (often done off-line with a training set), to be
suitable for online continuous learning in an environment where what
works may well change over time.
%%
Our aim is to use the operational knowledge that is encoded in the
agent program, but to learn refinements to that knowledge, and also to
learn adjustments in the case of changes in the behaviour of the
environment. 
%%
One of the key issues in any learning system is that of exploitation
vs. exploration, that is, how much to believe and exploit the current
(learnt) knowledge, versus how much to try things in order to gain new
knowledge. Important in this balance is an understanding of how much
learning has already been done, or how much has already been
explored. In~\cite{singh10:extending,singh10:learning}, a
``coverage-based" measure of confidence was used to capture how much
the agent should trust its current understanding (of good plan
selection), and therefore exploit rather than explore. Intuitively,
such confidence was based on the degree to which the space of possible
execution options for the plan has been explored (i.e., covered) so
far. The greater the extent to which this space has been explored, the
greater the confidence, and consequently the more likely the agent is
to exploit.   
%
As recognised by~\cite{singh10:learning}, the coverage-based
confidence approach does not support learning in a changing
environment. This is because the confidence increases
\emph{monotonically} and, as a result, there is no ability for the
agent to become less confident in its choices, even if its currently
learned behaviour becomes less successful due to changes in the
environment. 

Consider, for instance, a smart office building equipped with a large
battery system that can be charged when there is excess power, and
used (i.e., discharged) when there is excess demand, in order to
achieve an overall desired building consumption rate for a given
period. A battery installation is built from independent modules with
different chemistries. Initially, the embedded battery controller can
be programmed (or can learn) to operate optimally. However, over time,
the modules in a battery tend to operate less well or some may even
cease to function altogether. In addition, modules may be replaced
with some frequency.   
%%	
What is needed is an embedded controller agent that after having explored
the space well and developed a high confidence in what it has learned,
is able to observe when this learned knowledge becomes unstable, and
dynamically modify its confidence allowing for new exploration and
revised learning in an ongoing way. To achieve this we develop a
confidence measure based on an assessment of how well informed our
selections are, combined with an observation of the extent to which we
are seeing new and different situations.

The rest of the paper is as follows.
%
First, we introduce BDI agent systems as well as machine learning
concepts necessary to understand this work.  
%
We then describe the BDI learning framework, initially developed
by~\cite{airiau09:enhancing,singh10:extending,singh10:learning}, and
enhance this with our new confidence measure (Section~\ref{sec:confidence})
for learning in environments with changing dynamics.  
%
Next, we describe a battery controller agent, based on a real
application that requires ongoing learning, and show how it adjusts in
a variety of ways to an environment where battery behaviour changes.  
%
We conclude with related work and limitations requiring future work.



%\newpage
%In this work, we develop a new confidence measure, compatible with the BDI learning framework, which allows the agent to adjust its confidence as the environment changes.
%%%
%The new metric is built from two ingredients.
%%%
%First, it uses the notion of plan stability from~\cite{airiau09:enhancing,singh10:learning} to quickly estimate how much the different options for achieving a goal have been explored.
%%%
%Second, it considers how much the agent is experiencing states that have been seen before vs how much it is seeing new situations, by using a sliding window that checks what percentage of situations in the window have been seen previously. If a substantial number of new situations are being experienced, then the agent should be less confident in what it has previously learnt. 

%The rest of the paper is organised as follows.
%%%
%In the following section, we provide an overview of the basic BDI learning framework on which this work is based, as developed by~\cite{airiau09:enhancing,singh10:extending,singh10:learning}. 
%%%
%We then explain in detail our new proposal for a dynamic measure of confidence that may be used by the BDI agent at plan-selection time to continually adjust to a changing world. 
%%%
%Following that, we describe an energy storage domain taken from a real application where the environment dynamics changes over time requiring adaptive learning. We then specify some experiments evaluating the battery controller agent in different situations, and demonstrate that our learning approach for BDI systems does in fact allow the agent to adjust in a variety of ways to an environment where battery behaviour changes. 
%%%
%We conclude the paper by discussing related work and some limitations requiring future work.
%
%
