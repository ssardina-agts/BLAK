%!TEX root = ../ijcai11storage.tex
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:introduction}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The Belief-Desire-Intentions (BDI) agent programming paradigm
\cite{WooldridgeBook,BusettaRHL:AL99-JACK,Pokahr:EXP03-JADEX,jasonbook} is a
successful and popular approach to developing complex systems that
behave robustly in situations where the environment is
interacting with the system.
However, BDI agents do not incorporate learning, and once deployed,
have no ability to adjust to changes in the dynamics of an environment
that cause previously successful approaches to fail and vice versa.
%%
In this work we maintain the structure and advantages of a BDI agent
program, but integrate adaptive machine
learning techniques that are generally used to learn behaviour in a
fixed environment (often off-line with a training set),
for use in online continuous learning in an environment where what
works, may well change over time.
%%
In that way, our contribution is to agent programming---\emph{to make BDI programs more adaptive by seamlessly integrating learning techniques}---rather that to machine learning research.
%%
The aim is to use the operational knowledge encoded in the agent program, but learn refinements to it so as to cope with changes in the dynamics of the environment. 

A key issue when learning is that of exploitation \emph{vs.} exploration: how much to trust and exploit the current
(learnt) knowledge, versus how much to try things in order to gain new
knowledge. 
%Important in this balance is an understanding of how much
%knowledge has already been
%explored. 
In our earlier work~\cite{singh10:extending,singh10:learning}, a
``coverage-based" measure of confidence was used to capture how much
the BDI agent should trust its current understanding (of good plan
selection), and therefore exploit rather than explore. Intuitively,
this confidence was based on the degree to which the space of possible
execution options for the plan has been explored (covered).
The more this space has been explored, the
greater the confidence, and the more likely the agent is
to exploit.   
%
As recognised in this work, the coverage-based
confidence approach does not support learning in a changing
environment. This is because the confidence increases
\emph{monotonically} and, as a result, there is no ability for the
agent to become less confident in its choices, even if its currently
learned behaviour becomes less successful due to changes in the
environment. 

Consider a smart building equipped with a large
battery system that can be charged when there is excess power, and
used (discharged) when there is excess demand, in order to
achieve an overall desired building consumption rate for some
period. A battery installation is built from independent modules with
different chemistries. Initially, the embedded battery controller can
be programmed (or can learn) to operate optimally. However, over time
modules may operate less well or even
cease to function altogether. In addition, modules may be replaced
with some frequency.   
%%	
Thus, what is needed is a controller that, after having explored
the space well and developed high confidence in its learning,
is able to observe when learned knowledge becomes unstable, and
dynamically modify its confidence allowing for new exploration and
revised learning in an ongoing way. To achieve this we develop a
confidence measure based on an assessment of how well informed our
selections are, combined with an observation of the extent to which we
are seeing new situations.

The rest of the paper is as follows.
%
First, we introduce BDI systems and machine learning
concepts necessary to understand this work.  
%
We then describe our BDI learning framework, initially developed
in~\cite{airiau09:enhancing,singh10:extending,singh10:learning}, and
extend this with our new confidence measure (Section~\ref{sec:confidence})
for learning in environments with changing dynamics.  
%
Next, we describe a battery controller agent, based on a real
application that requires ongoing learning, and show how it adjusts in
a variety of ways to an environment where battery behaviour changes.  
%
We conclude with related work and limitations requiring future work.



%\newpage
%In this work, we develop a new confidence measure, compatible with the BDI learning framework, which allows the agent to adjust its confidence as the environment changes.
%%%
%The new metric is built from two ingredients.
%%%
%First, it uses the notion of plan stability from~\cite{airiau09:enhancing,singh10:learning} to quickly estimate how much the different options for achieving a goal have been explored.
%%%
%Second, it considers how much the agent is experiencing states that have been seen before vs how much it is seeing new situations, by using a sliding window that checks what percentage of situations in the window have been seen previously. If a substantial number of new situations are being experienced, then the agent should be less confident in what it has previously learnt. 

%The rest of the paper is organised as follows.
%%%
%In the following section, we provide an overview of the basic BDI learning framework on which this work is based, as developed by~\cite{airiau09:enhancing,singh10:extending,singh10:learning}. 
%%%
%We then explain in detail our new proposal for a dynamic measure of confidence that may be used by the BDI agent at plan-selection time to continually adjust to a changing world. 
%%%
%Following that, we describe an energy storage domain taken from a real application where the environment dynamics changes over time requiring adaptive learning. We then specify some experiments evaluating the battery controller agent in different situations, and demonstrate that our learning approach for BDI systems does in fact allow the agent to adjust in a variety of ways to an environment where battery behaviour changes. 
%%%
%We conclude the paper by discussing related work and some limitations requiring future work.
%
%
