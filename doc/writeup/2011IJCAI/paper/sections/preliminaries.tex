%!TEX root = ../ijcai11storage.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}\label{sec:preliminaries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{BDI Agent Systems}

%We begin by reviewing the basic agent programming framework that will be used throughout the paper~\cite{airiau09:enhancing,singh10:extending,singh10:learning}, which is a seamless integration of standard Belief-Desire-Intention (BDI) agent-oriented programming~\cite{Rao96:AgentSpeak,WooldridgeBook} with decision tree learning~\cite{Mitchell97:ML}. 

BDI agent programming languages are typically built around an explicit representation of propositional attitudes (e.g., beliefs, desires, intentions, etc.). A BDI architecture addresses how these components are modelled, updated, and processed, to determine the agent's actions.
%%
There are a plethora of agent programming languages and development platforms in the BDI tradition, including
%such as \PRS\ \cite{Georgeff89-PRS},
\JACK~\cite{BusettaRHL:AL99-JACK},  
\JADEX~\cite{Pokahr:EXP03-JADEX}, and
%\TAPL~\cite{Hindriks99:Agent} and
%\DAPL~\cite{Dastani:JAAMAS08-2APL}, 
\JASON~\cite{jasonbook}
%, and SRI's \SPARK~\cite{MorelyM:AAMAS04-SPARK}, 
among others. 
%%
Specifically, a BDI intelligent agent systematically chooses and executes \emph{plans} (i.e., operational procedures) to achieve or realise its goals, called \emph{events}.
%%
Such plans are extracted from the agent's \emph{plan library}, which encodes the ``know-how'' information of the domain the agent operates in.
%%
For instance, the plan library of an unmanned air vehicle (UAV) agent controller may include several plans to address the event-goal of landing the aircraft. Each plan is associated with a \emph{context condition} stating under which belief conditions the plan is a sensible strategy for addressing the goal in question. Whereas some plans for landing the aircraft may only be suitable under normal weather conditions, other plans may only be used under emergency operations.
%%%
Besides the actual execution of domain actions (e.g., lifting the flaps), a plan may require the resolution of (intermediate) sub-goal events (e.g., obtaining landing permission from the air control tower). As such, the execution of a BDI system can be seen as a \textit{context sensitive subgoal expansion}, allowing agents to ``act as they go'' by making \emph{plan choices} at each level of abstraction with respect to the current situation. The use of plans' context (pre)conditions to make choices as late as possible, together with the built-in goal-failure mechanisms, ensures that the system is responsive to changes in the environment. 

By grouping together plans responding to the same event type, the agent's plan library may be viewed as a set of goal-plan tree templates (e.g., Figure~\ref{fig:confidence}): a goal-event node (e.g., goal $G_1$) has children representing the alternative \emph{relevant} plans for achieving it (e.g., $P_a,P_b$ and $P_c$); and a plan node (e.g., $P_f$), in turn, has children nodes representing the subgoals (including primitive actions) of the plan (e.g., $G_4$ and $G_5$). These structures, can be seen as AND/OR trees: for a plan to succeed all the subgoals and actions of the plan must be successful (AND); for a subgoal to succeed one of the plans to achieve it must succeed (OR). Leaf plans interact directly with the environment and so, in a given world state, they can either succeed or fail when executed; this is marked accordingly in the figure for some particular world (of course such plans may behave differently in other states).


Figure \ref{fig:confidence} shows the possible outcomes when plan $P$ is selected in a given world $w$. In order for the first subgoal $G_1$ to succeed, plan $P_a$ must be selected followed by $P_i$ that succeeds in the world (as indicated by the $\surd$ symbol). The {\em active execution trace}~\footnote{An active execution trace~\cite{singh10:learning} describes decision sequences that terminate in the execution of a leaf plan.} for this selection then is described as $\lambda_1=G[P:w] \cdot G_1[P_a:w] \cdot G_3[P_i:w]$ (highlighted in Figure~\ref{fig:confidence} as the line-shaded path terminating in $P_i$) where the notation $G[P:w]$ indicates that goal $G$ was resolved by the selection of plan $P$ given world state $w$. Subsequently subgoal $G_2$ is posted whose successful resolution is described by the intermediate trace $\lambda_2=G[P:w] \cdot G_2[P_f:w'] \cdot G_4[P_k:w']$ followed by the final trace $\lambda_3=G[P:w] \cdot G_2[P_f:w'] \cdot G_5[P_m:w'']$. Note that the world $w'$ in $\lambda_2$ is the resulting world state from the successful execution of leaf plan $P_i$ in the preceding trace $\lambda_1$. Similarly, $w''$ is the resulting world state from the execution of $P_k$ in $\lambda_2$. There is only one way for plan $P$ to succeed in the initial world $w$ as described by the traces $\lambda_1 \ldots \lambda_3$. All other execution traces lead to failures (as depicted by the $\times$ symbol).

\begin{figure}[t]
\begin{center}
\resizebox{0.8\columnwidth}{!}{
\input{figs/fig-confidence}
}
\end{center}
\vskip -0.5cm
\caption{An example goal-plan hierarchy.}
\label{fig:confidence}
\end{figure}


Evidently, adequate plan selection is crucial in BDI systems. Whereas standard BDI platforms leverage domain expertise by means of \emph{fixed} logical context conditions of plans, in this work we are interested in ways of improving or \emph{learning} plan selection for a situated agent, based on experience.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Machine Learning Concepts}

Our aim is to determine a plan's applicability (context) condition by learning the structure of a {\em decision tree}~\cite{Mitchell97:ML} from experiences of trying the plan in different situations. The idea is that the learnt structure will summarise the set of experiences into a compact form i.e. correctly {\em classify} (as pass or fail) the given experiences (the plan failed in world $w_1$, passed in $w_2$, etc.), and may be used to decide the likely outcome in unseen situations.
%
We use decision trees because (i) they support hypotheses that are a disjunction of conjunctive terms and this is compatible with how context formulas are generally written; (ii) they can be converted to if-then rules that are human readable and can be validated by an expert; and (iii) they are robust against ``noisy'' training data such as in stochastic domains where valid plans may nevertheless fail due to unknown factors. 
%
% Specifically, we use the algorithm \propername{J48}, a version of Quinlan's \propername{C4.5}~\cite{quinlan93:c4.-5:-programs} , from the \weka\ learning package~\cite{weka99}.
%%
Specifically, we use the algorithm \propername{J48}, a version of Quinlan's~\propername{C4.5} from the \weka\ learning package~\cite{weka99}.



The second concept important for this work is that of {\em confidence}: how much should the agent rely on its current learning, given that acting and learning are interleaved. Confidence determines the exploration strategy: the lower it is, the less reliable the learning is and the more the agent {\em explores} the options available to it; the higher it is, the more it {\em exploits} its learning. Typically (e.g. in reinforcement learning) the exploration strategy is fixed upfront (e.g. constant or monotonically decreasing in $\epsilon$-greedy and Boltzmann exploration~\cite{sutton98:reinforcement}). The basic assumption is that learning is a one-off process that improves over time. We argue, however, that for an embedded learning agent this assumption is too limiting. Instead, we assume that in the course of its lifetime the deployed agent will likely experience many changes in the environment: some of which will cause previously learnt solutions to no longer work. For this reason, we propose a dynamic confidence measure (Section~\ref{sec:confidence}) that {\em adapts} accordingly with changes in performance, and forms the basis for a more practical heuristic for guiding exploration.
