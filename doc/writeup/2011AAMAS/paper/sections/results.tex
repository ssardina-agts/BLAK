\input{figs/fig-experiments}
%!TEX root = ../aamas11storage.tex
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results}\label{sec:results}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we show experiments that demonstrate the suitability of the framework developed in Section~\ref{sec:framework} using the energy domain example of Section~\ref{sec:application}. In particular, we report on three experiments that highlight the adaptive behaviour of the controller under different situations in which the environment dynamics are changing. 
%%
The first experiment shows how the agent recovers functionality against standard deterioration in module capacities. In the second one, the agent is exposed to partial (temporal) failures of the system, that is, modules malfunctioning that would still not preclude the battery to respond successfully to requests. Lastly, in the third experiment, we analyse the controller response and learning behaviour in the extreme case where the system suffers complete failure for some time and is thereafter restored.



\subsubsection{Experimentation Setup}

The following experimental setup applies to all experiments. 
%%
We conducted experiments for a battery system with \emph{five} modules (i.e., $k=5$). For each module, the current charge state is described by a discrete value in the range $[0:3]$, where zero indicates a fully discharged state and three indicates a fully charged state. In addition, each module has an assigned configuration for the current period from the set $\{+c, 0, -c\}$, where $c=1/k$. The operational model is simple: charging is meant to add $+c$ while discharging is meant to add $-c$ to a module's charge state, otherwise the state is unchanged for the period (i.e., there is no charge loss).
%%
Thus, the desired overall battery response is in the (normalized) range $[-1.0:+1.0]$ in discrete intervals of $\pm c$. 
%%
The complete state space for the problem is described by the number of modules ($5$), the possible requests ($11$), the charge state of the system ($4^5$), and the assigned configuration of the system ($3^5$), that is, $5 \times 11 \times 4^5 \times 3^5 \approx 13.7$ million states. Though significant, note that the agent does not have to learn over this space, because the filtering of nonsensical configurations by the plans' context conditions $\psi_i$ will reduce substantially it.




At the beginning of each learning episode, the configuration of each module is reset to $0$ (i.e., not in used). 
%%
The charge state of each module, however, is left untouched and carries over from the previous episode, as it would be the case in the actual deployed system. This has implications for learning, particularly that the state space is \emph{not} sampled uniformly. %%
Each episode corresponds to one $G(r,5,s)$ goal-event request: achieve overall battery response of $r$ under current module charge states $s$.  For simplicity of analysis, we assume only satisfiable requests: a solution always exists. The outcome of each episode is either no response (no configuration was executed), or a single invocation of the $\pExecute$ plan for operating (and evaluating) the battery.  The tolerance level is set to $0$, so that the battery response is deemed successful only when the sum of the module configurations matches the request exactly.

An important thing we used is an \emph{applicability threshold} for plan selection of $40\%$, meaning that plans with a likelihood of success below this value are discarded from consideration. While such feature does not alter the overal learning performance of the battery controller, it does preclude the battery to being operated (i.e., plan $\pExecute$) under module configurations that are bound to be unsuccessful. In fact, we found that the threshold used reduces the number of battery operations by $12\%$, which is substantial when considering battery life. Again, we stress that the difference in performance without the applicability threshold is not significant.


The threshold for stability calculation is set to $0.5$. We used an averaging window of $n=5$ for both the stability-based metric $\C_s(\cdot,\cdot,n)$ and the world-based metric $\C_d(\cdot,n)$, and a (balanced) weighting of $\alpha=0.5$ for the final confidence measure $\C(\cdot,\cdot,\cdot)$.
%%
Finally, each experiment is run five times and the reported results are averages from these runs.



\subsubsection{Experiment 1: Capacity Deterioration}

In this experiment we model the (typical) situation where module capacities deteriorate over time. In a real system this will happen gradually over several years of typical use. However, to ease the analysis of the impact of the change, we force this deterioration to occur instantaneously in this experiment. 
%%
Figure \ref{fig:experiment1} shows the results for this case. In the beginning of the experiment, the system performs ideally as programmed, and goes about recording its experiences although there is no evident use of the resulting learning yet. After some time (about $5K$ episodes), the capacity of all five modules drops instantaneously, from the initial range $[0:3]$) to range $[0:2]$. 
%%
These capacity changes result in a rapid drop in performance corresponding to the set of programmed/learnt solutions that no longer work. The ideally programmed system would, at this point, converge to $\approx 76\%$ performance. The learning system, however, aptly rectifies the situation by learning to mostly avoid module configurations that no longer work. 


%\begin{figure}[t]
%\begin{center}
%\input{figs/fig-experiment1}
%\end{center}
%\caption{Adapting to capacity deterioration.}
%\label{fig:experiment1}
%\end{figure}

\subsubsection{Experiment 2: Partial Failure with Restoration}

In this scenario, we model a series of module malfunctions and their subsequent restoration. In particular $Module1$ fails for the duration $[0:20k]$ after which it is reinstated, $Module2$ fails for the period $[20k:40k]$, and so on. Figure \ref{fig:experiment2} shows the system performance about this change.\footnote{The apparent difference in performance drops at $0k$ and $20k$ is due to the fact that the sampling of the state space is non-uniform and it just happens that more ``bad'' situations occurred about the first change than the second. The theoretical drop in performance for this change is $45\%$.} At the beginning of each change (at $0k$, and $20k$), the performance drops dramatically as the expected solutions that utilise the failed module no longer work. Following this, in each instance the system successfully learns to operate the battery without the module that is out of operation (ie. by always configuring that module to not-in-use). Note that by the time each failed module is restored, the system has already learnt to operate without it and so will not try to explore the restored possibilities until a future change causes a drop in performance and triggers new exploration.


%\begin{figure}[t]
%\begin{center}
%\input{figs/fig-experiment2}
%\end{center}
%\caption{Adapting to module failures/restorations over time.}
%\label{fig:experiment2}
%\end{figure}

\subsubsection{Experiment 3: Complete Failure with Restoration}

In this experiment, we model the extreme scenario of complete failure of the system for some time followed by full restoration. Here {\em all} solutions fail for the period $[0:5k]$, after which they are reinstated. Figure \ref{fig:experiment3} shows the system performance about this change. At the beginning of the experiment, performance drops to zero rapidly as all of the ideal solutions start to fail. After an initial period of failures (at around $2k$ episodes), the estimated likelihood of success of all plans drops below the applicability threshold of $40\%$. Beyond this point the battery operation comes to a complete halt since no plans are ever applicable in any situation and so the $\pExecute$ plan never gets invoked. The original behaviour is then reinstated at $5k$, however if by this stage no plans are being tried then new learning will also not occur. For this reason, the applicability threshold is implemented as a ``soft'' threshold. To be exact, the $40\%$ threshold applies $90\%$ of the time leaving some possibility for selecting plans below this threshold. This allows the battery to operate with some likelihood\footnote{Note that the battery operates after five $Set*$ plan selections. In the best case only one of these plans has failed the threshold and there is a $10\%$ chance that the battery will operate. However, if all plans fail the threshold then there is only a $0.1^5$ or $0.00001\%$ chance that the battery will operate.}, and the system is able to adapt to the new solution set beyond $6k$ episodes.

%\begin{figure}[t]
%\begin{center}
%\input{figs/fig-experiment3}
%\end{center}
%\caption{Adapting to system restoration after complete failure.}
%\label{fig:experiment3}
%\end{figure}

\Shout{Add closing para for the results section here.}
%\medskip

