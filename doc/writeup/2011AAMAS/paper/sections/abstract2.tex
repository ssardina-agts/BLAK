In this work we propose an important modification to a framework for
agent-oriented programming that integrates 
learning capabilities to the successful and popular
Belief-Desire-Intentions programming paradigm.
In learning which plan to select, a 
crucial issue in the online learning setting is how much to trust what
has been learnt so far (and therefore exploit it) versus how much to
explore further and increase the information on which the learning is
based. 
Previous work has employed a confidence measure that is based on an
estimate of how much of the space of options has been explored. A
problem with this approach however is that the confidence measure does
not adjust to a possibly changing situation/environment. In this paper
we take a notion of stability in the outcomes observed for a
particular plan, which was previously used for filtering training
data, and adapt it to form the basis of a new and more robust
confidence measure. We add to this a consideration of to what extent
we are still seeing new world situations.
This new confidence measure adjusts dynamically and thus allows, in
principle, infinitely many learning and re-learning phases as things change.
We demonstrate the utility of our approach with results obtained in a
practical energy storage domain. 

