%!TEX root = ../aamas11storage.tex
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Dynamic Confidence Measure}\label{sec:confidence}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Previous work has highlighted the importance of defining the exploration strategy for a general BDI learner in terms of it's goal-plan structure. The idea is that the extent of exploration of the structure in some way relates to our {\em confidence} in the resulting learning and may be used to construct the exploration heuristic. The notions of coverage \cite{singh10:learning} and structural complexity \cite{singh10:extending} both fall in this category. The benefit of these approaches is that they provide a monotonic confidence measure for guiding exploration i.e. plan selection in any general hierarchy. However they use an estimate of the \textit{potential choices} below each node in the hierarchy, that may be difficult to calculate. Comparatively, the \textit{stability} \cite{airiau09:enhancing,singh10:learning} measure relies purely on {\em actual} plan execution and being independent of the hierarchy also scales well. However, stability does not have the same granularity as the former, so is not very useful for guiding exploration\footnote{In previous work\cite{singh10:extending}, stability was used not as an exploration heuristic but as a filter to decide which experiences should be recorded for learning purposes.}. In this section, we describe a new confidence measure that combines the merits of the above approaches and overcomes their shortcomings. 

\begin{figure}[ht]
\begin{center}
%\resizebox{.45\textwidth}{!}{
\input{figs/fig-confidence}
%}
\end{center}
\caption{An example goal-plan hierarchy.}
\label{fig:confidence}
\end{figure}

To recap the definition of stability from \cite{singh10:extending}, ``A failed plan $P$ is considered to be stable for a particular world state $w$ if the rate of success of $P$ in $w$ is changing below a certain threshold $\epsilon$.'' Consider the example goal-plan structure of Figure \ref{fig:confidence} that shows the possible outcomes when plan $P$ is invoked in world state $w$. The $\surd$ and $\times$ symbols below the leaf plans indicate success and failure respectively. There is only one solution in world $w$ given by the sequential execution of the leaf plans $P_i$, $P_k$, and $P_m$. This is highlighted in Figure \ref{fig:confidence} as the shaded nodes. Line shading indicates initial execution traces while the solid shading highlights the final (active) execution trace. All other selection sequences lead to failures. Now let us consider the case where plan selection results in the failed active execution trace $\lambda_1=G[P:w] \cdot G1[P_a:w] \cdot G3[P_h:w]$. What should we make of this failure from a learning perspective? Should we record the negative sample for training our learners at non-leaf nodes $P_a$ and $P$? The concern stems from the fact that these non-leaf plans failed not because they were a bad choice for world $w$ but because a bad choice ($P_h$) was made further down in the hierarchy. The stability measure allows us to resolve the issue by recording only when plans are considered to be stable, or ``well-informed''. 

Given this, we define the {\em degree of stability of failed node $N$ in world $w$ as the ratio of the number of stable plans to the number of total applicable plans in the active execution trace starting at $N$ in $w$.} Here a node may be a plan or goal node. To see what this means, let us take the same failed trace $\lambda_1$ from before and say that leaf plan $P_h$ is stable. The degree of stability $s^o$ of each node in the trace then is given by:

\begin{eqnarray*}
s^o(P_h,w) = & \frac{stable~in~[P_h]}{total~in~[P_h]} & = \frac{1}{1}  \\
s^o(G_3,w) = & \frac{stable~in~[P_h,P_h,P_i]}{total~in~[P_h,P_h,P_i]} & = \frac{1}{3}  \\
s^o(P_a,w) = & \frac{stable~in~[P_a,P_h,P_h,P_i]}{total~in~[P_a,P_h,P_h,P_i]} & = \frac{1}{4} \\
s^o(G_1,w) = & \frac{stable~in~[P_a,P_h,P_h,P_i,P_b,P_c]}{total~in~[P_a,P_h,P_h,P_i,P_b,P_c]} & = \frac{1}{6}  \\
s^o(P,w) = & \frac{stable~in~[P,P_a,P_h,P_h,P_i,P_b,P_c]}{total~in~[P,P_a,P_h,P_h,P_i,P_b,P_c]} & = \frac{1}{7} 
\end{eqnarray*}

\Shout{Update the following paragraph and algorithm.}

Algorithm \ref{alg:degree} describes this calculation for any given active execution trace $\lambda$. Here $\eta$ is used to propagate the full list of sub-plans in the trace up to the parent. $StablePlan$ is a boolean function to check if the given plan $P_n$ is stable or not. For any given plan, $Choices$ holds the complete list of all executed sub-plans along with their stability status.

\begin{algorithm}[ht]
\KwData{$\lambda=G_0[P_0:w_0] \cdot \ldots \cdot G_n[P_n:w_n]$; $\eta=[P,b] \cdot \ldots \cdot [P_{r},b_{r}]$; $k\geq0$; $\epsilon\geq0$}
\KwResult{Updates confidence for plans in $\lambda$}
$choices(P_n,w_n) = choices(P_n,w_n) \cup P_n \cup \eta$\;
$confidence(P_n,w_n) +\!\!= |choices(P_n,w_n) == true|/|choices(P_n,w_n)|$\;
\If{$|\lambda| > 1$}{
	$\lambda'=G_0[P_0:w_0] \cdot \ldots \cdot G_{n-1}[P_{n-1}:w_{n-1}]$\;
	$\eta'+\!\!=[P_n,StablePlan(P_n,k,\epsilon)]$\;
	$UpdateConfidence(\lambda', \eta', k, \epsilon)$\;
}
\caption{$UpdateConfidence(\lambda, \eta, k, \epsilon)$}
\label{alg:degree}
\end{algorithm}

So the degree of stability of plan $P$ in $w$ after $\lambda_1$ is $1/7$. The same measure for a different failed trace $\lambda_2=G[P:w] \cdot G1[P_b:w]$ where plan $P_b$ is the only stable plan would be $1/4$. If we take the average degree of stability of plan $P$ over the last $n$ executions, then assuming all plans eventually become stable, this value is guaranteed to monotonically convergence to $1.0$. This {\em average degree of stability} then, may be used as a measure of our confidence in the decision tree for $P$ given $w$. 

\begin{equation}
\C_s(P,w,n) = \frac{s^o_0(P,w) + s^o_1(P,w) + \cdots + s^o_{n-1}(P,w)}{n}
\label{eqn:confidence-stability}
\end{equation}

Equation \ref{eqn:confidence-stability} defines this stability-based confidence $\C_s(P,w,n)$ for plan $P$ over the last $n$ executions in world $w$ . This measure monotonically increases as plans below $P$ start to become stable, and is $1.0$ when every plan that has been tried below $P$ is considered stable. 

This measure $\C_s$ would make a useful heuristic for exploration i.e. plan selection in it's own right: such that when the confidence is at it's lowest (i.e. $0.0$) we do maximum exploration and when the confidence is at it's highest (i.e. $1.0$) we fully utilise the decision tree. The problem with this approach is that $\C_s$ only covers the space of known worlds. This means that 
if a new world is witnessed, then $\C_s(P,w,n)=0.0$ meaning that we will choose plans randomly. This is hardly intelligent since what we would really like is to use the learnt generalisations to classify this new world rather than be agnostic about it. What is missing is a metric that contributes to our net confidence but that is independent of $w$.

One way to achieve this is by monitoring the rate at which new worlds are being witnessed by the plan $P$. During early exploration it is expected that the majority of worlds that a plan is selected for will be unique, therefore this rate is high and our confidence is low. Over time as exploration continues, the plan would get selected in all possible worlds and the rate of new worlds would approach zero while our confidence over this period would increase to it's maximum.  Equation \ref{eqn:confidence-domain} defines this confidence metric $\C_d(P,n)$ for plan $P$ over the last $n$ executions. Here, $W(P,*)$ is the set of all worlds witnessed by $P$ so far and $\triangle W(P,n)$ is the set of worlds witnessed in the last $n$ executions. $\C_d$ is guaranteed to converge to $1.0$ as long as all worlds are eventually witnessed.

\begin{equation}
\C_d(P,n) = \frac{|W(P,*)\cap \triangle W(P,n) |}{n}.
\label{eqn:confidence-domain}
\end{equation}

We are now ready to define our final confidence measure $\C$ based on the two component confidence metrics $\C_s$ and $\C_d$. Equation \ref{eqn:equation} describes this calculation. Here $\alpha$ is the weighting factor used to set a preference bias between the two components.

\begin{equation}
\C(P,w,n) = (\C_s(P,w,n)*\alpha) + [\C_d(P,n)*(1.0-\alpha)].
\label{eqn:confidence}
\end{equation}

Finally, Equation \ref{eqn:omega} shows how the confidence measure $\C(P,w,n)$ is used as a exploration heuristic during plan selection. Here $\P(P,w)$ is the probability of success of plan $P$ in world $w$ as given by it's decision tree and $\Omega(P,w,n)$ is the plan selection weight. This formulation of the plan selection weight is similar to the one presented in \cite{singh10:extending} bar the replacement of the coverage term with the new confidence term $\C(P,w,n)$.

\begin{equation}
\Omega(P,w,n) = 0.5 + \left[  \C(P,w,n) *  \left( \P(P,w) - 0.5 \right)  \right]
\label{eqn:omega}   
\end{equation}

\Shout{Now talk about the `dynamic' part and it's relevance to the application. Extract from application.tex.}

