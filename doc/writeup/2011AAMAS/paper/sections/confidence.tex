%!TEX root = ../aamas11storage.tex
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Dynamic Confidence Measure}\label{sec:confidence}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we describe a dynamic confidence measure that may be used to guide exploration when learning plan selection using the framework describes in Section \ref{sec:framework}. Conceptually, the value of the confidence measure relates to the degree of trust that the agent has in its current understanding of the world (from a learning perspective). Our new confidence measure improves upon previously used measures in two important ways. 
%
Firstly, it caters to a changing dynamics of the environment that often results in prior learning becoming less effective. The stability-based \cite{airiau09:enhancing} and coverage-based \cite{singh10:extending,singh10:learning} measures that have been previously proposed do not support the requirement for adaptability to changing dynamics. Moreover, the new measure proposed here subsumes the functionality of the former methods, as it behaves monotonically in environments where the dynamics are fixed. As such, it offers a direct replacement for the previous approaches. 
%
Secondly, the new measure does not rely on estimates of the number of choices in the goal-plan hierarchy as is the case in \cite{singh10:extending,singh10:learning}, and scales to any general goal-plan hierarchy irrespective of its complexity.


To recap the definition of stability from \cite{singh10:learning}:

\begin{quote}
\emph{``A failed plan $P$ is considered to be stable for a particular world state $w$ if the rate of success of $P$ in $w$ is changing below a certain threshold.''}
\end{quote} 

\newcommand{\ds}{\zeta}
\newcommand{\app}{\mathname{app}}
\newcommand{\stable}{\mathname{stable}}

Our aim is to use this notion to judge how ``stable'' the decisions the agent has made within a particular execution trace were. This is particularly meaningful for \emph{failed} execution traces: low stability suggests more exploration is needed before assuming no solution is possible (for the trace's top goal in question).
%%
To capture this, we define the \emph{degree of stability} of a (failed) execution trace $\lambda$, denoted $\ds(\lambda)$ as the ratio of stable plans to total applicable plans in the active execution trace below the top-level goal event in $\lambda$. Formally, when $\lambda= G_1[P_1:w_1] \cdots G_n[P_n:w_n]$ we define 
%%
\[
\ds(\lambda) = 
	\frac{ 
			\card{ \bigcup\limits_{i \in \{1,\ldots,n\}} 
						\set{P \mid P \in \Delta_{\app}(G_i,w_i), \stable(P,w_i)} } 
		}
		{
			\bigcup\limits_{i \in \{1,\ldots,n\}} \Delta_{\app}(G_i,w_i) 
		},
\]

\noindent
where  $\Delta_{\app}(G_i,w_i)$ denotes the set of all applicable (i.e., whose boolean context conditions hold true) plans in world state $w_i$ for goal event $G_i$, and $\stable(P,w_i)$ holds true if plan $P$ is deemed stable at world state $w_i$, as defined in~\cite{singh10:learning}.

For instance, take the the failed execution trace above $\lambda_1 = G[P:w] \cdot G_2[P_f:w_2] \cdot G_5[P_n:w_5]$ and assume that $P_n$ and $P_d$ are the only plans deemed stable (at the point where they are to execute in the trace). Suppose further that the applicable plans are $\Delta_{\app}(G,w) = \{P\}$, $\Delta_{\app}(G_2,w_2) = \{P_d,P_f\}$, and $\Delta_{\app}(G_5,w_5) = \{P_m,P_n,P_o\}$.
%%
Then the degree of stability for the whole trace is $\ds(\lambda_1)= 2/6$.
%%
Similarly, for the two subtraces $\lambda_1'= G_2[P_f:w_2] \cdot G_5[P_n:w_5]$ and $\lambda_1'' =G_5[P_n:w_5]$ of $\lambda_1$, we have $\ds(\lambda_1') = 2/5$ and $\ds(\lambda_1'') = 1/3$.



\newcommand{\StablePlan}{\mathname{StablePlan}}
\newcommand{\SetDegreeStability}{\mathname{RecordDegreeStability}}
\newcommand{\UpdateDegreeStability}{\mathname{RecordDegreeStabilityInTrace}}

Now, the idea is that every time the agent reaches a failing execution trace, the stability degree of each subtrace is stored in the plan that has produced such subtrace.
%%
So, for our example, for plan $P$ we store degree $\ds(\lambda_1')$ whereas for plan $P_f$ we record degree $\ds(\lambda_1'')$. Leaf plan nodes, like $P_n$, make no choices so their degree is simply $1$.
%%
Intuitively, by doing this, we record against each plan in the (failed) trace, an estimation on how informed the current (active) choices made for the plan were.  
%%
Algorithm~\ref{alg:degree} describes how this (hierarchical) recording happens given an active execution trace $\lambda$. Observe how a stability measure is reacorded against each plan in the trace: $\SetDegreeStability(P, w, d)$ records degree $d$ for plan $P$ in world state $w$.

\begin{algorithm}[h]
\KwData{$\lambda=G_1[P_1:w_1] \cdot \ldots \cdot G_n[P_n:w_n]$}
% ; $s\geq0$; $t\geq0$; $k\geq0$; $\epsilon\geq0$}
\KwResult{Records degree of stability for plans in $\lambda$.}
\If{$(n > 1)$}{
% 	$t = \Sigma_{i \in \{1,\ldots,n\}} \card{\Delta_{\app}(G_i,w_i)}$\;
% 	$s = \Sigma_{i \in \{1,\ldots,n\}} \card{\Delta_{\app}(G_i,w_i)}$\;
	$d = \ds(\lambda)$\;  
	$\SetDegreeStability(P_1, w_1, d)$\;
% 	\ForEach{$P_i$ in $T_n$; $P_i \neq P_n$}{
% 		$s' = s' + \StablePlan(P_i, w_n, k,\epsilon)$\;
% 		$t' = t + 1$\;
% 	}
	$\lambda'=G_2[P_2:w_2] \cdot \ldots \cdot G_n[P_n:w_n]$\;
	$\UpdateDegreeStability(\lambda')$\;
}
\Else{$\SetDegreeStability(P_1, w_1, 1)$\;}
\caption{$\UpdateDegreeStability(\lambda)$}
\label{alg:degree}
\end{algorithm}

As a plan is executed in different failed execution experiences, several degrees of stability are recorded against it. Note, however, that assuming all plans eventually do become stable, $\ds(P,w)$ is guaranteed to converge to $1$. 

So, as a measure of our confidence in the decision tree for plan $P$ in state $w$, we shall use the \emph{average degree of stability} over the last last $n$ executions of plan $P$ in $w$. We denote such measure $\C_s(P,w,n)$. Intuitively, $\C_s(P,w,n)$ tells us how stable were the decisions taken when performing $P$ in $w$ over the most recent failed executions.

Equation \ref{eqn:confidence-stability} defines this stability-based confidence measure $\C_s$ for plan $P$ over the last $n$ executions in world $w$ . This measure monotonically increases from $0.0$ as plans below $P$ start to become stable, and is $1.0$ when all tried plans below $P$ in the last $n$ executions are considered stable. 

\begin{equation}
\C_s(P,w,n) = \frac{s^o_0(P,w) + s^o_1(P,w) + \cdots + s^o_{n-1}(P,w)}{n}
\label{eqn:confidence-stability}
\end{equation}

\input{sections/fig-energy}

% \newcommand{\aSet}{\mathname{set}}
% \newcommand{\aOperate}{\mathname{operate}}
% \newcommand{\aEvaluate}{\mathname{evaluate}}
% 
% \newcommand{\pSet}{\mathname{Set*}}
% \newcommand{\pSetCharge}{\mathname{SetCharge}}
% \newcommand{\pSetDischarge}{\mathname{SetDischarge}}
% \newcommand{\pSetNotUsed}{\mathname{SetNotUsed}}
% \newcommand{\pExecute}{\mathname{Execute}}
% 
% \newcommand{\cSatisfies}{\psi}
% 
% \begin{figure*}[t]
% \begin{center}
% \subfigure[Use case scenario for a modular battery system.]{\label{fig:usecase}
% %\resizebox{0.9\columnwidth}{!}{
% \input{figs/fig-usecase}
% %}
% }
% \qquad
% \subfigure[Goal-plan hierarchy for a $k$-modules battery system.]{\label{fig:gptree}
% %\resizebox{0.9\columnwidth}{!}{
% \input{figs/fig-gptree}
% %}
% }
% \caption{An energy storage application.}
% \end{center}
% \label{fig:energystorage}
% \end{figure*}


The confidence measure $\C_s$ would make a useful heuristic for exploration (i.e., plan selection) in its own right: such that when the confidence is at its lowest we do maximum exploration and when it is at its highest we fully utilise the decision tree. The problem with this approach, however, is that $\C_s$ only covers the space of known worlds. This means that whenever a new world is witnessed, $\C_s=0.0$, meaning that we will choose randomly. This is hardly beneficial since what we would really like is to use the learnt generalisations to classify this new world rather than be agnostic about it. What is missing is a metric that contributes to our net confidence but that is independent of $w$.

One way to achieve this is by monitoring the rate at which new worlds are being witnessed by the plan $P$. During early exploration it is expected that the majority of worlds that a plan is selected for will be unique, therefore this rate is high and our confidence is low. Over time as exploration continues, the plan would get selected in all possible worlds and the rate of new worlds would approach zero while our confidence over this period would increase to its maximum.  Equation \ref{eqn:confidence-domain} defines this confidence metric $\C_d$ for plan $P$ over the last $n$ executions. Here, $W(P,*)$ is the set of all worlds witnessed by $P$ since the beginning and $\triangle W(P,n)$ is the set of worlds witnessed in the last $n$ executions. $\C_d$ is guaranteed to converge to $1.0$ as long as all worlds where the plan might apply are eventually witnessed.

\begin{equation}
\C_d(P,n) = \frac{|W(P,*)\cap \triangle W(P,n) |}{n}.
\label{eqn:confidence-domain}
\end{equation}

We are now ready to define our final confidence measure $\C$ based on the two component confidence metrics $\C_s$ and $\C_d$. Equation \ref{eqn:confidence} describes this calculation. Here $\alpha$ is the weighting factor used to set a preference bias between the two components.

\begin{equation}
\C(P,w,n) = (\C_s(P,w,n)*\alpha) + [\C_d(P,n)*(1.0-\alpha)].
\label{eqn:confidence}
\end{equation}

Finally, Equation \ref{eqn:omega} shows how the confidence measure $\C$ is used as a exploration heuristic during plan selection. Here $\P$ is the probability of success of plan $P$ in world $w$ as given by its decision tree and $\Omega$ is the plan selection weight. This formulation of the plan selection weight is similar to those presented in \cite{singh10:extending, singh10:learning} bar the replacement of earlier measures with the new confidence term $\C$.

\begin{equation}
\Omega(P,w,n) = 0.5 + \left[  \C(P,w,n) *  \left( \P(P,w) - 0.5 \right)  \right]
\label{eqn:omega}   
\end{equation}

The storage domain scenario (c.f. Section \ref{sec:application}) highlights the benefits of our dynamic confidence measure of Equation \ref{eqn:confidence}. In this domain it is easy to think of situations where the dynamics of the environment changes such that the solution space varies over time. For instance, our motivation for learning is that battery chemistry (and therefore performance) deteriorates over time, and the system should learn to avoid solutions that no longer work in the future. However, worn battery modules get replaced every now and then, and hence, some configurations that the learner may have eliminated previously may become applicable once again. Similar examples may be conjured for situations involving module malfunctions. The point is that each such change in the environment impacts the solution space in some way, however an explicit notification, that may be used to trigger a change in behaviour, is not always available (some changes may be continuous, for instance). Moreover, several such factors may play on the solution space at one time, and it is up to the learner to respond in this environment appropriately.


Here it becomes important for an agent to reliably recognise such changes, and respond by adjusting its exploration strategy accordingly (or perhaps, switching policies, choosing to learn afresh, and so on). Since the new confidence measure is built using observed data alone, it consequently reflects agent performance: our confidence is maximum when the rate of change in observed performance and worlds states is stable, and minimum when it is not. Importantly, this means that the measure is non-monotonic and as such may be used to dynamically tune the exploration strategy (for instance during plan selection as in Equation \ref{eqn:omega}) against a variable solution space.
