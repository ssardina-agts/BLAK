%!TEX root = ../aamas11storage.tex
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}\label{sec:discussion}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Here are some points in no particular order:

\begin{itemize}

\item We show how to combine context conditions with decision trees as suggested in earlier work.

\item We focus on an important feature of agent programming - adaptability. The aim is to build fully-functional systems that are also adaptable enough to rectify (some) environmental changes in the future. 

\item We show how our dynamic confidence measure is better able to cope with the practical issue that learning is not a one-shot affair with a definite start and end. Together with a means of limiting the training data (future work), it shows potential for building systems that learn ad infinitum.

\item We quantify the benefit of applicability thresholds. The threshold of $40\%$ used in the application reduces the number of battery operations by $12\%$ which is substantial when considering battery life. The difference in performance with and without the applicability threshold is not significant.

\item We show that for certain problems there may be a strong case for filtering training data. We see a dramatic reduction of $74\%$ in training set size for no noticeable change in performance in our experiments. This is promising and motivates further investigation on the matter (future work). More general filtering techniques are required and a better handle on the sorts of problems where this makes sense. 

\end{itemize}
