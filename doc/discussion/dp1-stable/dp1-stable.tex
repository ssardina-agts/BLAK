\documentclass[a4paper]{article}

% Use these
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{enumerate}
\usepackage{pdfpages}
\usepackage{float}
\usepackage{xspace}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}

% Allow page margins to be changed for specified block
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 
\setlength\fboxsep{0pt}

% Macros
\newcommand{\cc}{\emph{Concurrent}\xspace}
\newcommand{\st}{\emph{Stable}\xspace}
\newcommand{\tiv}{\emph{testImpactvars}\xspace}
\newcommand{\dt}{{decision tree}\xspace}
\newcommand{\DT}{{Decision Tree}\xspace}
\newcommand{\stephane}{{St\'ephane}\xspace}

% Begin
\title{BDI-Learning Discussion Paper:\\ Towards Improving \st Performance}

\author{
Dhirendra Singh\\ 
dhirendra.singh@rmit.edu.au\\
}

\begin{document}

\date{30 June 2009}

\maketitle

\setcounter{tocdepth}{2} 
\tableofcontents 

%% Section
\section{Motivating example}

\subsection{The scenario}
\begin{figure}[htbp]
   \centering
\fbox{\includegraphics[trim={3.7in 4.4in 3.5in 5in},clip,angle=-90,width=1\textwidth]{./testImpactvarsTree.pdf}}
   \caption{$testImpactvars$ input tree (sample portion only)}
   \label{fig:testImpactvarsTree}
\end{figure}

Continuing on with the investigation of learning context conditions in worlds with multiple variables, I created another test (referred to as \tiv in Figure \ref{fig:testImpactvarsTree} and herein) with a G/P tree that handles multiple variables and has the following properties.
\begin{itemize}
\item The tree handles $2^3$ worlds described by the variables set $[a,b,c]$.
\item All worlds have a unique solution in the G/P tree.
\item At level one, $TopGoal$ is handled by $8$ plans $[P1 \ldots P8]$ such that the worlds space is evenly distributed among these sub-plans and there is no overlap.
\item The plans ${P1..P8}$ have $3$ sub-goals each, such that the sequence required to succeed is of length $3$.
\item At level two, each sub-goal of $[P1 \ldots P8]$ is handled by $3$ leaf plans, only one of which will ever succeed. So the probability of selecting a successful sequence $p_{success}$ is given by the product of the probability of selecting a correct plan at level one and the probability of selecting $3$ correct sub-plans at level two. Therefore, $p_{success} = p_{level1} * {p_{level2}}^3 = \frac{1}{8} * {\frac{1}{3}}^3 = \frac{1}{216}$.
\item The G/P tree itself is evenly balanced i.e. the G/P hierarchy is of uniform breadth and depth.
\item Finally, the distribution of the worlds within the tree is also evenly balanced i.e. each sub-tree handled the same proportion of all possible worlds $\frac{1}{8}$.
\end{itemize}

In Figure \ref{fig:testImpactvarsTree}, the leaf nodes represent actions. Here actions with suffix $+0$ always fail. Actions with suffix $+a$ succeed when $a$ is $true$ while those with $-a$ succeed when $a$ is $false$. Similarly for $\pm b$ and $\pm c$. Looking at the sub-tree of plan $P4$ for instance, we can see that it will succeed only in the world $a\bar{b}\bar{c}$. In this way, the level one plans $[P1 \ldots P8]$ uniquely handle the worlds $[abc, ab\bar{c}, a\bar{b}c, a\bar{b}\bar{c}, \bar{a}bc, \bar{a}b\bar{c}, \bar{a}\bar{b}c, \bar{a}\bar{b}\bar{c}]$ respectively.

\begin{figure}[htbp]
   \centering
   \includegraphics[page=10,width=1.0\textwidth]{blak49.pdf}
   \caption{Performance comparison for $testImpactvars$}
   \label{fig:testImpactvarsperformance}
\end{figure}

Figure \ref{fig:testImpactvarsperformance} shows the performance of our \cc and \st approaches in this scenario. Notice that \st performance is almost four fold worse than \cc in this case. 

The reason why this result is relevant is that \tiv was not purposely crafted to favour one approach over the other. Furthermore, \tiv is relatively shallow and has a low branching factor compared to tests we have performed in the past. All in all, the test is simpler in hierarchy and is arguably a better representation of a \emph{typical} BDI tree than previously. The primary difference between this and previous tests (bar St\'ephane's tree) of course is that we are experimenting here with multiple worlds.


\subsection{Why \st performs poorly for \tiv}
\label{subsec:stablePoor}

So what causes \st to perform so poorly in this case? We can start with the obvious differences between the two approaches and see if we can eliminate this disparity. Intuitively, we know that \cc is an aggressive or \emph{optimistic} approach compared to \st that is controlled and relatively \emph{pessimistic}. 

The parameters that fine-tune \st behaviour are $k$ (the minimum number of instances \emph{of a given world} required for a \dt to be considered stable) and $\epsilon $ (the maximum change in probability between two instances before a \dt can be considered stable). For our experiments we use the default values of $k=3$ and $\epsilon=0.3$.

Already we can appreciate that $k=3$ imposes a strong constraint at our leaf level before failure updates can be propagated to the top. For instance in Figure \ref{fig:testImpactvarsTree}, goal $G4\_1$ will be considered stable for say world $abc$ when all its children $[P4\_1\_1 \ldots P4\_1\_3]$ are stable too. For $k=3$ that will take $3+3+3=9$ instances. For $P4$ to be updated, all its children $[G4\_1 \ldots G4\_3]$ must be stable. That in turn will take $9+9+9=27$ instances. Then for $P4$ to be stable for all possible worlds will take $27*8=216$ samples. For the entire tree to be stable will require a \emph{minimum} of $216*8=1728$ samples. The actual number will be more than that because samples are chosen randomly and will naturally result in duplicates.

Figure \ref{fig:testImpactvarsperformance} shows a simulation of $8000$ samples. Even considering the randomisation, shouldn't \st be performing optimally by then? Note that $k=3$ determines the lower bound on the number of samples. The actual number of samples required for stability of a node also depends on $\epsilon$, and to some extent on the \emph{noise} $n$ in the environment.

To reduce the disparity then, we run the experiment again with $k=1$, $\epsilon=1.0$, and $n=0.0$. This would make \st performance almost the same as \cc. Note that \st still requires the stability of each child and the entire tree still requires at least $(1+1+1)*3*8*8=576$ samples. While this number is the same for \cc, the difference is that the timing of \cc updates will be different to that of \st.

On conducting the experiment again with these lenient parameters the result is unexpected. Instead of \st performance converging towards \cc, there is \emph{no change} to \st performance when compared to Figure \ref{fig:testImpactvarsperformance}. 

This result suggests that other factors are at play here than those determined above. Debugging the implementation at length shows that some core decisions in the system introduce subtleties that eventually lead to performance degradation.


%% Section
\section{Understanding the Problem}
\label{sec:understanding}

\subsection{When is it ok to start using a \dt?}
\label{subsec:useDT}

The absolute minimum number of instances required to build a \dt is $1$. Currently this is the number we use to decide when to build and start using a tree, as determined by the  runtime parameter $m=1$. This decision causes several problems. 

At it's core, the problem is that we are constructing a \dt with a single sample of \emph{one} world and then using this tree to determine the probabilities for \emph{all} worlds. This is not reasonable. 

This problem manifests itself in various symptoms, some of which are listed here.
\begin{itemize}
\item Consider three leaf nodes $[Pi,Pj,Pk]$. At the start, none of the nodes have trees and always return a probability of $1$ by default. We are interested in world $W1$ where we know that $P_j$ will succeed. We may see the following possible sequence of events: 
\begin{enumerate}
\item The starting probabilities for selection in $W1$ are $[\frac{1}{3},\frac{1}{3},\frac{1}{3}]$. Let's say $Pi$ is randomly selected, executed, and fails in $W1$. It then builds a \dt from this sample and will use it thereon. 
\item Second time around in $W1$ the selection probabilities are $[[\approx0,\frac{1}{2},\frac{1}{2}]$. This time $Pk$ is randomly selected, fails, and builds a \dt.
\item Third time around in $W1$, the selection probabilities should be $[[\approx0,\frac{1}{1},\approx0]$ and $Pj$ should inevitably be selected. However the probabilities have somehow changed to $[\approx0, \approx0, \approx0]$. Why? Because sometime between the second and third instances of $W1$, $Pj$ was selected in \emph{some other world} and failed. It then constructed a single-sampled \dt in that world that it is now using in world $W1$ and returning a probability of $\approx0$. 
\end{enumerate}

That's the power of interpolation! The result is that the selection probabilities are now equal and back to the original value of $\frac{1}{3}$ each (but with each absolute probability $\approx 0$ instead of $1$ as at the start). The impact is that hereon the probability of selection of $P_j$ will not improve doesn't matter how many times $P_i$ and $P_k$ may fail in between. 

The problem worsens as the branching factor increases. Consider a set of $20$ plans, only one of which is setup to succeed. If it's probability incorrectly reduces to $\approx 0$ thanks to a misinformed \dt, then it's chances of selection will never improve beyond $\frac{1}{20}$ \emph{even though every other siblings may have been tried and failed numerous times}. For correct operation, the probability of this plan should gradually increase $\to1$ as other siblings are tried and fail. Note that here the environment is not stochastic so if a plan fails then it is a true failure and there is no chance of it passing in that given world in the future; furthermore the single plan wired for success will succeed in \textit{all} worlds not just a small portion of the worlds space. Even in this lenient case the problem is significant.

However, while the problem worsens as the branching factor increases, the probability of the problem state occurring in the first place decreases. In fact for M applicable plans, the probability of recovering from the problem is 1/M and the probability of witnessing the problem state is also 1/M.

\item A similar symptom is where the initial probabilities for $W1$ are all $[\frac{1}{3},\frac{1}{3},\frac{1}{3}]$, but change to say $[\frac{1}{2},\approx0,\frac{1}{2}]$ because a \dt was constructed for $P_j$ in some other world that is returning a misguided probability for $W1$. This even before $W1$ was ever encountered.

\end{itemize}

As a result the exploration of \st is unfairly biased leading to a slower convergence than expected. This is what I think we are seeing in \tiv. Note that this problem is evident in experiments with multiple worlds, hence why we haven't come across it earlier.

\subsection{When no \dt exists, what should the default $p$ be?}
\label{subsec:defaultP}

This question impacts the performance of both \cc and \st, and must be addressed carefully. So let us first ensure we understand the question clearly. Currently, the following (pseudo) code in every plan determines the likelihood of success in a given world.

\begin{verbatim}
probability = useDT(planID) ? probabilityDT(planID) : 1
\end{verbatim}

The decision is to use a probability of success of $1$ for any given world when the \dt for the plan is not ready for use (note that $useDT$ returns $false$ when we haven't encountered the minimum number of instance i.e. $m=1$), otherwise use the probability as determined by the \dt. We have already seen in Section \ref{subsec:useDT} what happens when the \dt being used is ill-informed and returns misleading probabilities for the world in question. But what about the other part of the equation? Does it matter what we use as the default probability when we have no \dt available? Turns out it does.

\begin{table}[htb]
  \begin{tabular}{ l l l p{0.45\textwidth}}
    \hline
    useDT & $p$ Used & Outcome & Comment\\ \hline
    
    [F F F] & [1 1 1] & Select $P_j$ and fail & The event is recorded for $P_j$. \\
    
    [F T F] & [1 $\approx$0 1] & Select $P_i$ and fail & This time around in $W$, a \dt was created for $P_j$ and used. The \dt returned $p\approx$0 for $W$. Subsequently $P_i$ was randomly selected and failed. The event was recorded for $P_i$.\\

    [T T F] & [$\approx$0 $\approx$0 1] & Select $P_k$ and pass & This time around in $W$, a \dt was created for $P_i$ and used along with the existing \dt for $P_j$. Both returned $p\approx$0 for $W$. Subsequently $P_k$ was inevitably selected and succeeded. The event was recorded for $P_k$.\\

    [T T T] & [$\approx$0 $\approx$0 1] & Select $P_k$ and pass & All {\dt}s are in use. Hereon $P_k$ will inevitably be selected most of the time which is what we expect.\\ \hline
    
    [F F F] & [1 1 1] & Select $P_k$ and pass & The event is recorded for $P_k$. \\
    
    [F F T] & [1 1 $\approx$1] & $\cdots$ & This time around in $W$, a \dt was created for $P_k$ and used. The \dt returned $p\approx$1 for $W$ which is what we expect. However since the default $p$ for $P_i$ and $P_j$ is also $1$, then the selection probabilities have not changed at all. So even though we have witnessed previously that $P_k$ succeeds in $W$, the probabilities used do not reflect this. This is not optimal.\\ \hline
    \hline
  \end{tabular}
  \caption{Impact of $p$ on plan selection in $W$ for a set of applicable plans $[P_i,P_j,P_k]$}
  \label{tab:p1Bad}
\end{table}

Table \ref{tab:p1Bad} shows the impact of the default probability $p$ on plan selection from a set of applicable plans $[P_i,P_j,P_k]$ in a given world $W$. It highlights the case when the choice of returning a default probability of $1$ does not work in our favour for plan selection. This poses the question if the default $p=1$ is the right choice and if not then what is? A default of $p\approx0$ does not work either for the following reasons:
\begin{itemize}
\item If the default is $p\approx0$ and some applicable plan is using a \dt that returns a probability $\to 1$ for the given world, then that plan will almost always be selected. This may cause other applicable plans to never be selected and tried. As a result the parent node will \emph{almost never} become stable (requirement for stable is that all children be stable so should have been tried in the given world at least $k$ times).
\item If the default is $p\approx0$ then failing in a given world will not change the probabilities. The impact is that the probability of selection of the good plan will not improve doesn't matter how many times other siblings have been tried and have failed. So even though we may witness numerous times that every other applicable plan has failed in the given world, the selection probability of the good plan (that has never been tried before and has the default $p\approx0$) will not improve.
\end{itemize}


%% Section
\section{Improving \textit{Stable} Performance I}

\subsection{When to use a \dt?}
\label{sec:deciding-when-to-use}

Section \ref{subsec:useDT} shows how the choice $m=1$ leads to ill-informed {\dt}s that distort plan selection probabilities. An obvious remedy is to increase $m$ to a suitable number that guarantees prediction within tolerance from the newly formed 
\dt. However, one cannot determine this optimal number since instances are generated randomly and include duplicates. Furthermore, the higher the number the longer we have to wait to use the power of {\dt}s, which is also not ideal.

The function $useDT$ currently determines when we are ready to start using a \dt as follows:
\begin{verbatim}
if(sub-treeOK && instances>=m){
\end{verbatim}
The code first checks to see that all children have their {\dt}s built and then confirms that the number of instances \emph{of any world} seen so far is greater than $m$.

I recommend we change this as follows:
\begin{verbatim}
if(sub-treeOK && instances>=m && (doStable?haveSeen(W):1)){
\end{verbatim}
The recommended change is that when deciding if we are ready to use a \dt, we include one additional check that we must have witnessed the world $W$ in question at least once before. In effect, we are saying that we are not confident in the tree for the given world unless we have seen that world at least once before, regardless of the number of total instances $m$ seen so far. The change applies only to \st and not \cc (determined by the $doStable?$ check).

At this point one could argue that the additional check is too restrictive because you lose the case where m is large enough that the resulting tree would still give a good estimate of the probability in $W$ even though we have never seen $W$ before (that's the power of {\dt}s remember). That is true, and we could form a more complex condition as follows:
\begin{verbatim}
if(...?(haveSeen(W) || instances>=newM ):..){
\end{verbatim}
This would allow us to start using the tree even when we have not seem $W$ but have seen enough instances to be confident that the \dt prediction will be meaningful. While $m$ is a static requirement, $newM$ could be calculated dynamically based on a number of factors one of which would be the total number of worlds. For instance we could say that we are confident in a \dt \emph{if we have seen the world $W$ before OR we have seen at least half (or any other fraction) of all possible worlds}. This decision is open for discussion, but for now I recommend only introducing the $haveSeen$ check.

\textit{Comment: This recommendation is declined as the $haveSeen$ check is too restrictive and defeats the purpose of using the \dt (for generalisation) in the first place.}

\subsection{When no \dt exists, default $p$ should be $0.5$}
\label{subsec:use-p-05}

Section \ref{subsec:defaultP} explains how the default values of $p=1$ or $p=0$ (for when no \dt exists for a given plan) can distort plan selection probabilities. I recommend we change the default probability to $p=0.5$ for the following reasons:
\begin{itemize}
\item Using a default $p=0.5$, when a plan finally switches to using the \dt $p$ will start to converge towards either $0$ or $1$ which is the true probability for that plan in the given world. We can say that the value $p=0.5$ is \emph{neutral} towards the true probability of $0$ or $1$.
\item When no other information is considered, and we have to \emph{estimate} (i.e. by setting a default) at design time what the chances of success of a plan are, then the logical choice is $50/50$, so a $p=0.5$ makes rational sense.
\end{itemize}

\subsection{Test Results}
The test results from applying the recommended changes are included in the Appendix \ref{sec:results1}.
\begin{itemize}
\item "Repository Revision 49" shows the baseline results before any changes were applied. Notice the problematic \tiv result here.
\item "Repository Revision 61" shows the results after applying the changes from Section \ref{subsec:use-p-05}. The change does not break any previous tests but also does not show any significant improvement in any results. Nonetheless, the change is appropriate and has been committed.
\end{itemize}


%% Section
\section{Improving \textit{Stable} Performance II}
\label{sec:stable2}

\subsection{When to use a \dt: The \textit{confidence} measure}
\label{subsec:confidence}
The core issue discussed in Section \ref{sec:understanding} is that of mis-classification by {\dt}s due to incorrect generalisation, that in turn results from the paucity of samples during the early stages of online learning. Our discussions highlight the necessity to consider two key elements in the use of {\dt}s in this case:

\begin{enumerate}
\item The current \textit{probability} of success in a given world as predicted by the \dt; and
\item Our \textit{confidence} in the current prediction.
\end{enumerate}

For measuring confidence we presently use a crude criterion, \textit{that the change in probabilities be small between successive queries}. Section \ref{sec:understanding} shows that this criterion is insufficient when dealing with multiple worlds as it does not consider the specific world in question. An improvement using a $haveSeen$ check suggested in Section \ref{sec:deciding-when-to-use} is also not appropriate since it forces the strict constraint that the world be witnessed before the \dt may be used to classify it, thereby defeating the purpose (i.e. interpolation) of using a \dt in the first place. 

In defining the characteristics of a \textit{confidence} measure we identify the following properties:
\begin{itemize}
\item It must consider the world $W$ being witnessed. Since we have not seen $W$ before, then this becomes a function of how times have we seen \textit{similar} worlds before. For a \dt, we might frame this as - given a \dt leaf node $L$ that will classify $W$, how many instances of other worlds $W' \neq W$ are being classified by the same node $L$.
\item Over time, the measure must monotonically tend from $0 \to 1$.
\end{itemize}

For instance, consider the issue described in Section \ref{subsec:useDT} where we have $20$ applicable plans such that $19$ bad plans report a correct $p=0$ while the single good plan reports an incorrect $p=0$ (generalisation error). Here $\frac{19}{20}$ times a bad plan will be executed but the choice will be \textit{wasted} because the result it will not change the relative probabilities (they are already all at zero). Ideally, one would expect that as the $19$ other plans are getting selected and failing, that the relative probability of selection of the good plan (not selected yet) should keep improving as a consequence. And when the good plan is finally selected and succeeds, it's relative probability should increase further $\to 1$. Currently, the former does not happen - we gain no information from the failure of the other $19$ plans when we should.

A well formed confidence measure would resolve this issue. In this case as each bad plan is selected and fails, this information is recorded in the confidence measure that consequently tends $\to 1$, even though the probability of success (already at zero) does not change with each failure.

Given this confidence measure, we can then use a threshold value to determine when it makes sense to use the probability given by the \dt. For situations where the confidence measure is lower than the threshold, the default probability to use is $0.5$ as described in Section \ref{subsec:use-p-05}.


\subsubsection{A first-pass confidence measure}
\label{subsubsec:confidence-measure}

We use a first-pass confidence measure based on the \textit{out-of-bag} error from a bootstrap aggregating (bagging) approach \cite{breiman1996baggi}. 

Given a standard training set $D$ of size $n$, bagging generates $p$ new training sets $D_i$ of size $n' \le n$, by sampling examples from $D$ uniformly and with replacement. By sampling with replacement it is likely that some examples will be repeated in each $D_i$. If $n'=n$, then for large $n$ the set $D_i$ is expected to have 63.2\% of the unique examples of $D$, the rest being duplicates. This kind of sample is known as a bootstrap sample. The $p$ models are fitted using the above $p$ bootstrap samples and combined for classification through a voting mechanism. The out-of-bag error here is the classification error on the remaining 36.8\% of the samples.

First, we change the semantics of the original $m$ parameter (that defines the number of instances required before a \dt may be used) slightly so that $m$ now determines \textit{the number of unique instances} required before a \dt may be utilised. This provides a more information rich sample set than originally. For a world consisting of $f$ binary \textit{features} (or variables), $m$ selects a subset $S' \subset S$ where $|S| = 2^f$. This way $m$ selects a subset (of all possible worlds) large enough to provide suitable performance. 

Second, we calculate the out-of-bag error for the training set $S'$. The choice of whether to use the \dt is determined by the out-of-bag error in relation to a pre-determined threshold value. For our implementation we use a threshold value of $0.5$. Put simply, this means that we will choose to utilise the \dt only when it's classification of the validation set is better than \textit{random}.

Note here that the choice of $m$ for a given experiment is not obvious. As a first-pass, we use the function $m=f$ for this value where $f$ is the number of features in the world. Our experiments show that this rather simple function gives us a sufficiently rich base training set to work from. 

%% Section
\subsection{Exploring when not using {\dt}s}

Introducing a measure of confidence for the {\dt}s (Section \ref{subsec:confidence}) implies that for each node in the goal/plan hierarchy several samples may pass before we begin to rely on the respective \dt. For the duration that the \dt is not ready, we currently use a random exploration strategy based on the default probability of success. This strategy to explore randomly when not using a \dt is far from ideal. A better strategy would be to utilise the information being recorded at each episode in order to make informed choices in these early stages. 

The inefficiency in using random exploration is apparent as we move further up the goal/plan tree. Firstly, since the stability idea effectively regulates the flow of (failure) information up the goal/plan hierarchy, then the amount of samples available at any one level is inherently less than the level below. This means that decisions have to be made using fewer samples at each higher level. Secondly, choices at higher levels have more weighing towards success. An incorrect choice at a higher level may ruin all chances of success in a given episode regardless of any bottom level decisions. Finally, it is not uncommon in our experiments for top level nodes to not become stable until late in the experiment and therefore to make decisions without help from suitably mature {\dt}s. This fact further motivates the requirement for a informed strategy in the absence of {\dt}s.

In repository revision 63, we have implemented an exploration strategy that is based on probabilities biased by the \textit{hamming distance} to a previously seen world, when the decision tree is not ready. The heuristic first finds the previously witnessed world $W'$ with the minimum hamming distance $h_{W'}$ to the current world $W$, and then biases the probability of success $p_W$ in $W$ with the witnessed result in $W'$, as show in Equation \ref{eqn:hdist}. A maximum bias is applied when $h_{W'}=0$, while no bias is applied for  $h_{W'}=h_{max}$.

\begin{equation}
\label{eqn:hdist}   
p_W= 0.5 + \left[ \left( \frac{{success_{W'}}}{attemepts_{W'}} - 0.5 \right) *  \left( 1 - \frac{h_{W'}}{H} \right)  \right]
\end{equation}

\textit{Comment: Biasing exploration based on feature similarity between worlds (as used in the hamming distance) is not a satisfactory solution because we cannot make a general claim that such a bias even makes sense. An exploration strategy based on information gain is preferred.}

\subsubsection{Ideas for information-based plan choice}

An informed exploration strategy might bias selection in order to maximise the \textit{information gain} from the choice. \stephane has proposed the following preliminary ideas for this extension:

\begin{itemize}
\item Given information about the number of instances $n$ classified by leaf node $L$ (where $L$ is the node that also classifies $W$), a first simple idea would be to select the plan with the lowest $n$. Equally we could select a plan with a probability inversely proportional to $n$.
\item One possible refinement to this would be to take into account the size of the input space represented by the leaf node. For example, if the leaf nodes for two plans correspond to worlds $[a]$ and $[a \cdot \bar{b} \cdot c]$ respectively and each contains the same number of instances, then the selection could favour the plan that expresses the larger portion of the worlds space, in this case $[a]$.
\item Another refinement is to use \textit{entropy}. This would take into account the number of failures/successes instead of a simple count of the instances contained in the node of the \dt. We can compute the change in entropy if the plan succeeds and if the plan fails, and maybe go with the plan with the biggest entropy drop. (\stephane says: I would need more time to check whether that makes sense, if observing say a failure is more promising than observing a success in terms of information gain for one DT/plan, and the opposite for another DT/plan, the decision is not that simple).
\item Another option is to \textit{simulate} the possible updates of the decision tree for each plan and base the decision on the outcome. For instance, it is possible that for the \dt of a plan $P1$ that regardless of whether $P1$ succeeds of fails, the \dt structure will not change (the probability in the corresponding leaf node of the \dt only will change). For another plan $P2$ however, the new instance may trigger a change in the \dt structure. In this case, we would favor $P2$ over $P1$.
\end{itemize}



%% Section
\subsection{Consequences of using a default $p=0.5$}
\label{subsec:consequence-p}

The decision to use a default $p=0.5$ (refer Section \ref{subsec:use-p-05}) introduces a further subtlety in the meta level probabilistic plan selection (PS) mechanism. Consider the case where we have two applicable plans $P_1$ and $P_2$ whose individual probabilities of success in world $w$ are given by the set $[0.5 , 1.0]$. This represents the situation where $P_1$ is using a default $p=0.5$ and $P_2$ is using a true $p=1.0$. In this case, one would expect $P_2$ to be selected more often than not because we are confident in it's success in world $W$. However, the PS mechanism will use the relative probabilities $[\frac{0.5}{1.5} , \frac{1.0}{1.5}]$ which means that $P_2$ has a selection probability of only $0.67$. Now, say $P_1$ was a deep subtree that does not hold a solution. Then we would be spending a third of our resources in exploring $P_1$ when we already have a suitable solution in $P_2$. The problem becomes more pronounced as the number of applicable plans grows. Consider the case of $20$ applicable plans, $19$ of which are bad but use a default $p=0.5$, and only one is good (has the solution) and is using the true $p=1.0$. In this case, the relative probability of selection of the good plan is only $\frac{1.0}{1.0 + 19*0.5} \approx 9.5\%$.

One way to improve the weighing of the higher probabilities during plan selection is to use a power function $p^a$ for the individual probabilities, however it is not clear how the value $a$ should be selected. A predefined $a$ is not suitable as the set of applicable plans may vary significantly. Using $a=f(n)$ where $n$ is the number of applicable plans is an option but our empirical testing shows that the plan selection (and the subsequent experiment outcome) is quite sensitive to the choice of $a$ and the several tried options for $f(n)$ did not produce a robust enough result across all experiments.

\begin{algorithm}[htb]
\caption{$pSelect(P)$}
\label{alg:pselect}
\begin{algorithmic}[1]
\STATE $p_m \leftarrow max(P)$
\STATE $P' \leftarrow (P/p_m)^a$
\STATE $P'' \leftarrow [P' \cdot (P' \neq 1)] + [P' \cdot (P' = 1) \cdot |P|]$
\STATE return $P''$
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:pselect} shows the final heuristic we use to calculate the relative probabilities of selection. Here $P$ is the set of probabilities of the individual plans such that $p_m$ is the maximum probability in the set $P$ (Line 1). We obtain a new set $P'$ with the original probabilities scaled to $1.0$ and using a power factor $a=3$ (Line 2), and further boost the probabilities of the best candidate plans thus obtained (i.e those that have  $p=1.0$ in the set P') by the size of the set $|P|$ (Line 3). This ensures that the relative probability of the best plans is greater than the remainder by a factor of \textit{at least} $|P|$. The final set $P''$ is then used to make the probabilistic selection. Our experiments show that this heuristic provides a good balance between the size $|P|$ of the set, and the maximum probability $p_m$ in the set.

Since the choice of the probabilistic selection mechanism has a significant impact on the results, a better approach would be to formally validate this function so as to guarantee a probability profile. Algorithm \ref{alg:pselect} has only been verified empirically.

\subsection{Changes to stability checking for child nodes}

When doing stability checks for children of a node, a termination check was added to ensure the following:

\begin{itemize}
\item For a \textit{Goal} node $N$, when checking the stability of the children plans set $C$ in world $W$, the stability checking should stop and the node $N$ be marked stable when a child $C_i$ is found to be \textit{successful} in world $W$. This is because if $C_i \in C$ is successful in $W$, then it will almost always be selected and the other plans $C_j \neq C_i$ would almost never become stable.
\item For a \textit{Plan} node $N$, when checking the stability of the children goals set $C$ in world $W$, the stability checking should stop and the node $N$ be marked stable when a child $C_i$ is found to be \textit{unsuccessful} in world $W$. This is because if $C_i \in C$ is unsuccessful in $W$, then the other subgoals $C_j \neq C_i$ would never be tried and would therefore never become stable.

\end{itemize}

This fixes an existing issue with the stability checking algorithm.

\subsection{Test Results}
The test results from applying the changes are included in the Appendix \ref{sec:results1}.
\begin{itemize}
\item "Repository Revision 61" is the baseline results before any changes were applied. Notice the problematic \tiv result.
\item "Repository Revision 63" shows the results after applying the changes from Section \ref{sec:stable2}. Noticeably, \tiv performance is now fixed. Interestingly, the change also resolves the dummy variables issue of test \textit{testDummyvars} (using m=20, where \st performs poorly when forced to consider features of the world that have no final bearing on the results (her $19/20$ features have no impact on results). The \textit{testDummyvars} result was produced using $m=20$ as described in Section \ref{subsubsec:confidence-measure}
\end{itemize}

\textit{Comment: Repository revision 63 fixes all our existing tests! However we have not approved these changes because (1) the hamming distance based exploration does not make good sense; (2) the modification to the probabilistc plan selection in Section \ref{subsec:consequence-p} is the result of an arbitrary formulation that works for our purpose but has not been evaluated for the general case; and (3) the confidence measure based on a modified $m$ semantic and a out-of-bag error is not robust enough. }


%% Section
%\section{Improving \textit{Stable} Performance III}
%\subsection{Improved book keeping}

%% Section 
\section{Acknowledgements}

Revisions of this document are a result of an ongoing discussion and contributions from \stephane Airiau of the University of Amsterdam, and Sebastian Sardina and Professor Lin Padgham of RMIT University.

%% Bibliography
\bibliographystyle{plain}
\bibliography{biblio} 


%% Appendix
\appendix
\section{Appendix}


%% Appendix Section
\subsection{Results}
\label{sec:results1}
\includepdf[pages={1-},nup=2x3]{blak49.pdf}
\includepdf[pages={1-},nup=2x3]{blak61.pdf}
\includepdf[pages={1-},nup=2x3]{blak63.pdf}


%% Appendix Section
\subsection{Test coverage}
Sebastian and I briefly discussed the \tiv result and agreed that it exposes some grey area about our understanding of \cc and \st. Moreover, from our current tests it is difficult to understand the conditions under which one approach performs better than the other. 

In considering how we can improve context learning, we need a better understanding of how the two current approaches perform in a combination of factors as listed in Table \ref{tab:perofrmanceFactors}. So far we have experimented with some of these factors but not enough to get a good understanding of how they influence our approaches.

\begin{table}[!htb]
  \begin{tabular}{ p{0.7\textwidth} l }
    \hline
    Factor & Tested\\ \hline
    Branching factor of tree & Yes\\
    Depth of tree & Yes\\
    Stochastic nature of the environment & Yes\\
    Number of worlds & Yes recently\\
    If the G/P hierarchy is balanced (all sub-trees have the same breath/depth) & Barely (recently)\\
    If the distribution of worlds within the G/P hierarchy is balanced (sub-trees handle an equal share of all possible worlds) & Barely (recently)\\
    If more than one sub-tree holds a solution & No\\
    If failure has a cost & No\\
    \hline
  \end{tabular}
  \caption{Factors that impact the performance of \cc and \st}
  \label{tab:perofrmanceFactors}
\end{table}

Our testing strategy is open for discussion. Should we conduct systematic testing to understand the influences of the various factors on the two approaches? If so, what is the best strategy here since the number of combinations is too high.

\subsection{Insights into the workings of \cc and \st}

Finally, here I have collected some insights into the workings of our two approaches in a generalised manner.

\begin{itemize}
\item \st performs better when
\begin{enumerate}
\item One solution exists in a deep sub-tree (note that differences between the approaches is amplified when the the probability of hitting that solution is lowered by fine-tuning the breath/depth of the sub-tree); and
\item At least one other sub-tree is \emph{shallower}; and
\item the shallower sub-tree \emph{does not} hold a solution.
\end{enumerate}

In this case, \st will realise first that the shallower sub-tree does not hold the solution and that the deeper sub-tree \emph{may}. So it will assign a lower probability to the shallower sub-tree. (\cc will assign more or less equal probability to all sub-trees since none of them seem to work). In effect the probability of picking the deeper sub-tree increases and therefore \st has a better chance of finding the solution there first.

\item \cc performs better when
\begin{enumerate}
\item One solution exists in a deep sub-tree (same as before); and
\item At least one other sub-tree is also \emph{deep}; and
\item All other \emph{deep} sub-trees \emph{do not} have a solution (the more the number of failing deep sub-trees the more amplified the difference).
\end{enumerate}

In this case, \cc performs the same as before. \st however takes a long time to be confident that the failing deep sub-trees are in fact fruitless so it does not change their probabilities for a long time. When a solution is finally found, \cc favours that sub-tree whereas \st still devotes exploration to the fruitless sub-trees until it is confident that no solution exists there.

\item At the leaf nodes, the differences between \cc and \st are minimal, but \st takes longer to be confident that an observation of failure is in fact a true failure and not due to a stochastic environment.

\end{itemize}

\end{document}
